"""
Veri yÃ¼kleme ve Ã¶rnek veri oluÅŸturma fonksiyonlarÄ±
"""

import numpy as np
import pandas as pd
from sklearn.datasets import make_blobs, fetch_olivetti_faces
import warnings
warnings.filterwarnings('ignore')


def generate_sample_data(n_samples=1000, n_features=50, n_clusters=5, random_state=42):
    """
    PCA iÃ§in Ã¶rnek veri oluÅŸturur
    
    Args:
        n_samples: Ã–rnek sayÄ±sÄ±
        n_features: Ã–zellik sayÄ±sÄ±
        n_clusters: KÃ¼me sayÄ±sÄ±
        random_state: Rastgelelik iÃ§in seed
        
    Returns:
        (X, y) tuple - Veri ve etiketler
    """
    X, y = make_blobs(
        n_samples=n_samples,
        n_features=n_features,
        centers=n_clusters,
        random_state=random_state,
        cluster_std=2.0
    )
    return X, y


def generate_rating_matrix(n_users=100, n_items=50, sparsity=0.7, random_state=42):
    """
    Ã–neri sistemi iÃ§in Ã¶rnek rating matrisi oluÅŸturur
    
    Args:
        n_users: KullanÄ±cÄ± sayÄ±sÄ±
        n_items: ÃœrÃ¼n sayÄ±sÄ±
        sparsity: Eksik veri oranÄ± (0-1)
        random_state: Rastgelelik iÃ§in seed
        
    Returns:
        Rating matrisi (NaN deÄŸerlerle)
    """
    np.random.seed(random_state)
    
    # GerÃ§ekÃ§i bir rating matrisi oluÅŸtur
    # BazÄ± kullanÄ±cÄ±lar daha yÃ¼ksek, bazÄ±larÄ± daha dÃ¼ÅŸÃ¼k rating verir
    user_biases = np.random.normal(0, 0.8, n_users)  # Daha fazla Ã§eÅŸitlilik
    item_biases = np.random.normal(0, 0.6, n_items)  # Daha fazla Ã§eÅŸitlilik
    
    # Latent faktÃ¶rler - daha fazla Ã§eÅŸitlilik iÃ§in
    n_factors = 15  # Daha fazla faktÃ¶r
    user_factors = np.random.normal(0, 1.2, (n_users, n_factors))  # Daha fazla varyans
    item_factors = np.random.normal(0, 1.2, (n_items, n_factors))
    
    # Rating matrisi - daha Ã§eÅŸitli rating'ler iÃ§in
    rating_matrix = (
        3.0 +  # Biraz daha dÃ¼ÅŸÃ¼k global ortalama (daha fazla Ã§eÅŸitlilik iÃ§in)
        user_biases[:, np.newaxis] +
        item_biases[np.newaxis, :] +
        np.dot(user_factors, item_factors.T) * 0.3 +  # FaktÃ¶r etkisini artÄ±r
        np.random.normal(0, 0.6, (n_users, n_items))  # Daha fazla gÃ¼rÃ¼ltÃ¼
    )
    
    # 1-5 aralÄ±ÄŸÄ±na sÄ±nÄ±rla
    rating_matrix = np.clip(rating_matrix, 1, 5)
    
    # Daha fazla Ã§eÅŸitlilik iÃ§in bazÄ± rating'leri ekstrem deÄŸerlere Ã§ek
    extreme_mask = np.random.random((n_users, n_items)) < 0.1  # %10 ekstrem
    rating_matrix[extreme_mask] = np.random.choice([1.0, 1.5, 4.5, 5.0], 
                                                   size=np.sum(extreme_mask))
    
    # Sparsity uygula (bazÄ± rating'leri NaN yap)
    mask = np.random.random((n_users, n_items)) < sparsity
    rating_matrix[mask] = np.nan
    
    return rating_matrix


def load_sample_images(n_images=100):
    """
    Ã–rnek gÃ¶rÃ¼ntÃ¼ verisi yÃ¼kler veya oluÅŸturur
    
    Args:
        n_images: YÃ¼klenecek gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±
        
    Returns:
        GÃ¶rÃ¼ntÃ¼ matrisi ve gÃ¶rÃ¼ntÃ¼ boyutlarÄ±
    """
    try:
        # Olivetti faces dataset'ini dene
        faces = fetch_olivetti_faces()
        images = faces.images[:n_images]
        image_shape = images[0].shape
        # GÃ¶rÃ¼ntÃ¼leri dÃ¼zleÅŸtir
        images_flat = images.reshape(n_images, -1)
        return images_flat, image_shape
    except:
        # EÄŸer yÃ¼klenemezse, sentetik gÃ¶rÃ¼ntÃ¼ler oluÅŸtur
        np.random.seed(42)
        image_shape = (64, 64)
        n_pixels = np.prod(image_shape)
        
        # Basit pattern'ler oluÅŸtur
        images_flat = []
        for i in range(n_images):
            # FarklÄ± pattern'ler
            pattern = np.random.choice(['circle', 'square', 'noise'])
            if pattern == 'circle':
                img = create_circle_image(image_shape, i)
            elif pattern == 'square':
                img = create_square_image(image_shape, i)
            else:
                img = np.random.random(image_shape)
            images_flat.append(img.flatten())
        
        return np.array(images_flat), image_shape


def create_circle_image(shape, seed):
    """Daire pattern'i oluÅŸturur"""
    np.random.seed(seed)
    h, w = shape
    center_x, center_y = w // 2, h // 2
    radius = min(w, h) // 3 + np.random.randint(-5, 5)
    
    y, x = np.ogrid[:h, :w]
    mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
    img = np.zeros(shape)
    img[mask] = 1.0
    return img


def create_square_image(shape, seed):
    """Kare pattern'i oluÅŸturur"""
    np.random.seed(seed)
    h, w = shape
    size = min(w, h) // 3 + np.random.randint(-5, 5)
    start_x = (w - size) // 2
    start_y = (h - size) // 2
    
    img = np.zeros(shape)
    img[start_y:start_y+size, start_x:start_x+size] = 1.0
    return img


def generate_text_corpus(n_documents=100):
    """
    Topic modeling iÃ§in Ã¶rnek metin korpusu oluÅŸturur
    
    Args:
        n_documents: DokÃ¼man sayÄ±sÄ±
        
    Returns:
        DokÃ¼man listesi
    """
    # FarklÄ± topic'ler iÃ§in kelime setleri
    topics = {
        'technology': ['computer', 'software', 'algorithm', 'data', 'network', 
                      'system', 'digital', 'code', 'programming', 'internet'],
        'science': ['research', 'experiment', 'hypothesis', 'theory', 'discovery',
                   'analysis', 'method', 'study', 'observation', 'evidence'],
        'sports': ['game', 'player', 'team', 'match', 'championship', 'victory',
                  'competition', 'training', 'coach', 'stadium'],
        'health': ['medicine', 'treatment', 'patient', 'disease', 'health',
                  'doctor', 'hospital', 'therapy', 'diagnosis', 'recovery'],
        'business': ['company', 'market', 'profit', 'investment', 'strategy',
                    'management', 'customer', 'revenue', 'growth', 'finance']
    }
    
    documents = []
    np.random.seed(42)
    
    for i in range(n_documents):
        # Her dokÃ¼man iÃ§in 1-2 topic seÃ§
        selected_topics = np.random.choice(list(topics.keys()), 
                                         size=np.random.randint(1, 3), 
                                         replace=False)
        
        # Her topic'ten 5-10 kelime seÃ§
        doc_words = []
        for topic in selected_topics:
            n_words = np.random.randint(5, 10)
            words = np.random.choice(topics[topic], size=n_words, replace=True)
            doc_words.extend(words)
        
        # DokÃ¼manÄ± oluÅŸtur
        document = ' '.join(doc_words)
        documents.append(document)
    
    return documents


def generate_noisy_data(original_data, noise_level=0.1):
    """
    GÃ¼rÃ¼ltÃ¼lÃ¼ veri oluÅŸturur (SVD noise reduction iÃ§in)
    
    Args:
        original_data: Orijinal veri matrisi
        noise_level: GÃ¼rÃ¼ltÃ¼ seviyesi (0-1)
        
    Returns:
        GÃ¼rÃ¼ltÃ¼lÃ¼ veri matrisi
    """
    noise = np.random.normal(0, noise_level, original_data.shape)
    noisy_data = original_data + noise
    return noisy_data


def load_rating_data_from_file(file, user_col=None, item_col=None, rating_col=None):
    """
    CSV/Excel dosyasÄ±ndan rating verisi yÃ¼kler ve rating matrisine Ã§evirir
    
    Args:
        file: YÃ¼klenen dosya (Streamlit file_uploader'dan veya BytesIO)
        user_col: KullanÄ±cÄ± ID sÃ¼tunu adÄ± (None ise otomatik tespit)
        item_col: ÃœrÃ¼n ID sÃ¼tunu adÄ± (None ise otomatik tespit)
        rating_col: Rating sÃ¼tunu adÄ± (None ise otomatik tespit)
        
    Returns:
        (rating_matrix, user_mapping, item_mapping) tuple
        - rating_matrix: n_users x n_items numpy array (NaN deÄŸerlerle)
        - user_mapping: Orijinal user ID'lerden indekslere mapping dict
        - item_mapping: Orijinal item ID'lerden indekslere mapping dict
    """
    # Streamlit file_uploader'dan gelen dosya stream'i bir kez okununca tÃ¼kenir
    # Bu yÃ¼zden dosya iÃ§eriÄŸini hafÄ±zaya al
    import io
    
    # Dosya adÄ±nÄ± al (BytesIO veya file objesi olabilir)
    file_name = getattr(file, 'name', 'unknown.csv')
    
    # Dosya iÃ§eriÄŸini oku
    try:
        # EÄŸer zaten BytesIO ise, iÃ§eriÄŸi al
        if isinstance(file, io.BytesIO):
            file_content = file.getvalue()
            file_bytes = io.BytesIO(file_content)
        else:
            # Streamlit file_uploader'dan gelen dosya
            file_content = file.read()
            file_bytes = io.BytesIO(file_content)
    except AttributeError:
        # EÄŸer read() metodu yoksa, direkt kullan
        file_bytes = file
    
    # Dosya tipine gÃ¶re yÃ¼kle
    if file_name.endswith('.csv'):
        df = pd.read_csv(file_bytes)
    elif file_name.endswith(('.xlsx', '.xls')):
        df = pd.read_excel(file_bytes)
    else:
        raise ValueError("Desteklenen formatlar: CSV, Excel (.xlsx, .xls)")
    
    # Dosya boÅŸ mu kontrol et
    if df.empty:
        raise ValueError("Dosya boÅŸ! LÃ¼tfen en az bir satÄ±r veri iÃ§eren bir dosya yÃ¼kleyin.")
    
    # SÃ¼tun kontrolÃ¼
    if len(df.columns) == 0:
        raise ValueError("Dosyada hiÃ§ sÃ¼tun bulunamadÄ±!")
    
    if len(df.columns) < 3:
        raise ValueError(
            f"Dosyada yeterli sÃ¼tun yok! En az 3 sÃ¼tun gerekli (user_id, item_id, rating).\n"
            f"Mevcut sÃ¼tunlar ({len(df.columns)} adet): {list(df.columns)}"
        )
    
    # SÃ¼tun isimlerini otomatik tespit et
    if user_col is None:
        # OlasÄ± kullanÄ±cÄ± sÃ¼tunu isimleri
        possible_user_cols = ['user_id', 'user', 'userId', 'UserID', 'userid', 
                             'customer_id', 'customer', 'CustomerID']
        user_col = None
        for col in df.columns:
            if col.lower() in [c.lower() for c in possible_user_cols]:
                user_col = col
                break
        if user_col is None:
            # Ä°lk sÃ¼tunu dene
            user_col = df.columns[0]
    
    if item_col is None:
        # OlasÄ± Ã¼rÃ¼n sÃ¼tunu isimleri
        possible_item_cols = ['item_id', 'item', 'itemId', 'ItemID', 'itemid',
                             'product_id', 'product', 'ProductID', 'movie_id', 'movie']
        item_col = None
        for col in df.columns:
            if col.lower() in [c.lower() for c in possible_item_cols]:
                item_col = col
                break
        if item_col is None:
            # Ä°kinci sÃ¼tunu dene
            item_col = df.columns[1] if len(df.columns) > 1 else df.columns[0]
    
    if rating_col is None:
        # OlasÄ± rating sÃ¼tunu isimleri
        possible_rating_cols = ['rating', 'Rating', 'RATING', 'score', 'Score',
                               'value', 'Value', 'preference', 'Preference']
        rating_col = None
        for col in df.columns:
            if col.lower() in [c.lower() for c in possible_rating_cols]:
                rating_col = col
                break
        if rating_col is None:
            # ÃœÃ§Ã¼ncÃ¼ sÃ¼tunu dene
            rating_col = df.columns[2] if len(df.columns) > 2 else df.columns[-1]
    
    # Gerekli sÃ¼tunlarÄ± kontrol et
    required_cols = [user_col, item_col, rating_col]
    if not all(col in df.columns for col in required_cols):
        available_cols = list(df.columns)
        raise ValueError(
            f"Gerekli sÃ¼tunlar bulunamadÄ±!\n"
            f"Tespit edilen sÃ¼tunlar: {required_cols}\n"
            f"Dosyadaki mevcut sÃ¼tunlar: {available_cols}\n"
            f"LÃ¼tfen dosyanÄ±zda 'user_id', 'item_id', 'rating' gibi sÃ¼tunlar olduÄŸundan emin olun."
        )
    
    # Veriyi temizle
    df_clean = df[[user_col, item_col, rating_col]].copy()
    original_count = len(df_clean)
    df_clean = df_clean.dropna()
    after_dropna_count = len(df_clean)
    
    # Rating deÄŸerlerini sayÄ±sal yap
    df_clean[rating_col] = pd.to_numeric(df_clean[rating_col], errors='coerce')
    after_numeric_count = len(df_clean)
    df_clean = df_clean.dropna()
    final_count = len(df_clean)
    
    # BoÅŸ veri kontrolÃ¼ - daha detaylÄ± hata mesajÄ±
    if df_clean.empty:
        # Diagnostik bilgileri topla
        error_details = []
        error_details.append(f"Tespit edilen sÃ¼tunlar: user_col='{user_col}', item_col='{item_col}', rating_col='{rating_col}'")
        error_details.append(f"Dosyadaki toplam satÄ±r sayÄ±sÄ±: {len(df)}")
        error_details.append(f"SeÃ§ilen sÃ¼tunlardaki toplam satÄ±r: {original_count}")
        error_details.append(f"dropna() sonrasÄ± satÄ±r sayÄ±sÄ±: {after_dropna_count}")
        error_details.append(f"to_numeric() sonrasÄ± satÄ±r sayÄ±sÄ±: {after_numeric_count}")
        error_details.append(f"Son temizleme sonrasÄ± satÄ±r sayÄ±sÄ±: {final_count}")
        
        # Ã–rnek veri gÃ¶ster
        if len(df) > 0:
            error_details.append(f"\nDosyadaki ilk 5 satÄ±r Ã¶rneÄŸi:")
            error_details.append(str(df.head()))
            error_details.append(f"\nSeÃ§ilen sÃ¼tunlarÄ±n veri tipleri:")
            error_details.append(f"  {user_col}: {df[user_col].dtype}")
            error_details.append(f"  {item_col}: {df[item_col].dtype}")
            error_details.append(f"  {rating_col}: {df[rating_col].dtype}")
            
            # Rating sÃ¼tunundaki benzersiz deÄŸerler (ilk 10)
            unique_ratings = df[rating_col].dropna().unique()[:10]
            error_details.append(f"\nRating sÃ¼tunundaki Ã¶rnek deÄŸerler (ilk 10): {list(unique_ratings)}")
        
        error_msg = "Dosyada geÃ§erli rating verisi bulunamadÄ±!\n\n"
        error_msg += "OlasÄ± nedenler:\n"
        error_msg += "1. Rating sÃ¼tunu sayÄ±sal deÄŸil (metin, boÅŸ, vb.)\n"
        error_msg += "2. YanlÄ±ÅŸ sÃ¼tunlar seÃ§ilmiÅŸ olabilir\n"
        error_msg += "3. TÃ¼m satÄ±rlarda eksik veri (NaN) var\n\n"
        error_msg += "Detaylar:\n" + "\n".join(error_details)
        error_msg += "\n\nLÃ¼tfen dosyanÄ±zda en az bir tane geÃ§erli (user_id, item_id, rating) Ã¼Ã§lÃ¼sÃ¼ olduÄŸundan emin olun."
        error_msg += "\nRating deÄŸerleri sayÄ±sal olmalÄ±dÄ±r (Ã¶rn: 1, 2, 3, 4, 5 veya 0.5, 1.0, 2.5 gibi)."
        
        raise ValueError(error_msg)
    
    # Rating aralÄ±ÄŸÄ±nÄ± kontrol et ve normalize et (1-5 aralÄ±ÄŸÄ±na)
    if len(df_clean) == 0:
        raise ValueError("Dosyada hiÃ§ geÃ§erli rating deÄŸeri yok!")
    
    min_rating = df_clean[rating_col].min()
    max_rating = df_clean[rating_col].max()
    
    # min/max kontrolÃ¼ (eÄŸer tÃ¼m deÄŸerler aynÄ±ysa)
    if pd.isna(min_rating) or pd.isna(max_rating):
        raise ValueError("Dosyada geÃ§erli rating deÄŸeri bulunamadÄ±! TÃ¼m rating deÄŸerleri NaN veya sayÄ±sal deÄŸil.")
    
    if max_rating > 5 or min_rating < 1:
        # Rating'leri 1-5 aralÄ±ÄŸÄ±na normalize et
        df_clean[rating_col] = 1 + (df_clean[rating_col] - min_rating) / (max_rating - min_rating) * 4
    
    # User ve Item ID'lerini indekslere Ã§evir
    unique_users = df_clean[user_col].unique()
    unique_items = df_clean[item_col].unique()
    
    user_mapping = {user_id: idx for idx, user_id in enumerate(sorted(unique_users))}
    item_mapping = {item_id: idx for idx, item_id in enumerate(sorted(unique_items))}
    
    # Rating matrisi oluÅŸtur - bÃ¼yÃ¼k matrisler iÃ§in sparse kullan
    n_users = len(unique_users)
    n_items = len(unique_items)
    
    # BÃ¼yÃ¼k matrisler iÃ§in sparse matrix kullan (10M+ hÃ¼cre)
    matrix_size = n_users * n_items
    use_sparse = matrix_size > 10_000_000  # 10 milyon hÃ¼creden bÃ¼yÃ¼kse
    
    if use_sparse:
        # Sparse matrix oluÅŸtur (sadece mevcut rating'leri sakla)
        from scipy.sparse import csr_matrix
        rows = []
        cols = []
        values = []
        
        for _, row in df_clean.iterrows():
            user_idx = user_mapping[row[user_col]]
            item_idx = item_mapping[row[item_col]]
            rows.append(user_idx)
            cols.append(item_idx)
            values.append(row[rating_col])
        
        rating_matrix = csr_matrix(
            (values, (rows, cols)),
            shape=(n_users, n_items),
            dtype=np.float64
        )
    else:
        # KÃ¼Ã§Ã¼k matrisler iÃ§in dense matrix
        rating_matrix = np.full((n_users, n_items), np.nan)
        
        # Veriyi matrise doldur
        for _, row in df_clean.iterrows():
            user_idx = user_mapping[row[user_col]]
            item_idx = item_mapping[row[item_col]]
            rating_matrix[user_idx, item_idx] = row[rating_col]
    
    return rating_matrix, user_mapping, item_mapping


def load_rating_matrix_from_file(file):
    """
    Zaten rating matrisi formatÄ±nda olan CSV/Excel dosyasÄ±nÄ± yÃ¼kler
    
    Args:
        file: YÃ¼klenen dosya (Streamlit file_uploader'dan veya BytesIO)
        
    Returns:
        Rating matrisi (numpy array, NaN deÄŸerlerle)
    """
    # Streamlit file_uploader'dan gelen dosya stream'i bir kez okununca tÃ¼kenir
    # Bu yÃ¼zden dosya iÃ§eriÄŸini hafÄ±zaya al
    import io
    
    # Dosya adÄ±nÄ± al (BytesIO veya file objesi olabilir)
    file_name = getattr(file, 'name', 'unknown.csv')
    
    # Dosya iÃ§eriÄŸini oku
    try:
        # EÄŸer zaten BytesIO ise, iÃ§eriÄŸi al
        if isinstance(file, io.BytesIO):
            file_content = file.getvalue()
            file_bytes = io.BytesIO(file_content)
        else:
            # Streamlit file_uploader'dan gelen dosya
            file_content = file.read()
            file_bytes = io.BytesIO(file_content)
    except AttributeError:
        # EÄŸer read() metodu yoksa, direkt kullan
        file_bytes = file
    
    # Dosya tipine gÃ¶re yÃ¼kle
    if file_name.endswith('.csv'):
        # CSV dosyasÄ± iÃ§in delimiter tespiti ve farklÄ± encoding'ler dene
        delimiters = [',', ';', '\t', '|']
        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
        
        df = None
        last_error = None
        
        # Ã–nce delimiter tespiti yap
        file_bytes.seek(0)
        try:
            first_line_bytes = file_bytes.readline()
            first_line = first_line_bytes.decode('utf-8', errors='ignore')
        except:
            # EÄŸer decode edilemezse, dosyayÄ± string olarak oku
            file_bytes.seek(0)
            first_line = file_bytes.read(1000).decode('utf-8', errors='ignore').split('\n')[0]
        
        # En uygun delimiter'i bul
        detected_delimiter = ','
        max_cols = 0
        for delim in delimiters:
            cols = first_line.split(delim)
            if len(cols) > max_cols:
                max_cols = len(cols)
                detected_delimiter = delim
        
        # FarklÄ± encoding'lerle dene
        for encoding in encodings:
            try:
                file_bytes.seek(0)  # DosyayÄ± baÅŸa al
                
                # Ã–NCE index_col OLMADAN OKU (daha gÃ¼venli)
                # Bu ÅŸekilde "No columns to parse" hatasÄ±ndan kaÃ§Ä±nÄ±rÄ±z
                df_temp = pd.read_csv(
                    file_bytes, 
                    sep=detected_delimiter, 
                    encoding=encoding, 
                    engine='python',
                    header=0  # Ä°lk satÄ±rÄ± header olarak kullan
                )
                
                # Dosya boÅŸ mu kontrol et
                if df_temp.empty:
                    raise ValueError("CSV dosyasÄ± boÅŸ! LÃ¼tfen geÃ§erli bir veri dosyasÄ± yÃ¼kleyin.")
                
                # En az 2 sÃ¼tun olmalÄ±
                if df_temp.shape[1] < 2:
                    raise ValueError(f"CSV dosyasÄ±nda yeterli sÃ¼tun yok! {df_temp.shape[1]} sÃ¼tun bulundu, en az 2 sÃ¼tun olmalÄ± (1 kullanÄ±cÄ± ID + en az 1 Ã¼rÃ¼n sÃ¼tunu).")
                
                # Ä°lk sÃ¼tunu index yap (Matrix Format iÃ§in)
                df = df_temp.set_index(df_temp.columns[0])
                
                # BaÅŸarÄ±lÄ± oldu, dÃ¶ngÃ¼den Ã§Ä±k
                break
                
            except (UnicodeDecodeError, pd.errors.ParserError) as e:
                last_error = e
                continue
            except ValueError as e:
                # ValueError'larÄ± direkt yÃ¼kselt (boÅŸ dosya, yetersiz sÃ¼tun vb.)
                raise e
            except Exception as e:
                # DiÄŸer hatalar iÃ§in
                error_str = str(e)
                if "No columns to parse" in error_str:
                    # Delimiter yanlÄ±ÅŸ olabilir, otomatik tespit dene
                    try:
                        file_bytes.seek(0)
                        # Delimiter=None ile otomatik tespit
                        df_temp = pd.read_csv(
                            file_bytes, 
                            sep=None, 
                            encoding=encoding, 
                            engine='python',
                            header=0
                        )
                        if df_temp.shape[1] >= 2:
                            df = df_temp.set_index(df_temp.columns[0])
                            break
                        else:
                            last_error = ValueError(f"CSV dosyasÄ±nda yeterli sÃ¼tun yok! {df_temp.shape[1]} sÃ¼tun bulundu.")
                    except Exception as e2:
                        last_error = e2
                        continue
                else:
                    last_error = e
                    continue
        
        if df is None:
            error_msg = f"CSV dosyasÄ± okunamadÄ±: {str(last_error) if last_error else 'Bilinmeyen hata'}"
            error_msg += "\n\nğŸ’¡ Ä°puÃ§larÄ±:"
            error_msg += "\n- CSV dosyasÄ±nÄ±n ilk sÃ¼tunu kullanÄ±cÄ± ID'leri olmalÄ±"
            error_msg += "\n- En az 2 sÃ¼tun olmalÄ± (1 kullanÄ±cÄ± ID + en az 1 Ã¼rÃ¼n sÃ¼tunu)"
            error_msg += f"\n- Delimiter olarak ÅŸunlar deneniyor: {', '.join(delimiters)}"
            error_msg += "\n- Dosya formatÄ±nÄ± kontrol edin"
            raise ValueError(error_msg)
            
    elif file_name.endswith(('.xlsx', '.xls')):
        try:
            # Excel dosyasÄ± iÃ§in de BytesIO kullan
            file_bytes.seek(0)
            df = pd.read_excel(file_bytes, index_col=0)
            # En az 1 sÃ¼tun olmalÄ± (index hariÃ§)
            if df.shape[1] == 0:
                # index_col=0 olmadan dene
                file_bytes.seek(0)
                df_temp = pd.read_excel(file_bytes)
                if df_temp.shape[1] > 1:
                    df = df_temp.set_index(df_temp.columns[0])
                else:
                    raise ValueError("Excel dosyasÄ±nda yeterli sÃ¼tun yok! En az 2 sÃ¼tun olmalÄ± (1 kullanÄ±cÄ± ID + en az 1 Ã¼rÃ¼n sÃ¼tunu).")
        except Exception as e:
            error_msg = f"Excel dosyasÄ± okunamadÄ±: {str(e)}"
            error_msg += "\n\nğŸ’¡ Ä°puÃ§larÄ±:"
            error_msg += "\n- Excel dosyasÄ±nÄ±n ilk sÃ¼tunu kullanÄ±cÄ± ID'leri olmalÄ±"
            error_msg += "\n- En az 2 sÃ¼tun olmalÄ± (1 kullanÄ±cÄ± ID + en az 1 Ã¼rÃ¼n sÃ¼tunu)"
            raise ValueError(error_msg)
    else:
        raise ValueError("Desteklenen formatlar: CSV, Excel (.xlsx, .xls)")
    
    # BoÅŸ dosya kontrolÃ¼
    if df.empty:
        raise ValueError("Dosya boÅŸ! LÃ¼tfen geÃ§erli bir veri dosyasÄ± yÃ¼kleyin.")
    
    # DataFrame'i numpy array'e Ã§evir
    rating_matrix = df.values
    
    # Shape kontrolÃ¼
    if rating_matrix.shape[0] == 0 or rating_matrix.shape[1] == 0:
        raise ValueError(f"Veri matrisi boÅŸ! Shape: {rating_matrix.shape}. En az 1 satÄ±r ve 1 sÃ¼tun olmalÄ±.")
    
    # NaN deÄŸerleri koru
    try:
        rating_matrix = rating_matrix.astype(float)
    except (ValueError, TypeError) as e:
        raise ValueError(f"Rating deÄŸerleri sayÄ±sal deÄŸil! LÃ¼tfen tÃ¼m rating deÄŸerlerinin sayÄ±sal olduÄŸundan emin olun. Hata: {str(e)}")
    
    # En az bir tane geÃ§erli (NaN olmayan) deÄŸer olmalÄ±
    valid_values = np.sum(~np.isnan(rating_matrix))
    if valid_values == 0:
        raise ValueError("Dosyada hiÃ§ geÃ§erli rating deÄŸeri yok! TÃ¼m deÄŸerler NaN. LÃ¼tfen en az bir tane sayÄ±sal rating deÄŸeri olduÄŸundan emin olun.")
    
    return rating_matrix

