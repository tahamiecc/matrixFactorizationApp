"""
Streamlit Web UygulamasÄ±
Matris FaktÃ¶rizasyon AlgoritmalarÄ± - KapsamlÄ± Uygulama
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.metrics import mean_squared_error

# Klasik Algoritmalar
from algorithms.svd import SVDRecommender, SVDNoiseReducer
from algorithms.pca import PCAAnalyzer
from algorithms.nmf import NMFImageProcessor, NMFTopicModeler
from algorithms.als import ALSRecommender

# Modern Deep Learning Algoritmalar - PyTorch tabanlÄ± (Python 3.14 uyumlu âœ…)
MODERN_AVAILABLE = False
try:
    import torch
    from algorithms.ncf import NCFRecommender
    from algorithms.autoencoder import DenoisingAutoencoder, VAERecommender
    from algorithms.fm import FactorizationMachine, DeepFM
    from algorithms.transformer import TransformerRecommender
    from algorithms.gnn import GNNRecommender
    MODERN_AVAILABLE = True
except ImportError as e:
    st.warning(f"âš ï¸ Modern algoritmalar iÃ§in PyTorch gerekli: {e}")
    st.info("YÃ¼klemek iÃ§in: `pip install torch torch-geometric`")

# YardÄ±mcÄ± fonksiyonlar
from utils.data_loader import (
    generate_sample_data, 
    generate_rating_matrix,
    load_sample_images,
    generate_text_corpus,
    generate_noisy_data,
    load_rating_data_from_file,
    load_rating_matrix_from_file
)
from utils.visualization import (
    plot_ratings_matrix,
    plot_recommendations,
    plot_image_grid,
    plot_topic_words
)

# Sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(
    page_title="Matris FaktÃ¶rizasyon AlgoritmalarÄ±",
    page_icon="ğŸ”¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .algorithm-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .metric-card {
        background-color: #e8f4f8;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)


def main():
    """Ana uygulama"""
    st.markdown('<h1 class="main-header">ğŸ”¢ Matris FaktÃ¶rizasyon AlgoritmalarÄ±</h1>', 
                unsafe_allow_html=True)
    
    # Sidebar
    st.sidebar.title("ğŸ“‹ MenÃ¼")
    menu_items = [
        "ğŸ  Ana Sayfa",
        "--- Klasik Algoritmalar ---",
        "ğŸ“Š SVD - Ã–neri Sistemi",
        "ğŸ”‡ SVD - GÃ¼rÃ¼ltÃ¼ Temizleme",
        "ğŸ“ˆ PCA - Veri GÃ¶rselleÅŸtirme",
        "ğŸ–¼ï¸ NMF - GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme",
        "ğŸ“ NMF - Topic Modeling",
        "âš¡ ALS - Ã–neri Sistemi",
        "--- Modern Algoritmalar ---",
        "ğŸ§  NCF - Neural Collaborative Filtering",
        "ğŸ¨ Autoencoder - GÃ¼rÃ¼ltÃ¼ Temizleme",
        "ğŸ¯ VAE - Variational Autoencoder",
        "ğŸ”— Factorization Machines",
        "ğŸš€ DeepFM",
        "ğŸ”„ Transformer - Sequential Recommendation",
        "ğŸ•¸ï¸ GNN - Graph Neural Network",
        "--- KarÅŸÄ±laÅŸtÄ±rma ---",
        "ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±",
        "--- AI AsistanÄ± ---",
        "ğŸ¤– AI Chat - Veri AsistanÄ±"
    ]
    
    page = st.sidebar.selectbox("Sayfa SeÃ§in", menu_items)
    
    if page == "ğŸ  Ana Sayfa":
        show_homepage()
    elif page == "ğŸ“Š SVD - Ã–neri Sistemi":
        show_svd_recommender()
    elif page == "ğŸ”‡ SVD - GÃ¼rÃ¼ltÃ¼ Temizleme":
        show_svd_noise_reduction()
    elif page == "ğŸ“ˆ PCA - Veri GÃ¶rselleÅŸtirme":
        show_pca_visualization()
    elif page == "ğŸ–¼ï¸ NMF - GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme":
        show_nmf_image_processing()
    elif page == "ğŸ“ NMF - Topic Modeling":
        show_nmf_topic_modeling()
    elif page == "âš¡ ALS - Ã–neri Sistemi":
        show_als_recommender()
    elif page == "ğŸ§  NCF - Neural Collaborative Filtering" and MODERN_AVAILABLE:
        show_ncf_recommender()
    elif page == "ğŸ¨ Autoencoder - GÃ¼rÃ¼ltÃ¼ Temizleme" and MODERN_AVAILABLE:
        show_autoencoder_denoising()
    elif page == "ğŸ¯ VAE - Variational Autoencoder" and MODERN_AVAILABLE:
        show_vae_recommender()
    elif page == "ğŸ”— Factorization Machines" and MODERN_AVAILABLE:
        show_fm_recommender()
    elif page == "ğŸš€ DeepFM" and MODERN_AVAILABLE:
        show_deepfm_recommender()
    elif page == "ğŸ”„ Transformer - Sequential Recommendation" and MODERN_AVAILABLE:
        show_transformer_recommender()
    elif page == "ğŸ•¸ï¸ GNN - Graph Neural Network" and MODERN_AVAILABLE:
        show_gnn_recommender()
    elif page == "ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±":
        show_performance_comparison()
    elif page == "ğŸ¤– AI Chat - Veri AsistanÄ±":
        show_ai_chatbot()
    elif page.startswith("---"):
        show_homepage()
    else:
        if not MODERN_AVAILABLE:
            st.error("âš ï¸ Modern algoritmalar iÃ§in PyTorch gerekli!")
            st.info("ğŸ’¡ LÃ¼tfen ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: `pip install torch torch-geometric`")
        else:
            show_homepage()


def show_homepage():
    """Ana sayfa"""
    st.markdown("""
    ## HoÅŸ Geldiniz! ğŸ‘‹
    
    Bu uygulama, matris faktÃ¶rizasyon algoritmalarÄ±nÄ±n detaylÄ± kullanÄ±mÄ±nÄ± gÃ¶steren 
    kapsamlÄ± bir Python uygulamasÄ±dÄ±r.
    
    ### ğŸ“š Algoritmalar
    
    #### ğŸ¯ Klasik Algoritmalar
    
    **1. SVD (Singular Value Decomposition)**
    - Matrisi tekil deÄŸerlerine ayÄ±rÄ±r. Matematiksel olarak en kesin yÃ¶ntemdir.
    - âœ… Ã–neri sistemleri, GÃ¼rÃ¼ltÃ¼ temizleme
    
    **2. PCA (Principal Component Analysis)**
    - Veriyi daha dÃ¼ÅŸÃ¼k boyutlu bir uzaya izdÃ¼ÅŸÃ¼rÃ¼r.
    - âœ… Veri gÃ¶rselleÅŸtirme, Ã–zellik seÃ§imi
    
    **3. NMF (Non-negative Matrix Factorization)**
    - Matrisleri sadece pozitif deÄŸerlerle ayÄ±rÄ±r.
    - âœ… GÃ¶rÃ¼ntÃ¼ iÅŸleme, Metin madenciliÄŸi (Topic Modeling)
    
    **4. ALS (Alternating Least Squares)**
    - BÃ¼yÃ¼k Ã¶lÃ§ekli sistemlerde paralel Ã§alÄ±ÅŸmaya Ã§ok uygundur.
    - âœ… BÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶neri motorlarÄ±
    
    #### ğŸš€ Modern Deep Learning Algoritmalar
    
    **5. NCF (Neural Collaborative Filtering)**
    - SVD'nin Deep Learning versiyonu. DoÄŸrusal olmayan iliÅŸkileri Ã¶ÄŸrenir.
    - âœ… Netflix, YouTube gibi modern Ã¶neri sistemleri
    
    **6. Autoencoder (Denoising & VAE)**
    - SVD ve PCA'in Deep Learning karÅŸÄ±lÄ±ÄŸÄ±.
    - âœ… GÃ¼rÃ¼ltÃ¼ temizleme, Variational Autoencoder ile Ã¶neriler
    
    **7. Factorization Machines (FM) & DeepFM**
    - Context-aware Ã¶neri sistemi. Yan bilgileri kullanÄ±r.
    - âœ… Reklam tÄ±klama tahmini (CTR), Context-aware Ã¶neriler
    
    **8. Transformer (BERT4Rec/SASRec)**
    - ChatGPT mimarisinin Ã¶neri sistemlerine uyarlanmÄ±ÅŸ hali.
    - âœ… TikTok, YouTube gibi sequential Ã¶neriler
    
    **9. GNN (Graph Neural Network)**
    - Veriyi tablo deÄŸil, aÄŸ (graph) olarak gÃ¶rÃ¼r.
    - âœ… Pinterest, Uber Eats, Sosyal aÄŸ tabanlÄ± Ã¶neriler
    
    ### ğŸš€ KullanÄ±m
    
    Sol menÃ¼den istediÄŸiniz algoritmayÄ± seÃ§erek baÅŸlayabilirsiniz!
    
    **Not**: Modern algoritmalar iÃ§in TensorFlow/PyTorch gerekebilir.
    """)
    
    # HÄ±zlÄ± istatistikler
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Toplam Algoritma", "9" if MODERN_AVAILABLE else "4")
    with col2:
        st.metric("Klasik Algoritma", "4")
    with col3:
        st.metric("Modern Algoritma", "5" if MODERN_AVAILABLE else "0")
    with col4:
        st.metric("KullanÄ±m Ã–rneÄŸi", "8+")
    
    if not MODERN_AVAILABLE:
        st.warning("âš ï¸ Modern algoritmalar iÃ§in PyTorch gerekli!")
        st.info("ğŸ’¡ Modern algoritmalarÄ± kullanmak iÃ§in: `pip install torch torch-geometric`")


def get_optimal_model_params(model_name, data_shape=None, n_samples=None, n_features=None):
    """
    Veri boyutuna gÃ¶re optimal model parametrelerini dÃ¶ndÃ¼rÃ¼r
    
    Args:
        model_name: Model adÄ±
        data_shape: (n_users, n_items) veya (n_samples, n_features) tuple
        n_samples: Ã–rnek sayÄ±sÄ±
        n_features: Ã–zellik sayÄ±sÄ±
        
    Returns:
        Parametreler dictionary'si
    """
    params = {}
    
    if data_shape:
        n_samples = data_shape[0]
        n_features = data_shape[1] if len(data_shape) > 1 else None
    
    if model_name == "svd":
        # SVD: n_components genelde min(n_users, n_items) / 3 ile min(n_users, n_items) / 2 arasÄ±
        if data_shape and len(data_shape) == 2:
            n_users, n_items = data_shape
            max_comp = min(n_users, n_items)
            params['n_components'] = max(10, min(30, max_comp // 3))
        else:
            params['n_components'] = 20
    
    elif model_name == "als":
        # ALS: n_factors genelde 20-50 arasÄ±, regularization 0.1, iterations 15-20
        if data_shape and len(data_shape) == 2:
            n_users, n_items = data_shape
            max_factors = min(n_users, n_items)
            params['n_factors'] = max(10, min(30, max_factors // 5))
        else:
            params['n_factors'] = 20
        params['regularization'] = 0.1
        params['iterations'] = 15
    
    elif model_name == "pca":
        # PCA: n_components genelde min(n_features, n_samples-1) / 2
        if n_features:
            params['n_components'] = min(20, max(5, n_features // 2))
        else:
            params['n_components'] = 20
    
    elif model_name == "nmf_image":
        # NMF Image: n_components genelde 10-30 arasÄ±
        params['n_components'] = 20
    
    elif model_name == "nmf_topic":
        # NMF Topic: n_topics genelde 5-10, max_features 500-1000
        params['n_topics'] = 5
        params['max_features'] = 1000
    
    elif model_name == "autoencoder":
        # AutoEncoder: encoding_dim genelde n_features / 2 ile n_features / 4 arasÄ±
        if n_features:
            params['encoding_dim'] = max(10, min(50, n_features // 2))
        else:
            params['encoding_dim'] = 20
        params['epochs'] = 50
        params['noise_factor'] = 0.2
    
    elif model_name == "vae":
        # VAE: latent_dim genelde 50-100, epochs 30-50
        params['latent_dim'] = 50
        params['epochs'] = 30
    
    elif model_name == "ncf":
        # NCF: n_factors 32-64, epochs 10-20, batch_size 128-256
        params['n_factors'] = 50
        params['epochs'] = 10
        params['batch_size'] = 256
        params['dropout'] = 0.2
    
    elif model_name == "transformer":
        # Transformer: d_model 64-128, n_heads 4-8, max_seq_length 50, epochs 10
        params['d_model'] = 128
        params['n_heads'] = 4
        params['max_seq_length'] = 50
        params['epochs'] = 10
    
    elif model_name == "gnn":
        # GNN: embedding_dim 32-128, epochs 30-50
        params['embedding_dim'] = 64
        params['epochs'] = 50
    
    elif model_name == "fm" or model_name == "deepfm":
        # FM/DeepFM: embedding_dim 32-64, epochs 30-50
        params['embedding_dim'] = 64
        params['epochs'] = 30
    
    return params


def get_file_format_selector(model_name="model", include_image=False):
    """
    Dosya formatÄ± seÃ§ici widget'Ä± dÃ¶ndÃ¼rÃ¼r
    
    Args:
        model_name: Model adÄ± (key iÃ§in)
        include_image: GÃ¶rÃ¼ntÃ¼ formatÄ± seÃ§eneÄŸini dahil et (varsayÄ±lan: False)
        
    Returns:
        SeÃ§ilen format string'i
    """
    format_options = [
        "ğŸ“Š Excel FormatÄ± (Long Format: user_id, item_id, rating)",
        "ğŸ“‹ Matris FormatÄ± (Rating Matrisi: SatÄ±rlar=kullanÄ±cÄ±, SÃ¼tunlar=Ã¼rÃ¼n)",
        "ğŸ“ Her Ä°kisi (Otomatik Tespit)",
        "ğŸ–¼ï¸ FotoÄŸraf/GÃ¶rÃ¼ntÃ¼ (Sadece gÃ¶rÃ¼ntÃ¼ iÅŸleme modelleri iÃ§in)"
    ]
    
    # GÃ¶rÃ¼ntÃ¼ formatÄ± dahil edilecekse tÃ¼m seÃ§enekleri gÃ¶ster
    if include_image:
        options_to_show = format_options
    else:
        options_to_show = format_options[:3]  # FotoÄŸraf sadece gÃ¶rÃ¼ntÃ¼ modellerinde
    
    selected_format = st.radio(
        "ğŸ“„ Dosya FormatÄ± SeÃ§in",
        options_to_show,
        key=f"{model_name}_format",
        help="YÃ¼kleyeceÄŸiniz dosyanÄ±n formatÄ±nÄ± seÃ§in"
    )
    
    return selected_format


def show_svd_recommender():
    """SVD Ã¶neri sistemi"""
    st.header("ğŸ“Š SVD - Ã–neri Sistemi")
    
    # Info bÃ¶lÃ¼mÃ¼
    with st.expander("â„¹ï¸ SVD Ã–neri Sistemi HakkÄ±nda Bilgi", expanded=False):
        st.markdown("""
        ### SVD (Singular Value Decomposition) Nedir?
        
        **SVD**, bir matrisi Ã¼Ã§ matrisin Ã§arpÄ±mÄ±na ayÄ±ran matematiksel bir yÃ¶ntemdir:
        - **U**: Sol tekil vektÃ¶rler matrisi (kullanÄ±cÄ± Ã¶zellikleri)
        - **Î£**: Tekil deÄŸerler matrisi (kÃ¶ÅŸegen matris)
        - **V^T**: SaÄŸ tekil vektÃ¶rler matrisi (Ã¼rÃ¼n Ã¶zellikleri)
        
        ### Ã–neri Sistemlerinde KullanÄ±mÄ±
        
        1. **Rating Matrisi FaktÃ¶rizasyonu**: KullanÄ±cÄ±-Ã¼rÃ¼n rating matrisini dÃ¼ÅŸÃ¼k rank matrislere ayÄ±rÄ±r
        2. **Latent FaktÃ¶rler**: Gizli kullanÄ±cÄ± ve Ã¼rÃ¼n Ã¶zelliklerini keÅŸfeder
        3. **Eksik Rating Tahmini**: KullanÄ±cÄ±larÄ±n henÃ¼z deÄŸerlendirmediÄŸi Ã¼rÃ¼nler iÃ§in rating tahmin eder
        
        ### Parametreler
        
        - **Tekil DeÄŸer SayÄ±sÄ±**: KullanÄ±lacak latent faktÃ¶r sayÄ±sÄ±. Daha fazla = daha detaylÄ± ama daha yavaÅŸ
        - **Eksik Veri OranÄ±**: Rating matrisindeki boÅŸ hÃ¼cre oranÄ± (sparsity)
        
        ### Metrikler
        
        - **RMSE (Root Mean Square Error)**: Tahmin hatasÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼. DÃ¼ÅŸÃ¼k deÄŸer = daha iyi performans
        - **Tekil DeÄŸerler**: Her bileÅŸenin Ã¶nemini gÃ¶sterir. BÃ¼yÃ¼k deÄŸerler = daha Ã¶nemli bileÅŸenler
        """)
    
    st.markdown("""
    SVD (Singular Value Decomposition) kullanarak Ã¶neri sistemi oluÅŸturma.
    """)
    
    # Veri yÃ¼kleme seÃ§eneÄŸi
    data_source = st.radio(
        "Veri KaynaÄŸÄ±",
        ["ğŸ“Š Ã–rnek Veri OluÅŸtur", "ğŸ“ Dosyadan YÃ¼kle"],
        horizontal=True
    )
    
    # Dosya formatÄ± seÃ§imi (sadece dosya yÃ¼kleme seÃ§ildiyse)
    file_format = None
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        file_format = get_file_format_selector("svd", include_image=False)
    
    # Session state ile veriyi koru
    if 'svd_rating_matrix' not in st.session_state:
        st.session_state.svd_rating_matrix = None
        st.session_state.svd_user_mapping = None
        st.session_state.svd_item_mapping = None
    
    rating_matrix = st.session_state.svd_rating_matrix
    user_mapping = st.session_state.svd_user_mapping
    item_mapping = st.session_state.svd_item_mapping
    
    # VarsayÄ±lan deÄŸerler
    n_users = None
    n_items = None
    n_components = 30
    sparsity = 0.6  # VarsayÄ±lan deÄŸer
    
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
        st.info("""
        **Desteklenen Formatlar:** CSV, Excel (.xlsx, .xls)
        
        **Veri FormatÄ± SeÃ§enekleri:**
        1. **Long Format** (Ã–nerilen): Her satÄ±r bir kullanÄ±cÄ±-Ã¼rÃ¼n-rating Ã¼Ã§lÃ¼sÃ¼
           - SÃ¼tunlar: `user_id`, `item_id`, `rating`
           - Ã–rnek: `user_id,item_id,rating` â†’ `1,5,4.5`
        
        2. **Matrix Format**: Zaten rating matrisi formatÄ±nda
           - Ä°lk sÃ¼tun: KullanÄ±cÄ± ID'leri (index)
           - DiÄŸer sÃ¼tunlar: ÃœrÃ¼n ID'leri
           - DeÄŸerler: Rating'ler (NaN = eksik veri)
        """)
        
        file = st.file_uploader(
            "Veri dosyasÄ±nÄ± seÃ§in",
            type=['csv', 'xlsx', 'xls'],
            help="CSV veya Excel dosyasÄ± yÃ¼kleyin"
        )
        
        if file is not None:
            try:
                # Dosya Ã¶nizlemesi ve format Ã¶nerisi
                # Dosya stream'i bir kez okununca tÃ¼kenir, bu yÃ¼zden iÃ§eriÄŸi hafÄ±zaya al
                import io
                file_content = file.read()
                file_bytes = io.BytesIO(file_content)
                
                import pandas as pd
                if file.name.endswith('.csv'):
                    # Delimiter tespiti
                    file_bytes.seek(0)
                    first_line = file_bytes.readline().decode('utf-8', errors='ignore')
                    delimiters = [',', ';', '\t', '|']
                    detected_delimiter = ','
                    max_cols = 0
                    for delim in delimiters:
                        cols = first_line.split(delim)
                        if len(cols) > max_cols:
                            max_cols = len(cols)
                            detected_delimiter = delim
                    
                    file_bytes.seek(0)
                    preview_df = pd.read_csv(file_bytes, nrows=5, sep=detected_delimiter, engine='python')
                    # Toplam satÄ±r sayÄ±sÄ± iÃ§in dosyayÄ± tekrar oku
                    file_bytes.seek(0)
                    total_df = pd.read_csv(file_bytes, sep=detected_delimiter, engine='python')
                elif file.name.endswith(('.xlsx', '.xls')):
                    file_bytes.seek(0)
                    preview_df = pd.read_excel(file_bytes, nrows=5)
                    # Toplam satÄ±r sayÄ±sÄ± iÃ§in dosyayÄ± tekrar oku
                    file_bytes.seek(0)
                    total_df = pd.read_excel(file_bytes)
                else:
                    preview_df = None
                    total_df = None
                
                if preview_df is not None:
                    with st.expander("ğŸ‘ï¸ Dosya Ã–nizleme (Ä°lk 5 SatÄ±r)", expanded=False):
                        st.dataframe(preview_df, width='stretch')
                        st.info(f"""
                        **Dosya Bilgileri:**
                        - **SatÄ±r SayÄ±sÄ±**: {len(total_df) if total_df is not None else 'Bilinmiyor'} (tahmini)
                        - **SÃ¼tun SayÄ±sÄ±**: {len(preview_df.columns)}
                        - **SÃ¼tun Ä°simleri**: {', '.join(preview_df.columns.tolist()[:10])}{'...' if len(preview_df.columns) > 10 else ''}
                        
                        **Format Ã–nerisi:**
                        - **3 sÃ¼tun varsa** â†’ Long Format seÃ§in (user_id, item_id, rating)
                        - **10+ sÃ¼tun varsa** â†’ Matrix Format seÃ§in (ilk sÃ¼tun kullanÄ±cÄ± ID, diÄŸerleri Ã¼rÃ¼n ID)
                        """)
                
                # Dosya stream'ini tekrar kullanÄ±labilir hale getir
                file.seek(0)
                
                # Dosya formatÄ±na gÃ¶re veri formatÄ±nÄ± belirle
                if file_format and file_format.startswith("ğŸ“Š"):
                    data_format = "Long Format (user_id, item_id, rating)"
                elif file_format and file_format.startswith("ğŸ“‹"):
                    data_format = "Matrix Format (Rating Matrisi)"
                elif file_format and file_format.startswith("ğŸ“"):
                    # Otomatik tespit - sÃ¼tun sayÄ±sÄ±na gÃ¶re
                    if len(preview_df.columns) == 3:
                        data_format = "Long Format (user_id, item_id, rating)"
                    elif len(preview_df.columns) > 3:
                        data_format = "Matrix Format (Rating Matrisi)"
                    else:
                        data_format = "Long Format (user_id, item_id, rating)"  # VarsayÄ±lan
                else:
                    # Eski yÃ¶ntem (geriye dÃ¶nÃ¼k uyumluluk)
                    data_format = st.radio(
                        "Veri FormatÄ±",
                        ["Long Format (user_id, item_id, rating)", "Matrix Format (Rating Matrisi)"],
                        horizontal=True
                    )
                
                if data_format == "Long Format (user_id, item_id, rating)":
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        user_col = st.text_input("KullanÄ±cÄ± SÃ¼tunu", value="", 
                                                help="KullanÄ±cÄ± ID sÃ¼tunu adÄ± (boÅŸ bÄ±rakÄ±rsanÄ±z otomatik tespit edilir)")
                    with col2:
                        item_col = st.text_input("ÃœrÃ¼n SÃ¼tunu", value="",
                                                help="ÃœrÃ¼n ID sÃ¼tunu adÄ± (boÅŸ bÄ±rakÄ±rsanÄ±z otomatik tespit edilir)")
                    with col3:
                        rating_col = st.text_input("Rating SÃ¼tunu", value="",
                                                  help="Rating sÃ¼tunu adÄ± (boÅŸ bÄ±rakÄ±rsanÄ±z otomatik tespit edilir)")
                    
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            # Dosya bilgilerini kaydet
                            file_name = file.name
                            file_size = file.size
                            
                            rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(
                                file,
                                user_col=user_col if user_col else None,
                                item_col=item_col if item_col else None,
                                rating_col=rating_col if rating_col else None
                            )
                            
                            # Dosya bilgilerini session state'e kaydet
                            st.session_state.svd_file_name = file_name
                            st.session_state.svd_file_size = file_size
                            st.session_state.svd_user_col = user_col if user_col else "Otomatik tespit edildi"
                            st.session_state.svd_item_col = item_col if item_col else "Otomatik tespit edildi"
                            st.session_state.svd_rating_col = rating_col if rating_col else "Otomatik tespit edildi"
                            
                            # Session state'e kaydet
                            st.session_state.svd_rating_matrix = rating_matrix
                            st.session_state.svd_user_mapping = user_mapping
                            st.session_state.svd_item_mapping = item_mapping
                            
                            # Veri istatistiklerini hesapla
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                                ratings_data = rating_matrix.data
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                                ratings_data = rating_matrix[mask]
                            
                            # Rating istatistikleri - boÅŸ array kontrolÃ¼
                            if len(ratings_data) == 0:
                                # EÄŸer hiÃ§ rating yoksa varsayÄ±lan deÄŸerler
                                min_rating = 0.0
                                max_rating = 0.0
                                mean_rating = 0.0
                                median_rating = 0.0
                                st.warning("âš ï¸ UyarÄ±: Dosyada hiÃ§ rating deÄŸeri bulunamadÄ±! TÃ¼m deÄŸerler NaN olabilir.")
                            else:
                                min_rating = float(np.min(ratings_data))
                                max_rating = float(np.max(ratings_data))
                                mean_rating = float(np.mean(ratings_data))
                                median_rating = float(np.median(ratings_data))
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            
                            # DetaylÄ± dosya analizi bÃ¶lÃ¼mÃ¼
                            with st.expander("ğŸ“‹ Dosya Analizi - KullanÄ±lan Veriler", expanded=True):
                                st.markdown("### ğŸ“ Dosya Bilgileri")
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Dosya AdÄ±", file_name)
                                    st.metric("Dosya Boyutu", f"{file_size / 1024:.2f} KB")
                                with col2:
                                    st.metric("Veri FormatÄ±", "Long Format")
                                    st.metric("Toplam SatÄ±r SayÄ±sÄ±", f"{n_ratings:,}")
                                
                                st.markdown("### ğŸ“Š KullanÄ±lan SÃ¼tunlar")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.info(f"**KullanÄ±cÄ± SÃ¼tunu:**\n{st.session_state.svd_user_col}")
                                with col2:
                                    st.info(f"**ÃœrÃ¼n SÃ¼tunu:**\n{st.session_state.svd_item_col}")
                                with col3:
                                    st.info(f"**Rating SÃ¼tunu:**\n{st.session_state.svd_rating_col}")
                                
                                st.markdown("### ğŸ“ˆ Veri Ä°statistikleri")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("KullanÄ±cÄ± SayÄ±sÄ±", f"{rating_matrix.shape[0]:,}")
                                with col2:
                                    st.metric("ÃœrÃ¼n SayÄ±sÄ±", f"{rating_matrix.shape[1]:,}")
                                with col3:
                                    st.metric("Toplam Rating", f"{n_ratings:,}")
                                with col4:
                                    st.metric("Veri YoÄŸunluÄŸu", f"{density:.2f}%")
                                
                                st.markdown("### â­ Rating DaÄŸÄ±lÄ±mÄ±")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("Minimum Rating", f"{min_rating:.2f}")
                                with col2:
                                    st.metric("Maksimum Rating", f"{max_rating:.2f}")
                                with col3:
                                    st.metric("Ortalama Rating", f"{mean_rating:.2f}")
                                with col4:
                                    st.metric("Medyan Rating", f"{median_rating:.2f}")
                                
                                st.markdown("""
                                **ğŸ“ AÃ§Ä±klama:**
                                - **Dosya Bilgileri**: YÃ¼klenen dosyanÄ±n adÄ± ve boyutu
                                - **KullanÄ±lan SÃ¼tunlar**: Veri iÅŸlemede kullanÄ±lan sÃ¼tun isimleri
                                - **Veri Ä°statistikleri**: Matris boyutlarÄ± ve veri yoÄŸunluÄŸu
                                - **Rating DaÄŸÄ±lÄ±mÄ±**: Rating deÄŸerlerinin istatistiksel Ã¶zeti
                                
                                Bu veriler, SVD (Singular Value Decomposition) algoritmasÄ± ile iÅŸlenecek ve 
                                kullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimlerinden latent faktÃ¶rler Ã§Ä±karÄ±lacaktÄ±r.
                                """)
                            
                            st.rerun()  # SayfayÄ± yenile
                else:
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            # Dosya bilgilerini kaydet
                            file_name = file.name
                            file_size = file.size
                            
                            try:
                                rating_matrix = load_rating_matrix_from_file(file)
                            except Exception as e:
                                st.error(f"âŒ Hata: {str(e)}")
                                st.info("""
                                **ğŸ’¡ Matrix Format iÃ§in:**
                                - Ä°lk sÃ¼tun kullanÄ±cÄ± ID'leri olmalÄ± (index)
                                - DiÄŸer sÃ¼tunlar Ã¼rÃ¼n ID'leri olmalÄ±
                                - DeÄŸerler rating'ler olmalÄ± (NaN = eksik veri)
                                - CSV dosyasÄ±nda ilk sÃ¼tun otomatik olarak index olarak okunur
                                
                                **Ã–rnek Format:**
                                ```
                                user_id,item_1,item_2,item_3,...
                                1,4.5,3.0,5.0,...
                                2,2.5,4.0,NaN,...
                                ```
                                """)
                                st.stop()
                            
                            # Dosya bilgilerini session state'e kaydet
                            st.session_state.svd_file_name = file_name
                            st.session_state.svd_file_size = file_size
                            
                            # Session state'e kaydet
                            st.session_state.svd_rating_matrix = rating_matrix
                            st.session_state.svd_user_mapping = None
                            st.session_state.svd_item_mapping = None
                            
                            # Veri istatistiklerini hesapla
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                                ratings_data = rating_matrix.data
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                                ratings_data = rating_matrix[mask]
                            
                            # Rating istatistikleri - boÅŸ array kontrolÃ¼
                            if len(ratings_data) == 0:
                                # EÄŸer hiÃ§ rating yoksa varsayÄ±lan deÄŸerler
                                min_rating = 0.0
                                max_rating = 0.0
                                mean_rating = 0.0
                                median_rating = 0.0
                                st.warning("âš ï¸ UyarÄ±: Dosyada hiÃ§ rating deÄŸeri bulunamadÄ±! TÃ¼m deÄŸerler NaN olabilir.")
                            else:
                                min_rating = float(np.min(ratings_data))
                                max_rating = float(np.max(ratings_data))
                                mean_rating = float(np.mean(ratings_data))
                                median_rating = float(np.median(ratings_data))
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            
                            # DetaylÄ± dosya analizi bÃ¶lÃ¼mÃ¼
                            with st.expander("ğŸ“‹ Dosya Analizi - KullanÄ±lan Veriler", expanded=True):
                                st.markdown("### ğŸ“ Dosya Bilgileri")
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Dosya AdÄ±", file_name)
                                    st.metric("Dosya Boyutu", f"{file_size / 1024:.2f} KB")
                                with col2:
                                    st.metric("Veri FormatÄ±", "Matrix Format")
                                    st.metric("Toplam Rating", f"{n_ratings:,}")
                                
                                st.markdown("### ğŸ“ˆ Veri Ä°statistikleri")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("KullanÄ±cÄ± SayÄ±sÄ±", f"{rating_matrix.shape[0]:,}")
                                with col2:
                                    st.metric("ÃœrÃ¼n SayÄ±sÄ±", f"{rating_matrix.shape[1]:,}")
                                with col3:
                                    st.metric("Toplam Rating", f"{n_ratings:,}")
                                with col4:
                                    st.metric("Veri YoÄŸunluÄŸu", f"{density:.2f}%")
                                
                                st.markdown("### â­ Rating DaÄŸÄ±lÄ±mÄ±")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("Minimum Rating", f"{min_rating:.2f}")
                                with col2:
                                    st.metric("Maksimum Rating", f"{max_rating:.2f}")
                                with col3:
                                    st.metric("Ortalama Rating", f"{mean_rating:.2f}")
                                with col4:
                                    st.metric("Medyan Rating", f"{median_rating:.2f}")
                                
                                st.markdown("""
                                **ğŸ“ AÃ§Ä±klama:**
                                - **Dosya Bilgileri**: YÃ¼klenen dosyanÄ±n adÄ± ve boyutu
                                - **Veri Ä°statistikleri**: Matris boyutlarÄ± ve veri yoÄŸunluÄŸu
                                - **Rating DaÄŸÄ±lÄ±mÄ±**: Rating deÄŸerlerinin istatistiksel Ã¶zeti
                                
                                Bu veriler, SVD (Singular Value Decomposition) algoritmasÄ± ile iÅŸlenecek ve 
                                kullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimlerinden latent faktÃ¶rler Ã§Ä±karÄ±lacaktÄ±r.
                                """)
                            
                            st.rerun()  # SayfayÄ± yenile
                            
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen veri formatÄ±nÄ± kontrol edin. Ã–rnek format iÃ§in yukarÄ±daki bilgi kutusuna bakÄ±n.")
    else:
        # Ã–rnek veri oluÅŸtur - varsayÄ±lan deÄŸerleri gÃ¼ncelle
        col1, col2, col3 = st.columns(3)
        with col1:
            n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 500, 100)
        with col2:
            n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 200, 50)
        with col3:
            n_components = st.slider("Tekil DeÄŸer SayÄ±sÄ±", 5, 50, 30, 
                                    help="Daha fazla bileÅŸen = daha Ã§eÅŸitli tahminler ama daha yavaÅŸ")
        
        sparsity = st.slider("Eksik Veri OranÄ±", 0.3, 0.9, 0.6,
                            help="Daha az eksik veri = daha iyi Ã¶ÄŸrenme")
    
    # Session state'ten gÃ¼ncel deÄŸerleri al
    rating_matrix = st.session_state.svd_rating_matrix
    
    # Model parametreleri (veri yÃ¼klendiyse)
    if rating_matrix is not None:
        n_users, n_items = rating_matrix.shape
        
        # Veri seti boÅŸ mu kontrol et
        if n_users == 0 or n_items == 0:
            st.error("âŒ Hata: Veri seti boÅŸ! LÃ¼tfen geÃ§erli bir veri dosyasÄ± yÃ¼kleyin.")
            n_components = 5  # VarsayÄ±lan deÄŸer
        else:
            # Optimal parametreleri al
            optimal_params = get_optimal_model_params("svd", data_shape=(n_users, n_items))
            optimal_n_components = optimal_params['n_components']
            
            max_components = min(50, min(n_users, n_items))
            min_components = min(5, max_components)  # min_value max_value'dan kÃ¼Ã§Ã¼k olmalÄ±
            default_components = max(min_components, min(max_components, optimal_n_components))
            
            # EÄŸer max_components Ã§ok kÃ¼Ã§Ã¼kse veya 0 ise, slider yerine sabit deÄŸer kullan
            if max_components <= 0 or max_components < min_components:
                n_components = max(1, max_components)  # En az 1 bileÅŸen
                if max_components <= 0:
                    st.error("âŒ Hata: Veri seti Ã§ok kÃ¼Ã§Ã¼k! BileÅŸen sayÄ±sÄ± ayarlanamÄ±yor.")
                else:
                    st.info(f"âš ï¸ Veri seti kÃ¼Ã§Ã¼k olduÄŸu iÃ§in bileÅŸen sayÄ±sÄ± otomatik olarak {n_components} olarak ayarlandÄ±.")
            else:
                n_components = st.slider(
                    "Tekil DeÄŸer SayÄ±sÄ±", 
                    min_components, 
                    max_components, 
                    default_components,
                    help=f"Ã–nerilen deÄŸer: {optimal_n_components} (veri boyutuna gÃ¶re otomatik hesaplandÄ±). Daha fazla bileÅŸen = daha Ã§eÅŸitli tahminler ama daha yavaÅŸ"
                )
                if n_components != optimal_n_components:
                    st.info(f"ğŸ’¡ Veri boyutunuza gÃ¶re Ã¶nerilen deÄŸer: {optimal_n_components}")
    
    if st.button("ğŸš€ Modeli EÄŸit"):
        if rating_matrix is None and data_source == "ğŸ“ Dosyadan YÃ¼kle":
            st.warning("âš ï¸ LÃ¼tfen Ã¶nce veri dosyasÄ±nÄ± yÃ¼kleyin!")
        else:
            with st.spinner("Model eÄŸitiliyor..."):
                # Veri yoksa oluÅŸtur
                if rating_matrix is None:
                    # sparsity deÄŸiÅŸkeni sadece Ã¶rnek veri iÃ§in tanÄ±mlÄ±
                    rating_matrix = generate_rating_matrix(
                        n_users=n_users, 
                        n_items=n_items, 
                        sparsity=sparsity
                    )
                
                # n_users ve n_items'Ä± gÃ¼ncelle (analiz seÃ§enekleri iÃ§in)
                n_users, n_items = rating_matrix.shape
            
            # Train-test split (sparse matrix desteÄŸi ile)
            from scipy.sparse import issparse
            
            np.random.seed(42)
            
            if issparse(rating_matrix):
                # Sparse matrix iÃ§in
                rows, cols = rating_matrix.nonzero()
                n_ratings = len(rows)
                test_size = min(int(0.2 * n_ratings), 10000)  # Max 10k test
                test_sample_indices = np.random.choice(n_ratings, size=test_size, replace=False)
                
                test_rows = rows[test_sample_indices]
                test_cols = cols[test_sample_indices]
                # Sparse matrix'ten deÄŸerleri al - matrix objesi iÃ§in np.array kullan
                test_matrix_slice = rating_matrix[test_rows, test_cols]
                # matrix objesi iÃ§in A property veya np.array kullan
                if hasattr(test_matrix_slice, 'A'):
                    test_values = test_matrix_slice.A.flatten()
                elif hasattr(test_matrix_slice, 'toarray'):
                    test_values = test_matrix_slice.toarray().flatten()
                else:
                    test_values = np.array(test_matrix_slice).flatten()
                
                # Train matrix - test deÄŸerlerini Ã§Ä±kar
                train_matrix = rating_matrix.copy()
                train_matrix[test_rows, test_cols] = 0
                train_matrix.eliminate_zeros()
                
                test_indices_tuple = (test_rows, test_cols)
            else:
                # Dense matrix iÃ§in
                mask = ~np.isnan(rating_matrix)
                n_ratings = np.sum(mask)
                test_size = min(int(0.2 * n_ratings), 10000)  # Max 10k test
                
                valid_indices = np.where(mask)
                test_sample_indices = np.random.choice(
                    len(valid_indices[0]), 
                    size=test_size, 
                    replace=False
                )
                
                test_mask = np.zeros_like(mask, dtype=bool)
                test_mask[valid_indices[0][test_sample_indices], valid_indices[1][test_sample_indices]] = True
                
                train_matrix = rating_matrix.copy()
                train_matrix[test_mask] = np.nan
                
                test_values = rating_matrix[test_mask]
                test_indices_tuple = np.where(test_mask)
            
            # Model eÄŸit (NaN deÄŸerleri ortalama ile doldur)
            svd_model = SVDRecommender(n_components=n_components)
            svd_model.fit(train_matrix, fill_na_with_mean=True)
            
            # DeÄŸerlendirme (bÃ¼yÃ¼k veri setleri iÃ§in optimize edilmiÅŸ)
            # Test deÄŸerlerini tahmin et - batch processing ile
            test_predictions = []
            batch_size = 1000  # Her seferde 1000 tahmin
            
            with st.spinner("Test seti Ã¼zerinde tahmin yapÄ±lÄ±yor..."):
                for i in range(0, len(test_indices_tuple[0]), batch_size):
                    batch_end = min(i + batch_size, len(test_indices_tuple[0]))
                    batch_users = test_indices_tuple[0][i:batch_end]
                    batch_items = test_indices_tuple[1][i:batch_end]
                    
                    batch_preds = [svd_model.predict(u, it) for u, it in zip(batch_users, batch_items)]
                    test_predictions.extend(batch_preds)
            
            test_predictions = np.array(test_predictions)
            
            # RMSE hesapla
            rmse = np.sqrt(mean_squared_error(test_values, test_predictions))
            singular_values = svd_model.get_singular_values()
            
            # Model ve sonuÃ§larÄ± session state'e kaydet (analiz seÃ§enekleri iÃ§in)
            st.session_state.svd_model_trained = True
            st.session_state.svd_model = svd_model
            st.session_state.svd_singular_values = singular_values
            st.session_state.svd_rating_matrix = rating_matrix
            st.session_state.svd_n_users = n_users
            st.session_state.svd_n_items = n_items
            st.session_state.svd_n_components = n_components
            
            # SonuÃ§lar
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Test RMSE", f"{rmse:.4f}")
            with col2:
                st.metric("KullanÄ±lan BileÅŸen", len(singular_values))
            with col3:
                variance_explained = np.sum(singular_values**2) / np.sum(singular_values**2)
                st.metric("Varyans AÃ§Ä±klama", f"{variance_explained:.2%}")
            
            # DetaylÄ± sonuÃ§ aÃ§Ä±klamasÄ±
            with st.expander("ğŸ“ SonuÃ§ AÃ§Ä±klamasÄ± - Ne Elde Edildi?", expanded=True):
                st.markdown("### ğŸ” KullanÄ±lan Veriler")
                if data_source == "ğŸ“ Dosyadan YÃ¼kle" and 'svd_file_name' in st.session_state:
                    st.info(f"""
                    **Dosya**: {st.session_state.svd_file_name} ({st.session_state.svd_file_size / 1024:.2f} KB)
                    - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                    - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                    - **Toplam Rating**: {n_ratings:,}
                    """)
                else:
                    st.info(f"""
                    **Ã–rnek Veri**:
                    - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                    - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                    """)
                
                st.markdown("### âš™ï¸ Model Parametreleri")
                st.info(f"""
                - **Tekil DeÄŸer SayÄ±sÄ± (n_components)**: {n_components}
                - **EÄŸitim Verisi**: {n_ratings - len(test_values):,} rating
                - **Test Verisi**: {len(test_values):,} rating
                """)
                
                st.markdown("### ğŸ“ˆ Elde Edilen SonuÃ§lar")
                st.success(f"""
                **Model BaÅŸarÄ±yla EÄŸitildi!**
                
                1. **Test RMSE**: {rmse:.4f}
                   - Bu deÄŸer ne kadar dÃ¼ÅŸÃ¼kse, model o kadar iyi tahmin yapÄ±yor demektir
                   - RMSE, gerÃ§ek rating'ler ile tahmin edilen rating'ler arasÄ±ndaki ortalama hata miktarÄ±nÄ± gÃ¶sterir
                   - Ã–rnek: RMSE = {rmse:.4f} â†’ Ortalama {rmse:.2f} puanlÄ±k hata var
                
                2. **KullanÄ±lan BileÅŸen SayÄ±sÄ±**: {len(singular_values)}
                   - Model, veriyi {len(singular_values)} boyutlu latent faktÃ¶r uzayÄ±na indirgedi
                   - Her bileÅŸen, kullanÄ±cÄ± ve Ã¼rÃ¼n Ã¶zelliklerini temsil eden bir boyuttur
                
                3. **Varyans AÃ§Ä±klama**: {variance_explained:.2%}
                   - Model, verideki bilginin {variance_explained:.1%}'ini koruyor
                   - YÃ¼ksek deÄŸer = daha az bilgi kaybÄ±
                
                4. **Tekil DeÄŸerler**: {len(singular_values)} adet
                   - Ä°lk birkaÃ§ tekil deÄŸer genellikle en Ã¶nemli bilgiyi taÅŸÄ±r
                   - DÃ¼ÅŸÃ¼k tekil deÄŸerler genellikle gÃ¼rÃ¼ltÃ¼yÃ¼ temsil eder
                """)
                
                st.markdown("### ğŸ¯ Ne YapÄ±ldÄ±?")
                st.markdown("""
                **SVD (Singular Value Decomposition) AlgoritmasÄ±** ÅŸu adÄ±mlarÄ± izledi:
                
                1. **Veri HazÄ±rlama**: Rating matrisi train ve test setlerine ayrÄ±ldÄ±
                2. **Matris FaktÃ¶rizasyonu**: Rating matrisi Ã¼Ã§ matrise ayrÄ±ldÄ±:
                   - **U**: KullanÄ±cÄ± latent faktÃ¶rleri
                   - **Î£**: Tekil deÄŸerler (bileÅŸen Ã¶nemleri)
                   - **V^T**: ÃœrÃ¼n latent faktÃ¶rleri
                3. **Boyut Ä°ndirgeme**: Sadece en Ã¶nemli {n_components} bileÅŸen kullanÄ±ldÄ±
                4. **Tahmin**: Eksik rating'ler, latent faktÃ¶rler kullanÄ±larak tahmin edildi
                5. **DeÄŸerlendirme**: Test seti Ã¼zerinde RMSE hesaplandÄ±
                
                **SonuÃ§**: Model, kullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimlerinden Ã¶ÄŸrendiÄŸi kalÄ±plarÄ± kullanarak 
                yeni rating'leri tahmin edebiliyor. Bu sayede kullanÄ±cÄ±lara henÃ¼z gÃ¶rmedikleri 
                Ã¼rÃ¼nler iÃ§in kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler sunulabilir.
                """)
                
                st.markdown("### ğŸ’¡ Sonraki AdÄ±mlar")
                st.info("""
                - **Ã–neriler**: AÅŸaÄŸÄ±daki "KullanÄ±cÄ± Ã–nerileri" bÃ¶lÃ¼mÃ¼nden belirli bir kullanÄ±cÄ± iÃ§in Ã¶neriler gÃ¶rebilirsiniz
                - **Tekil DeÄŸerler Analizi**: Grafikten optimal bileÅŸen sayÄ±sÄ±nÄ± belirleyebilirsiniz
                - **Rating Matrisi GÃ¶rselleÅŸtirmesi**: Veri yapÄ±sÄ±nÄ± gÃ¶rsel olarak inceleyebilirsiniz
                - **SVD Analiz SeÃ§enekleri**: Latent matrix, benzerlik analizleri ve tahmini puanlar
                """)
            
            # Tekil deÄŸerler grafiÄŸi
            st.subheader("ğŸ“ˆ Tekil DeÄŸerler Analizi")
            with st.expander("â„¹ï¸ Bu Grafik Ne Anlama Geliyor?", expanded=False):
                st.markdown("""
                **Tekil DeÄŸerler GrafiÄŸi**:
                - Her bileÅŸenin (component) Ã¶nemini gÃ¶sterir
                - **YÃ¼ksek deÄŸerler**: Daha Ã¶nemli, daha fazla bilgi taÅŸÄ±yan bileÅŸenler
                - **DÃ¼ÅŸÃ¼k deÄŸerler**: Daha az Ã¶nemli, gÃ¼rÃ¼ltÃ¼ iÃ§eren bileÅŸenler
                - Genellikle ilk birkaÃ§ bileÅŸen en Ã¶nemlidir (elbow point)
                - Bu grafik, optimal bileÅŸen sayÄ±sÄ±nÄ± seÃ§mek iÃ§in kullanÄ±lÄ±r
                """)
            
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.plot(range(1, len(singular_values) + 1), singular_values, 'o-', linewidth=2, markersize=6)
            ax.set_xlabel('BileÅŸen NumarasÄ±', fontsize=12)
            ax.set_ylabel('Tekil DeÄŸer (Singular Value)', fontsize=12)
            ax.set_title('Tekil DeÄŸerler - BileÅŸen Ã–nemi Analizi', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3)
            st.pyplot(fig)
    
    # SVD Analiz SeÃ§enekleri (Model eÄŸitildikten sonra her zaman gÃ¶ster)
    if st.session_state.get('svd_model_trained', False):
        st.subheader("ğŸ” SVD Analiz SeÃ§enekleri")
        analysis_option = st.radio(
            "Hangi analizi gÃ¶rmek istersiniz?",
            [
                "ğŸ¯ KullanÄ±cÄ± Ã–nerileri",
                "ğŸ“Š Latent Matrix (Gizli Ã–zellik SkorlarÄ±)",
                "ğŸ‘¥ Kaynak BenzerliÄŸi (KullanÄ±cÄ± BenzerliÄŸi)",
                "ğŸ“š Konu BenzerliÄŸi (ÃœrÃ¼n BenzerliÄŸi)",
                "ğŸ”® Tahmini Puanlar (Predicted Scores)",
                "ğŸ“ˆ Rating Matrisi GÃ¶rselleÅŸtirmesi"
            ],
            key="svd_analysis_option"
        )
        
        # Session state'ten deÄŸiÅŸkenleri al
        svd_model = st.session_state.svd_model
        singular_values = st.session_state.svd_singular_values
        rating_matrix = st.session_state.svd_rating_matrix
        n_users = st.session_state.svd_n_users
        n_items = st.session_state.svd_n_items
        n_components = st.session_state.svd_n_components
        
        # SeÃ§eneklere gÃ¶re iÃ§erik gÃ¶ster
        try:
                if analysis_option == "ğŸ¯ KullanÄ±cÄ± Ã–nerileri":
                    # Ã–neriler
                    st.subheader("ğŸ¯ KullanÄ±cÄ± Ã–nerileri")
                    with st.expander("â„¹ï¸ Ã–neriler NasÄ±l OluÅŸturuluyor?", expanded=False):
                        st.markdown("""
                        **Ã–neri Sistemi Ã‡alÄ±ÅŸma Prensibi**:
                        1. Model, kullanÄ±cÄ±nÄ±n geÃ§miÅŸ rating'lerini analiz eder
                        2. Latent faktÃ¶rler kullanarak kullanÄ±cÄ± tercihlerini Ã¶ÄŸrenir
                        3. Benzer kullanÄ±cÄ±larÄ±n beÄŸendiÄŸi Ã¼rÃ¼nleri bulur
                        4. KullanÄ±cÄ±nÄ±n henÃ¼z gÃ¶rmediÄŸi Ã¼rÃ¼nler iÃ§in rating tahmin eder
                        5. En yÃ¼ksek tahmin edilen rating'lere sahip Ã¼rÃ¼nleri Ã¶nerir
                        
                        **Tahmin Edilen Rating**: Modelin, kullanÄ±cÄ±nÄ±n bu Ã¼rÃ¼ne vereceÄŸi rating tahmini (1-5 arasÄ±)
                        """)
                    
                    user_idx = st.selectbox("KullanÄ±cÄ± SeÃ§in", range(min(10, n_users)), key="svd_user_select")
                    
                    predictions = svd_model.predict_all()[user_idx]
                    # Sparse matrix desteÄŸi
                    from scipy.sparse import issparse
                    if issparse(rating_matrix):
                        # Sparse matrix iÃ§in - sadece mevcut rating'leri kontrol et
                        user_row = rating_matrix.getrow(user_idx)
                        rated_items = np.zeros(rating_matrix.shape[1], dtype=bool)
                        rated_items[user_row.indices] = True
                    else:
                        rated_items = ~np.isnan(rating_matrix[user_idx])
                    unrated_items = ~rated_items
                    
                    # Debug: Tahmin Ã§eÅŸitliliÄŸini kontrol et
                    if np.sum(unrated_items) > 0:
                        unrated_predictions = predictions[unrated_items]
                        unique_predictions = len(np.unique(np.round(unrated_predictions, 2)))
                        prediction_range = (np.max(unrated_predictions) - np.min(unrated_predictions))
                        prediction_std = np.std(unrated_predictions)
                    else:
                        unique_predictions = len(np.unique(np.round(predictions, 2)))
                        prediction_range = (np.max(predictions) - np.min(predictions))
                        prediction_std = np.std(predictions)
                    
                    if unique_predictions < 3 or prediction_range < 0.5:
                        st.warning(f"âš ï¸ UyarÄ±: Tahminlerde Ã§ok az Ã§eÅŸitlilik var!")
                        st.info(f"""
                        ğŸ’¡ **Ä°pucular**:
                        - BileÅŸen sayÄ±sÄ±nÄ± artÄ±rÄ±n (ÅŸu an: {n_components})
                        - Daha fazla kullanÄ±cÄ±/Ã¼rÃ¼n kullanÄ±n
                        - FarklÄ± bir kullanÄ±cÄ± seÃ§meyi deneyin
                        
                        **Mevcut durum**: {unique_predictions} farklÄ± tahmin, aralÄ±k: {prediction_range:.2f}
                        """)
                    
                    predictions[rated_items] = -np.inf
                    
                    top_items = np.argsort(predictions)[::-1][:10]
                    top_ratings = predictions[top_items]
                    
                    # Rating'leri 1-5 aralÄ±ÄŸÄ±na sÄ±nÄ±rla ve yuvarla
                    top_ratings = np.clip(top_ratings, 1, 5)
                    
                    # Ã–neri tablosu
                    recommendations_df = pd.DataFrame({
                        'ÃœrÃ¼n ID': top_items + 1,
                        'Tahmin Edilen Rating': np.round(top_ratings, 2)
                    })
                    st.dataframe(recommendations_df, width='stretch')
                    
                    # Ä°statistikler
                    with st.expander("ğŸ“Š Tahmin Ä°statistikleri", expanded=False):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("FarklÄ± Tahmin SayÄ±sÄ±", unique_predictions)
                        with col2:
                            st.metric("Tahmin AralÄ±ÄŸÄ±", f"{prediction_range:.2f}")
                        with col3:
                            st.metric("Tahmin Std Sapma", f"{prediction_std:.2f}")
                
                elif analysis_option == "ğŸ“Š Latent Matrix (Gizli Ã–zellik SkorlarÄ±)":
                    st.subheader("ğŸ“Š Latent Matrix (Gizli Ã–zellik SkorlarÄ±)")
                    with st.expander("â„¹ï¸ Latent Matrix Nedir?", expanded=False):
                        st.markdown("""
                        **Latent Matrix** (Gizli Ã–zellik SkorlarÄ±):
                        - Her kaynaÄŸÄ±n (kullanÄ±cÄ±nÄ±n) gizli Ã¶zelliklerdeki skorlarÄ±nÄ± gÃ¶sterir
                        - `fit_transform()` ile oluÅŸturulan matristir
                        - Her satÄ±r bir kullanÄ±cÄ±yÄ±, her sÃ¼tun bir gizli Ã¶zelliÄŸi temsil eder
                        - Bu matris, kullanÄ±cÄ±larÄ±n latent space'deki konumunu gÃ¶sterir
                        - Benzer kullanÄ±cÄ±lar benzer skorlara sahip olacaktÄ±r
                        """)
                    
                    latent_matrix = svd_model.get_latent_matrix()
                    
                    # KullanÄ±cÄ± ID'leri
                    user_ids = [f"User_{i+1}" for i in range(latent_matrix.shape[0])]
                    component_ids = [f"Component_{i+1}" for i in range(latent_matrix.shape[1])]
                    
                    latent_df = pd.DataFrame(
                        latent_matrix,
                        index=user_ids,
                        columns=component_ids
                    )
                    
                    st.dataframe(latent_df, width='stretch', height=400)
                    
                    # Ä°statistikler
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("KullanÄ±cÄ± SayÄ±sÄ±", latent_matrix.shape[0])
                    with col2:
                        st.metric("Gizli Ã–zellik SayÄ±sÄ±", latent_matrix.shape[1])
                    with col3:
                        st.metric("Toplam DeÄŸer", f"{latent_matrix.size:,}")
                    
                    st.info("ğŸ’¡ Bu matris, her kullanÄ±cÄ±nÄ±n gizli Ã¶zelliklerdeki skorlarÄ±nÄ± gÃ¶sterir. Benzer kullanÄ±cÄ±lar benzer skorlara sahip olacaktÄ±r.")
                
                elif analysis_option == "ğŸ‘¥ Kaynak BenzerliÄŸi (KullanÄ±cÄ± BenzerliÄŸi)":
                    st.subheader("ğŸ‘¥ Kaynak BenzerliÄŸi (KullanÄ±cÄ± BenzerliÄŸi)")
                    with st.expander("â„¹ï¸ Kaynak BenzerliÄŸi Nedir?", expanded=False):
                        st.markdown("""
                        **Kaynak BenzerliÄŸi** (KullanÄ±cÄ± BenzerliÄŸi):
                        - Latent matrix Ã¼zerinden kosinÃ¼s benzerliÄŸi kullanÄ±larak hesaplanÄ±r
                        - Hangi kullanÄ±cÄ±larÄ±n benzer latent profillere sahip olduÄŸunu gÃ¶sterir
                        - Benzerlik deÄŸeri 1'e yakÄ±nsa kullanÄ±cÄ±lar Ã§ok benzerdir
                        - Benzerlik deÄŸeri 0'a yakÄ±nsa kullanÄ±cÄ±lar farklÄ±dÄ±r
                        - Bu bilgi, "benzer kullanÄ±cÄ±lar benzer Ã¼rÃ¼nleri beÄŸenir" prensibine dayanÄ±r
                        """)
                    
                    user_similarity = svd_model.get_user_similarity()
                    
                    # KullanÄ±cÄ± ID'leri
                    user_ids = [f"User_{i+1}" for i in range(user_similarity.shape[0])]
                    
                    similarity_df = pd.DataFrame(
                        user_similarity,
                        index=user_ids,
                        columns=user_ids
                    )
                    
                    st.dataframe(similarity_df, width='stretch', height=400)
                    
                    # En benzer kullanÄ±cÄ± Ã§iftleri
                    st.markdown("### ğŸ” En Benzer KullanÄ±cÄ± Ã‡iftleri")
                    # Diagonal'i -1 yap (kendisiyle benzerlik hariÃ§)
                    similarity_matrix_copy = user_similarity.copy()
                    np.fill_diagonal(similarity_matrix_copy, -1)
                    
                    # En yÃ¼ksek benzerlik deÄŸerlerini bul
                    n_top = min(10, len(user_ids))
                    top_similarities = []
                    for i in range(len(user_ids)):
                        for j in range(i+1, len(user_ids)):
                            top_similarities.append((i, j, similarity_matrix_copy[i, j]))
                    
                    top_similarities.sort(key=lambda x: x[2], reverse=True)
                    top_similarities = top_similarities[:n_top]
                    
                    top_sim_df = pd.DataFrame({
                        'KullanÄ±cÄ± 1': [f"User_{i+1}" for i, _, _ in top_similarities],
                        'KullanÄ±cÄ± 2': [f"User_{j+1}" for _, j, _ in top_similarities],
                        'Benzerlik Skoru': [f"{sim:.4f}" for _, _, sim in top_similarities]
                    })
                    st.dataframe(top_sim_df, width='stretch')
                    
                    # GÃ¶rselleÅŸtirme
                    st.markdown("### ğŸ“Š Benzerlik Matrisi GÃ¶rselleÅŸtirmesi")
                    fig, ax = plt.subplots(figsize=(10, 8))
                    im = ax.imshow(user_similarity, cmap='viridis', aspect='auto')
                    ax.set_xlabel('KullanÄ±cÄ±lar', fontsize=12)
                    ax.set_ylabel('KullanÄ±cÄ±lar', fontsize=12)
                    ax.set_title('KullanÄ±cÄ± Benzerlik Matrisi (KosinÃ¼s BenzerliÄŸi)', fontsize=14, fontweight='bold')
                    plt.colorbar(im, ax=ax, label='Benzerlik Skoru')
                    st.pyplot(fig)
                
                elif analysis_option == "ğŸ“š Konu BenzerliÄŸi (ÃœrÃ¼n BenzerliÄŸi)":
                    st.subheader("ğŸ“š Konu BenzerliÄŸi (ÃœrÃ¼n BenzerliÄŸi)")
                    with st.expander("â„¹ï¸ Konu BenzerliÄŸi Nedir?", expanded=False):
                        st.markdown("""
                        **Konu BenzerliÄŸi** (ÃœrÃ¼n BenzerliÄŸi):
                        - `svd.components_` Ã¼zerinden kosinÃ¼s benzerliÄŸi kullanÄ±larak hesaplanÄ±r
                        - Hangi Ã¼rÃ¼nlerin (konularÄ±n) benzer latent profillere sahip olduÄŸunu gÃ¶sterir
                        - Benzerlik deÄŸeri 1'e yakÄ±nsa Ã¼rÃ¼nler Ã§ok benzerdir
                        - Benzerlik deÄŸeri 0'a yakÄ±nsa Ã¼rÃ¼nler farklÄ±dÄ±r
                        - Bu bilgi, "benzer Ã¼rÃ¼nler benzer kullanÄ±cÄ±lar tarafÄ±ndan beÄŸenilir" prensibine dayanÄ±r
                        """)
                    
                    item_similarity = svd_model.get_item_similarity()
                    
                    # ÃœrÃ¼n ID'leri
                    item_ids = [f"Item_{i+1}" for i in range(item_similarity.shape[0])]
                    
                    similarity_df = pd.DataFrame(
                        item_similarity,
                        index=item_ids,
                        columns=item_ids
                    )
                    
                    st.dataframe(similarity_df, width='stretch', height=400)
                    
                    # En benzer Ã¼rÃ¼n Ã§iftleri
                    st.markdown("### ğŸ” En Benzer ÃœrÃ¼n Ã‡iftleri")
                    # Diagonal'i -1 yap (kendisiyle benzerlik hariÃ§)
                    similarity_matrix_copy = item_similarity.copy()
                    np.fill_diagonal(similarity_matrix_copy, -1)
                    
                    # En yÃ¼ksek benzerlik deÄŸerlerini bul
                    n_top = min(10, len(item_ids))
                    top_similarities = []
                    for i in range(len(item_ids)):
                        for j in range(i+1, len(item_ids)):
                            top_similarities.append((i, j, similarity_matrix_copy[i, j]))
                    
                    top_similarities.sort(key=lambda x: x[2], reverse=True)
                    top_similarities = top_similarities[:n_top]
                    
                    top_sim_df = pd.DataFrame({
                        'ÃœrÃ¼n 1': [f"Item_{i+1}" for i, _, _ in top_similarities],
                        'ÃœrÃ¼n 2': [f"Item_{j+1}" for _, j, _ in top_similarities],
                        'Benzerlik Skoru': [f"{sim:.4f}" for _, _, sim in top_similarities]
                    })
                    st.dataframe(top_sim_df, width='stretch')
                    
                    # GÃ¶rselleÅŸtirme
                    st.markdown("### ğŸ“Š Benzerlik Matrisi GÃ¶rselleÅŸtirmesi")
                    fig, ax = plt.subplots(figsize=(10, 8))
                    im = ax.imshow(item_similarity, cmap='viridis', aspect='auto')
                    ax.set_xlabel('ÃœrÃ¼nler', fontsize=12)
                    ax.set_ylabel('ÃœrÃ¼nler', fontsize=12)
                    ax.set_title('ÃœrÃ¼n Benzerlik Matrisi (KosinÃ¼s BenzerliÄŸi)', fontsize=14, fontweight='bold')
                    plt.colorbar(im, ax=ax, label='Benzerlik Skoru')
                    st.pyplot(fig)
                
                elif analysis_option == "ğŸ”® Tahmini Puanlar (Predicted Scores)":
                    st.subheader("ğŸ”® Tahmini Puanlar (Predicted Scores)")
                    with st.expander("â„¹ï¸ Tahmini Puanlar Nedir?", expanded=False):
                        st.markdown("""
                        **Tahmini Puanlar** (Predicted Scores):
                        - `latent_matrix @ svd.components_` ile yeniden oluÅŸturulan matristir
                        - Orijinalde NaN olan hÃ¼creler iÃ§in SVD modelinin tahmin ettiÄŸi puanlarÄ± iÃ§erir
                        - Bu matris, bir kaynaÄŸa henÃ¼z iÃ§erik Ã¼retmediÄŸi konular iÃ§in potansiyel puanlarÄ± gÃ¶sterir
                        - Orijinal rating'ler ile karÅŸÄ±laÅŸtÄ±rÄ±larak model performansÄ± deÄŸerlendirilebilir
                        """)
                    
                    # KullanÄ±cÄ± ve Ã¼rÃ¼n ID'leri
                    user_ids = [f"User_{i+1}" for i in range(n_users)]
                    item_ids = [f"Item_{i+1}" for i in range(n_items)]
                    
                    predicted_scores_df = svd_model.get_predicted_scores(
                        as_dataframe=True,
                        user_ids=user_ids,
                        item_ids=item_ids
                    )
                    
                    st.dataframe(predicted_scores_df, width='stretch', height=400)
                    
                    # Ä°statistikler
                    predicted_scores = svd_model.get_predicted_scores(as_dataframe=False)
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Minimum Tahmin", f"{np.min(predicted_scores):.2f}")
                    with col2:
                        st.metric("Maksimum Tahmin", f"{np.max(predicted_scores):.2f}")
                    with col3:
                        st.metric("Ortalama Tahmin", f"{np.mean(predicted_scores):.2f}")
                    with col4:
                        st.metric("Std Sapma", f"{np.std(predicted_scores):.2f}")
                    
                    # Orijinal matris ile karÅŸÄ±laÅŸtÄ±rma (eÄŸer varsa)
                    st.markdown("### ğŸ“Š Orijinal vs Tahmini Puanlar KarÅŸÄ±laÅŸtÄ±rmasÄ±")
                    from scipy.sparse import issparse
                    if issparse(rating_matrix):
                        original_ratings = rating_matrix.toarray()
                    else:
                        original_ratings = rating_matrix.copy()
                    
                    # Sadece mevcut rating'leri karÅŸÄ±laÅŸtÄ±r
                    mask = ~np.isnan(original_ratings)
                    if np.sum(mask) > 0:
                        original_values = original_ratings[mask]
                        predicted_values = predicted_scores[mask]
                        
                        comparison_df = pd.DataFrame({
                            'Orijinal Rating': original_values[:20],  # Ä°lk 20 Ã¶rnek
                            'Tahmini Rating': predicted_values[:20],
                            'Fark': np.abs(original_values[:20] - predicted_values[:20])
                        })
                        st.dataframe(comparison_df, width='stretch')
                        
                        # RMSE hesapla
                        rmse_comparison = np.sqrt(np.mean((original_values - predicted_values)**2))
                        st.metric("RMSE (Orijinal vs Tahmini)", f"{rmse_comparison:.4f}")
                
                elif analysis_option == "ğŸ“ˆ Rating Matrisi GÃ¶rselleÅŸtirmesi":
                    # Rating matrisi gÃ¶rselleÅŸtirme
                    st.subheader("ğŸ“Š Rating Matrisi GÃ¶rselleÅŸtirmesi")
                    with st.expander("â„¹ï¸ Rating Matrisi Ne GÃ¶steriyor?", expanded=False):
                        st.markdown("""
                        **Rating Matrisi**:
                        - **SatÄ±rlar**: KullanÄ±cÄ±lar
                        - **SÃ¼tunlar**: ÃœrÃ¼nler
                        - **Renkler**: Rating deÄŸerleri (koyu = dÃ¼ÅŸÃ¼k, aÃ§Ä±k = yÃ¼ksek)
                        - **Beyaz alanlar**: Eksik rating'ler (kullanÄ±cÄ± henÃ¼z deÄŸerlendirmemiÅŸ)
                        - Bu matris, Ã¶neri sisteminin temel girdisidir
                        - SVD, bu matrisi faktÃ¶rize ederek eksik deÄŸerleri tahmin eder
                        """)
                    
                    fig2 = plot_ratings_matrix(rating_matrix)
                    st.pyplot(fig2)
        except Exception as e:
            st.error(f"âŒ Analiz sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}")
            import traceback
            with st.expander("ğŸ” DetaylÄ± Hata MesajÄ±"):
                st.code(traceback.format_exc())
            st.info("ğŸ’¡ LÃ¼tfen modelin baÅŸarÄ±yla eÄŸitildiÄŸinden emin olun.")


def get_data_file_format_selector(model_name="model"):
    """
    Veri dosyasÄ± formatÄ± seÃ§ici (veri matrisi iÃ§in - rating matrisi deÄŸil)
    
    Args:
        model_name: Model adÄ± (key iÃ§in)
        
    Returns:
        SeÃ§ilen format string'i
    """
    format_options = [
        "ğŸ“Š Excel/CSV FormatÄ± (Her satÄ±r bir Ã¶rnek, her sÃ¼tun bir Ã¶zellik)",
        "ğŸ“‹ Matris FormatÄ± (Zaten matris formatÄ±nda)",
        "ğŸ“ Her Ä°kisi (Otomatik Tespit)"
    ]
    
    selected_format = st.radio(
        "ğŸ“„ Dosya FormatÄ± SeÃ§in",
        format_options,
        key=f"{model_name}_data_format",
        help="YÃ¼kleyeceÄŸiniz veri dosyasÄ±nÄ±n formatÄ±nÄ± seÃ§in"
    )
    
    # Format aÃ§Ä±klamasÄ±
    if selected_format.startswith("ğŸ“Š"):
        st.info("ğŸ’¡ **Excel/CSV FormatÄ±**: Her satÄ±r bir veri Ã¶rneÄŸi, her sÃ¼tun bir Ã¶zellik. SayÄ±sal veri olmalÄ±dÄ±r.")
    elif selected_format.startswith("ğŸ“‹"):
        st.info("ğŸ’¡ **Matris FormatÄ±**: Veri zaten matris formatÄ±nda (n_samples x n_features)")
    elif selected_format.startswith("ğŸ“"):
        st.info("ğŸ’¡ **Otomatik Tespit**: Sistem dosyayÄ± analiz edip uygun formatÄ± otomatik seÃ§er")
    
    return selected_format


def show_svd_noise_reduction():
    """SVD gÃ¼rÃ¼ltÃ¼ temizleme"""
    st.header("ğŸ”‡ SVD - GÃ¼rÃ¼ltÃ¼ Temizleme")
    
    # Info bÃ¶lÃ¼mÃ¼
    with st.expander("â„¹ï¸ SVD GÃ¼rÃ¼ltÃ¼ Temizleme HakkÄ±nda Bilgi", expanded=False):
        st.markdown("""
        ### SVD ile GÃ¼rÃ¼ltÃ¼ Temizleme NasÄ±l Ã‡alÄ±ÅŸÄ±r?
        
        **Prensip**: SVD, veriyi Ã¶nemli bileÅŸenlere ve gÃ¼rÃ¼ltÃ¼ye ayÄ±rÄ±r.
        
        ### AdÄ±mlar:
        
        1. **SVD Uygulama**: Veri matrisini tekil deÄŸerlerine ayÄ±rÄ±r
        2. **BileÅŸen SeÃ§imi**: Sadece Ã¶nemli bileÅŸenleri tutar (yÃ¼ksek tekil deÄŸerler)
        3. **GÃ¼rÃ¼ltÃ¼ Filtreleme**: DÃ¼ÅŸÃ¼k tekil deÄŸerli bileÅŸenleri atar (gÃ¼rÃ¼ltÃ¼)
        4. **Yeniden OluÅŸturma**: SeÃ§ilen bileÅŸenlerle temiz veriyi yeniden oluÅŸturur
        
        ### Varyans EÅŸiÄŸi
        
        - **0.95 (95%)**: Verinin %95'ini korur, %5 gÃ¼rÃ¼ltÃ¼yÃ¼ temizler
        - **0.90 (90%)**: Daha agresif temizleme, daha fazla gÃ¼rÃ¼ltÃ¼ kaldÄ±rÄ±r
        - **0.99 (99%)**: Ã‡ok az temizleme, neredeyse tÃ¼m veriyi korur
        
        ### KullanÄ±m AlanlarÄ±
        
        - GÃ¶rÃ¼ntÃ¼ gÃ¼rÃ¼ltÃ¼ temizleme
        - Sinyal iÅŸleme
        - Veri Ã¶n iÅŸleme
        - Ã–zellik Ã§Ä±karÄ±mÄ±
        """)
    
    st.markdown("""
    SVD kullanarak veri gÃ¼rÃ¼ltÃ¼sÃ¼nÃ¼ temizleme.
    """)
    
    col1, col2 = st.columns(2)
    with col1:
        n_samples = st.slider("Ã–rnek SayÄ±sÄ±", 100, 500, 200)
        n_features = st.slider("Ã–zellik SayÄ±sÄ±", 50, 200, 100)
    with col2:
        noise_level = st.slider("GÃ¼rÃ¼ltÃ¼ Seviyesi", 0.05, 0.5, 0.2)
        threshold = st.slider("Varyans EÅŸiÄŸi", 0.8, 0.99, 0.95)
    
    if st.button("GÃ¼rÃ¼ltÃ¼ Temizle"):
        with st.spinner("GÃ¼rÃ¼ltÃ¼ temizleniyor..."):
            # Veri oluÅŸtur
            X, _ = generate_sample_data(n_samples=n_samples, n_features=n_features)
            X_noisy = generate_noisy_data(X, noise_level=noise_level)
            
            # GÃ¼rÃ¼ltÃ¼ temizle
            noise_reducer = SVDNoiseReducer(n_components=None, threshold=threshold)
            noise_reducer.fit(X_noisy)
            X_denoised = noise_reducer.denoise(X_noisy)
            
            # Metrikler
            mse_original = np.mean((X - X_noisy)**2)
            mse_denoised = np.mean((X - X_denoised)**2)
            improvement = ((mse_original - mse_denoised) / mse_original * 100)
            noise_reduction_ratio = noise_reducer.get_noise_reduction_ratio()
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("GÃ¼rÃ¼ltÃ¼ MSE", f"{mse_original:.6f}")
            with col2:
                st.metric("TemizlenmiÅŸ MSE", f"{mse_denoised:.6f}")
            with col3:
                st.metric("Ä°yileÅŸtirme", f"{improvement:.2f}%")
            with col4:
                st.metric("Varyans Korunma", f"{noise_reduction_ratio:.2%}")
            
            # GÃ¶rselleÅŸtirme
            st.subheader("ğŸ“Š GÃ¼rÃ¼ltÃ¼ Temizleme SonuÃ§larÄ±")
            with st.expander("â„¹ï¸ Bu GÃ¶rselleÅŸtirme Ne GÃ¶steriyor?", expanded=False):
                st.markdown("""
                **ÃœÃ§ Panel KarÅŸÄ±laÅŸtÄ±rmasÄ±**:
                
                1. **Orijinal Veri**: Temiz, gÃ¼rÃ¼ltÃ¼sÃ¼z orijinal veri
                2. **GÃ¼rÃ¼ltÃ¼lÃ¼ Veri**: Rastgele gÃ¼rÃ¼ltÃ¼ eklenmiÅŸ veri (gerÃ§ek dÃ¼nya senaryosu)
                3. **TemizlenmiÅŸ Veri**: SVD ile gÃ¼rÃ¼ltÃ¼ temizlenmiÅŸ veri
                
                **Renk HaritasÄ± (Viridis)**:
                - Koyu renkler: DÃ¼ÅŸÃ¼k deÄŸerler
                - AÃ§Ä±k renkler: YÃ¼ksek deÄŸerler
                - DÃ¼zgÃ¼n geÃ§iÅŸler: Temiz veri
                - Rastgele noktalar: GÃ¼rÃ¼ltÃ¼
                
                **KarÅŸÄ±laÅŸtÄ±rma**: TemizlenmiÅŸ veri, orijinal veriye ne kadar yakÄ±nsa, algoritma o kadar baÅŸarÄ±lÄ±dÄ±r.
                """)
            
            fig, axes = plt.subplots(1, 3, figsize=(18, 5))
            
            # Orijinal
            im1 = axes[0].imshow(X[:50, :50], cmap='viridis', aspect='auto')
            axes[0].set_title('Orijinal Veri (Temiz)', fontsize=12, fontweight='bold')
            plt.colorbar(im1, ax=axes[0])
            
            # GÃ¼rÃ¼ltÃ¼lÃ¼
            im2 = axes[1].imshow(X_noisy[:50, :50], cmap='viridis', aspect='auto')
            axes[1].set_title('GÃ¼rÃ¼ltÃ¼lÃ¼ Veri', fontsize=12, fontweight='bold')
            plt.colorbar(im2, ax=axes[1])
            
            # TemizlenmiÅŸ
            im3 = axes[2].imshow(X_denoised[:50, :50], cmap='viridis', aspect='auto')
            axes[2].set_title('TemizlenmiÅŸ Veri (SVD ile)', fontsize=12, fontweight='bold')
            plt.colorbar(im3, ax=axes[2])
            
            st.pyplot(fig)
            
            # Varyans analizi
            st.subheader("ğŸ“ˆ Varyans Korunma Analizi")
            with st.expander("â„¹ï¸ Varyans Analizi Ne Anlama Geliyor?", expanded=False):
                st.markdown("""
                **KÃ¼mÃ¼latif Varyans GrafiÄŸi**:
                - **X Ekseni**: KullanÄ±lan bileÅŸen sayÄ±sÄ±
                - **Y Ekseni**: Korunan veri varyansÄ± oranÄ± (0-1 arasÄ±)
                - **KÄ±rmÄ±zÄ± Ã‡izgi**: SeÃ§ilen eÅŸik deÄŸeri (Ã¶rn: %95)
                - **YeÅŸil Ã‡izgi**: Otomatik seÃ§ilen optimal bileÅŸen sayÄ±sÄ±
                
                **Yorumlama**:
                - Ä°lk birkaÃ§ bileÅŸen genellikle varyansÄ±n Ã§oÄŸunu aÃ§Ä±klar
                - EÄŸri yataylaÅŸtÄ±ÄŸÄ±nda, ek bileÅŸenler Ã§ok az bilgi ekler
                - Optimal nokta: EÅŸik deÄŸerine ulaÅŸan minimum bileÅŸen sayÄ±sÄ±
                - Daha fazla bileÅŸen = daha az gÃ¼rÃ¼ltÃ¼ temizleme ama daha yavaÅŸ iÅŸlem
                """)
            
            component_counts, variance_ratios = noise_reducer.get_optimal_components(X_noisy)
            
            fig2, ax = plt.subplots(figsize=(10, 5))
            ax.plot(component_counts, variance_ratios, 'o-', linewidth=2, markersize=6)
            ax.axhline(y=threshold, color='r', linestyle='--', linewidth=2, label=f'{threshold:.0%} EÅŸiÄŸi')
            ax.axvline(x=noise_reducer.n_components, color='g', 
                      linestyle='--', linewidth=2, label=f'SeÃ§ilen ({noise_reducer.n_components})')
            ax.set_xlabel('BileÅŸen SayÄ±sÄ±', fontsize=12)
            ax.set_ylabel('KÃ¼mÃ¼latif Varyans OranÄ±', fontsize=12)
            ax.set_title('Varyans Korunma Analizi - Optimal BileÅŸen SeÃ§imi', fontsize=14, fontweight='bold')
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
            st.pyplot(fig2)


def show_pca_visualization():
    """PCA gÃ¶rselleÅŸtirme"""
    st.header("ğŸ“ˆ PCA - Veri GÃ¶rselleÅŸtirme")
    
    # Info bÃ¶lÃ¼mÃ¼
    with st.expander("â„¹ï¸ PCA (Principal Component Analysis) HakkÄ±nda Bilgi", expanded=False):
        st.markdown("""
        ### PCA Nedir?
        
        **PCA**, yÃ¼ksek boyutlu veriyi daha dÃ¼ÅŸÃ¼k boyutlu bir uzaya izdÃ¼ÅŸÃ¼ren bir boyut azaltma tekniÄŸidir.
        
        ### NasÄ±l Ã‡alÄ±ÅŸÄ±r?
        
        1. **Kovaryans Matrisi**: Veri Ã¶zellikleri arasÄ±ndaki iliÅŸkileri hesaplar
        2. **Ã–zvektÃ¶rler**: Verinin ana yÃ¶nlerini (principal components) bulur
        3. **Ã–zdeÄŸerler**: Her bileÅŸenin ne kadar varyans aÃ§Ä±kladÄ±ÄŸÄ±nÄ± gÃ¶sterir
        4. **Ä°zdÃ¼ÅŸÃ¼m**: Veriyi yeni dÃ¼ÅŸÃ¼k boyutlu uzaya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        
        ### KullanÄ±m AlanlarÄ±
        
        - **Veri GÃ¶rselleÅŸtirme**: YÃ¼ksek boyutlu veriyi 2D/3D'de gÃ¶rselleÅŸtirme
        - **Ã–zellik SeÃ§imi**: En Ã¶nemli Ã¶zellikleri belirleme
        - **Boyut Azaltma**: GÃ¼rÃ¼ltÃ¼yÃ¼ azaltma ve hesaplama hÄ±zÄ±nÄ± artÄ±rma
        - **Ã–n Ä°ÅŸleme**: Makine Ã¶ÄŸrenmesi modelleri iÃ§in veri hazÄ±rlama
        
        ### Metrikler
        
        - **AÃ§Ä±klanan Varyans**: Her bileÅŸenin verinin ne kadarÄ±nÄ± aÃ§Ä±kladÄ±ÄŸÄ±
        - **KÃ¼mÃ¼latif Varyans**: Ä°lk N bileÅŸenin toplam aÃ§Ä±kladÄ±ÄŸÄ± varyans
        - **%95 Varyans KuralÄ±**: Verinin %95'ini aÃ§Ä±klayan minimum bileÅŸen sayÄ±sÄ±
        """)
    
    st.markdown("""
    PCA kullanarak veri gÃ¶rselleÅŸtirme ve Ã¶zellik seÃ§imi.
    """)
    
    # Optimal parametreleri al
    optimal_params = get_optimal_model_params("pca", n_samples=500, n_features=50)
    optimal_n_components = optimal_params['n_components']
    
    col1, col2 = st.columns(2)
    with col1:
        n_samples = st.slider("Ã–rnek SayÄ±sÄ±", 100, 1000, 500)
        n_features = st.slider("Ã–zellik SayÄ±sÄ±", 20, 100, 50)
    with col2:
        n_clusters = st.slider("KÃ¼me SayÄ±sÄ±", 2, 10, 5)
        n_components = st.slider(
            "GÃ¶sterilecek BileÅŸen", 
            5, 30, 
            optimal_n_components,
            help=f"Ã–nerilen deÄŸer: {optimal_n_components}"
        )
    
    if st.button("PCA Uygula"):
        with st.spinner("PCA uygulanÄ±yor..."):
            # Veri oluÅŸtur
            X, y = generate_sample_data(
                n_samples=n_samples, 
                n_features=n_features, 
                n_clusters=n_clusters
            )
            
            # PCA
            pca = PCAAnalyzer(n_components=None)
            X_transformed = pca.fit_transform(X)
            
            # Metrikler
            n_95 = pca.get_optimal_components(X, variance_threshold=0.95)
            variance_first_5 = np.sum(pca.explained_variance_ratio_[:5])
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Toplam BileÅŸen", len(pca.explained_variance_ratio_))
            with col2:
                st.metric("%95 Varyans iÃ§in BileÅŸen", n_95)
            with col3:
                st.metric("Ä°lk 5 BileÅŸen VaryansÄ±", f"{variance_first_5:.2%}")
            
            # AÃ§Ä±klanan varyans grafiÄŸi
            st.subheader("ğŸ“Š AÃ§Ä±klanan Varyans Analizi")
            with st.expander("â„¹ï¸ Bu Grafikler Ne Anlama Geliyor?", expanded=False):
                st.markdown("""
                **Sol Grafik - Bireysel Varyans**:
                - Her bileÅŸenin (PC) ne kadar varyans aÃ§Ä±kladÄ±ÄŸÄ±nÄ± gÃ¶sterir
                - Ä°lk birkaÃ§ bileÅŸen genellikle en yÃ¼ksek varyansa sahiptir
                - "Elbow" noktasÄ±: Optimal bileÅŸen sayÄ±sÄ±nÄ± belirler
                
                **SaÄŸ Grafik - KÃ¼mÃ¼latif Varyans**:
                - Ä°lk N bileÅŸenin toplam aÃ§Ä±kladÄ±ÄŸÄ± varyans
                - %95 eÅŸiÄŸi: Verinin %95'ini korumak iÃ§in gerekli bileÅŸen sayÄ±sÄ±
                - EÄŸri yataylaÅŸtÄ±ÄŸÄ±nda, ek bileÅŸenler az bilgi ekler
                """)
            
            fig = pca.plot_explained_variance(n_components=n_components)
            st.pyplot(fig)
            
            # 2D ve 3D izdÃ¼ÅŸÃ¼m
            st.subheader("ğŸ¨ Veri GÃ¶rselleÅŸtirme (2D & 3D)")
            with st.expander("â„¹ï¸ Ä°zdÃ¼ÅŸÃ¼m Grafikleri Ne GÃ¶steriyor?", expanded=False):
                st.markdown("""
                **2D Ä°zdÃ¼ÅŸÃ¼m (PC1 vs PC2)**:
                - YÃ¼ksek boyutlu veriyi 2 boyuta indirger
                - Renkler: FarklÄ± sÄ±nÄ±flarÄ±/kÃ¼meleri gÃ¶sterir
                - YakÄ±n noktalar: Benzer veri Ã¶rnekleri
                - AyrÄ±k gruplar: FarklÄ± sÄ±nÄ±flar/kÃ¼meler
                
                **3D Ä°zdÃ¼ÅŸÃ¼m (PC1 vs PC2 vs PC3)**:
                - Daha fazla bilgi korur (3 boyut)
                - Daha iyi ayrÄ±m saÄŸlar
                - Ä°nteraktif olarak dÃ¶ndÃ¼rÃ¼lebilir
                
                **Yorumlama**:
                - Ä°yi ayrÄ±lmÄ±ÅŸ gruplar = PCA baÅŸarÄ±lÄ±
                - KarÄ±ÅŸÄ±k noktalar = Veri karmaÅŸÄ±k veya daha fazla bileÅŸen gerekli
                """)
            
            col1, col2 = st.columns(2)
            with col1:
                fig2 = pca.plot_2d_projection(X, y=y)
                st.pyplot(fig2)
            
            # 3D izdÃ¼ÅŸÃ¼m
            with col2:
                fig3 = pca.plot_3d_projection(X, y=y)
                st.pyplot(fig3)
            
            # Ã–zellik Ã¶nemi
            st.subheader("ğŸ” Ã–zellik Ã–nemi Analizi")
            with st.expander("â„¹ï¸ Ã–zellik Ã–nemi Ne Anlama Geliyor?", expanded=False):
                st.markdown("""
                **Ã–zellik Ã–nemi SkorlarÄ±**:
                - Her Ã¶zelliÄŸin PCA bileÅŸenlerindeki katkÄ±sÄ±nÄ± gÃ¶sterir
                - **YÃ¼ksek skor**: Ã–zellik, verinin varyansÄ±nÄ± aÃ§Ä±klamada Ã¶nemli
                - **DÃ¼ÅŸÃ¼k skor**: Ã–zellik daha az Ã¶nemli veya gÃ¼rÃ¼ltÃ¼lÃ¼
                
                **KullanÄ±m**:
                - Ã–zellik seÃ§imi: YÃ¼ksek skorlu Ã¶zellikleri tut
                - GÃ¼rÃ¼ltÃ¼ temizleme: DÃ¼ÅŸÃ¼k skorlu Ã¶zellikleri kaldÄ±r
                - Model optimizasyonu: Ã–nemli Ã¶zellikleri Ã¶nceliklendir
                """)
            
            feature_importance = pca.get_feature_importance()
            top_features = np.argsort(feature_importance)[-20:][::-1]
            
            fig4, ax = plt.subplots(figsize=(10, 6))
            ax.barh(range(len(top_features)), feature_importance[top_features], alpha=0.7)
            ax.set_yticks(range(len(top_features)))
            ax.set_yticklabels([f'Ã–zellik {i+1}' for i in top_features])
            ax.set_xlabel('Ã–nem Skoru', fontsize=12)
            ax.set_title('En Ã–nemli 20 Ã–zellik - PCA Ã–zellik Ã–nemi', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3, axis='x')
            st.pyplot(fig4)


def show_nmf_image_processing():
    """NMF gÃ¶rÃ¼ntÃ¼ iÅŸleme"""
    st.header("ğŸ–¼ï¸ NMF - GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme")
    
    # Info bÃ¶lÃ¼mÃ¼
    with st.expander("â„¹ï¸ NMF GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme HakkÄ±nda Bilgi", expanded=False):
        st.markdown("""
        ### NMF (Non-negative Matrix Factorization) Nedir?
        
        **NMF**, matrisleri sadece **pozitif deÄŸerlerle** faktÃ¶rize eden bir yÃ¶ntemdir.
        
        ### GÃ¶rÃ¼ntÃ¼ Ä°ÅŸlemede KullanÄ±mÄ±
        
        1. **Basis Images (Temel GÃ¶rÃ¼ntÃ¼ler)**: GÃ¶rÃ¼ntÃ¼lerin temel yapÄ± taÅŸlarÄ±nÄ± bulur
        2. **KatsayÄ± Matrisi**: Her gÃ¶rÃ¼ntÃ¼nÃ¼n bu temel yapÄ± taÅŸlarÄ±nÄ± nasÄ±l kullandÄ±ÄŸÄ±nÄ± gÃ¶sterir
        3. **Yeniden OluÅŸturma**: Temel gÃ¶rÃ¼ntÃ¼ler ve katsayÄ±larla orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ yeniden oluÅŸturur
        
        ### AvantajlarÄ±
        
        - **Pozitif DeÄŸerler**: GÃ¶rÃ¼ntÃ¼ piksel deÄŸerleri doÄŸal olarak pozitiftir
        - **Yorumlanabilirlik**: Temel gÃ¶rÃ¼ntÃ¼ler anlamlÄ± pattern'ler iÃ§erir
        - **SÄ±kÄ±ÅŸtÄ±rma**: Az sayÄ±da temel gÃ¶rÃ¼ntÃ¼ ile Ã§ok gÃ¶rÃ¼ntÃ¼yÃ¼ temsil edebilir
        - **GÃ¼rÃ¼ltÃ¼ Azaltma**: DÃ¼ÅŸÃ¼k rank yaklaÅŸÄ±mÄ± gÃ¼rÃ¼ltÃ¼yÃ¼ filtreler
        
        ### SÄ±kÄ±ÅŸtÄ±rma OranÄ±
        
        - **YÃ¼ksek oran**: Daha fazla sÄ±kÄ±ÅŸtÄ±rma, daha az kalite
        - **DÃ¼ÅŸÃ¼k oran**: Daha az sÄ±kÄ±ÅŸtÄ±rma, daha yÃ¼ksek kalite
        - Optimal: Kalite ve boyut arasÄ±nda denge
        """)
    
    # Veri kaynaÄŸÄ± seÃ§imi
    data_source = st.radio(
        "Veri KaynaÄŸÄ± SeÃ§in",
        ["ğŸ“ GÃ¶rÃ¼ntÃ¼ DosyasÄ± YÃ¼kle", "ğŸ² Ã–rnek GÃ¶rÃ¼ntÃ¼ler Kullan"],
        horizontal=True
    )
    
    # Dosya formatÄ± seÃ§imi (sadece gÃ¶rÃ¼ntÃ¼ yÃ¼kleme seÃ§ildiyse)
    if data_source == "ğŸ“ GÃ¶rÃ¼ntÃ¼ DosyasÄ± YÃ¼kle":
        file_format = get_file_format_selector("nmf_image", include_image=True)
        if not file_format.startswith("ğŸ–¼ï¸"):
            st.warning("âš ï¸ Bu model gÃ¶rÃ¼ntÃ¼ iÅŸleme iÃ§in tasarlanmÄ±ÅŸtÄ±r. LÃ¼tfen fotoÄŸraf/gÃ¶rÃ¼ntÃ¼ formatÄ±nÄ± seÃ§in.")
    
    uploaded_file = None
    images_flat = None
    image_shape = None
    img_array = None  # Orijinal gÃ¶rÃ¼ntÃ¼ array'i (gÃ¶rselleÅŸtirme iÃ§in)
    
    if data_source == "ğŸ“ GÃ¶rÃ¼ntÃ¼ DosyasÄ± YÃ¼kle":
        st.markdown("### ğŸ“¤ GÃ¶rÃ¼ntÃ¼ DosyasÄ± YÃ¼kle")
        
        # Dosya formatÄ± kontrolÃ¼
        if file_format and file_format.startswith("ğŸ–¼ï¸"):
            uploaded_file = st.file_uploader(
                "GÃ¶rÃ¼ntÃ¼ dosyasÄ± seÃ§in (JPG, PNG, BMP)",
                type=['jpg', 'jpeg', 'png', 'bmp'],
                help="YÃ¼klediÄŸiniz gÃ¶rÃ¼ntÃ¼ analiz edilecek ve NMF ile temel bileÅŸenlere ayrÄ±lacaktÄ±r."
            )
        else:
            uploaded_file = None
            st.info("ğŸ’¡ LÃ¼tfen yukarÄ±da 'FotoÄŸraf/GÃ¶rÃ¼ntÃ¼' formatÄ±nÄ± seÃ§in.")
        
        if uploaded_file is not None:
            try:
                from PIL import Image
                import io
                
                # Dosya iÃ§eriÄŸini oku (bir kez okunabilir, bu yÃ¼zden sakla)
                file_bytes = uploaded_file.read()
                uploaded_file.seek(0)  # Reset file pointer
                
                # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle
                image = Image.open(io.BytesIO(file_bytes))
                
                # GÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶ster
                st.image(image, caption="YÃ¼klenen GÃ¶rÃ¼ntÃ¼", use_container_width=True)
                
                # GÃ¶rÃ¼ntÃ¼ bilgileri
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("GeniÅŸlik", f"{image.width} px")
                with col2:
                    st.metric("YÃ¼kseklik", f"{image.height} px")
                with col3:
                    st.metric("Mod", image.mode)
                
                # GÃ¶rÃ¼ntÃ¼yÃ¼ gri tonlamaya Ã§evir ve normalize et
                if image.mode != 'L':
                    image = image.convert('L')
                
                # NumPy array'e Ã§evir
                img_array = np.array(image, dtype=np.float32)
                img_array = img_array / 255.0  # Normalize et [0, 1]
                
                # GÃ¶rÃ¼ntÃ¼yÃ¼ tek bir gÃ¶rÃ¼ntÃ¼ olarak iÅŸle (1 gÃ¶rÃ¼ntÃ¼, dÃ¼zleÅŸtirilmiÅŸ)
                image_shape = img_array.shape
                images_flat = img_array.flatten().reshape(1, -1)
                
                st.success(f"âœ… GÃ¶rÃ¼ntÃ¼ yÃ¼klendi! Boyut: {image_shape}")
                
            except Exception as e:
                st.error(f"âŒ GÃ¶rÃ¼ntÃ¼ yÃ¼klenirken hata oluÅŸtu: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen geÃ§erli bir gÃ¶rÃ¼ntÃ¼ dosyasÄ± yÃ¼klediÄŸinizden emin olun.")
    
    else:
        st.markdown("### ğŸ² Ã–rnek GÃ¶rÃ¼ntÃ¼ler")
        col1, col2 = st.columns(2)
        # Optimal parametreleri al
        optimal_params = get_optimal_model_params("nmf_image")
        optimal_n_components = optimal_params['n_components']
        
        with col1:
            n_images = st.slider("GÃ¶rÃ¼ntÃ¼ SayÄ±sÄ±", 50, 200, 100)
        with col2:
            n_components = st.slider(
                "BileÅŸen SayÄ±sÄ±", 
                5, 50, 
                optimal_n_components,
                help=f"Ã–nerilen deÄŸer: {optimal_n_components}"
            )
    
    # BileÅŸen sayÄ±sÄ± slider'Ä± (gÃ¶rÃ¼ntÃ¼ yÃ¼klendiyse gÃ¶ster)
    if data_source == "ğŸ“ GÃ¶rÃ¼ntÃ¼ DosyasÄ± YÃ¼kle" and images_flat is not None:
        n_components = st.slider("BileÅŸen SayÄ±sÄ±", 5, 100, 20, key="uploaded_components")
    
    # Analiz butonu
    analyze_button = st.button("ğŸ” GÃ¶rÃ¼ntÃ¼yÃ¼ Analiz Et" if data_source == "ğŸ“ GÃ¶rÃ¼ntÃ¼ DosyasÄ± YÃ¼kle" else "ğŸ² Ã–rnek GÃ¶rÃ¼ntÃ¼leri Ä°ÅŸle")
    
    if analyze_button:
        if data_source == "ğŸ“ GÃ¶rÃ¼ntÃ¼ DosyasÄ± YÃ¼kle":
            if uploaded_file is None:
                st.warning("âš ï¸ LÃ¼tfen Ã¶nce bir gÃ¶rÃ¼ntÃ¼ dosyasÄ± yÃ¼kleyin!")
            elif images_flat is not None and image_shape is not None and img_array is not None:
                
                with st.spinner("GÃ¶rÃ¼ntÃ¼ analiz ediliyor..."):
                    try:
                        # NMF
                        nmf_image = NMFImageProcessor(n_components=n_components)
                        nmf_image.fit(images_flat)
                        
                        # Yeniden oluÅŸtur
                        reconstructed = nmf_image.reconstruct()
                        basis_images = nmf_image.get_basis_images(image_shape)
                        compression_ratio = nmf_image.get_compression_ratio(image_shape)
                        
                        # Metrikler
                        mse = np.mean((images_flat - reconstructed)**2)
                        psnr = 20 * np.log10(1.0 / (np.sqrt(mse) + 1e-10)) if mse > 0 else float('inf')
                        
                        # SonuÃ§larÄ± gÃ¶ster
                        st.subheader("ğŸ“Š Analiz SonuÃ§larÄ±")
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("SÄ±kÄ±ÅŸtÄ±rma OranÄ±", f"{compression_ratio:.2f}x")
                        with col2:
                            st.metric("MSE", f"{mse:.6f}")
                        with col3:
                            st.metric("PSNR (dB)", f"{psnr:.2f}")
                        with col4:
                            st.metric("Basis GÃ¶rÃ¼ntÃ¼ SayÄ±sÄ±", len(basis_images))
                        
                        # GÃ¶rselleÅŸtirme
                        st.subheader("ğŸ–¼ï¸ GÃ¶rÃ¼ntÃ¼ Analizi")
                        with st.expander("â„¹ï¸ Analiz SonuÃ§larÄ± HakkÄ±nda", expanded=False):
                            st.markdown("""
                            **Analiz SonuÃ§larÄ±**:
                            
                            1. **Orijinal GÃ¶rÃ¼ntÃ¼**: YÃ¼klediÄŸiniz gÃ¶rÃ¼ntÃ¼
                            2. **Temel GÃ¶rÃ¼ntÃ¼ler (Basis Images)**: NMF'nin bulduÄŸu temel yapÄ± taÅŸlarÄ±
                               - Her temel gÃ¶rÃ¼ntÃ¼, gÃ¶rÃ¼ntÃ¼nÃ¼zdeki ortak pattern'leri temsil eder
                               - Ã–rnek: Kenarlar, kÃ¶ÅŸeler, dairesel ÅŸekiller, dokular vb.
                            3. **Yeniden OluÅŸturulmuÅŸ GÃ¶rÃ¼ntÃ¼**: Temel gÃ¶rÃ¼ntÃ¼ler kullanÄ±larak yeniden oluÅŸturulan gÃ¶rÃ¼ntÃ¼
                               - Orijinal gÃ¶rÃ¼ntÃ¼ye ne kadar yakÄ±nsa, analiz o kadar baÅŸarÄ±lÄ±
                            
                            **Metrikler**:
                            - **MSE (Mean Squared Error)**: DÃ¼ÅŸÃ¼k = Daha iyi kalite
                            - **PSNR (Peak Signal-to-Noise Ratio)**: YÃ¼ksek = Daha iyi kalite
                            - **SÄ±kÄ±ÅŸtÄ±rma OranÄ±**: Orijinal boyut / SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ boyut
                            """)
                        
                        # Orijinal gÃ¶rÃ¼ntÃ¼
                        st.markdown("### ğŸ“¸ Orijinal GÃ¶rÃ¼ntÃ¼")
                        fig_orig, ax_orig = plt.subplots(figsize=(8, 8))
                        ax_orig.imshow(img_array, cmap='gray')
                        ax_orig.set_title("Orijinal GÃ¶rÃ¼ntÃ¼", fontsize=14, fontweight='bold')
                        ax_orig.axis('off')
                        st.pyplot(fig_orig)
                        
                        # Temel gÃ¶rÃ¼ntÃ¼ler
                        st.markdown("### ğŸ§© Temel GÃ¶rÃ¼ntÃ¼ler (Basis Images)")
                        with st.expander("â„¹ï¸ Temel GÃ¶rÃ¼ntÃ¼ler HakkÄ±nda", expanded=False):
                            st.markdown("""
                            **Temel GÃ¶rÃ¼ntÃ¼ler (Basis Images)**:
                            - NMF algoritmasÄ±nÄ±n Ã¶ÄŸrendiÄŸi temel yapÄ± taÅŸlarÄ±dÄ±r
                            - GÃ¶rÃ¼ntÃ¼nÃ¼z, bu temel gÃ¶rÃ¼ntÃ¼lerin bir kombinasyonudur
                            - SayÄ±larÄ± bileÅŸen sayÄ±sÄ±na eÅŸittir (Ã¶rn: 20 bileÅŸen = 20 temel gÃ¶rÃ¼ntÃ¼)
                            - AnlamlÄ± pattern'ler iÃ§erir (kenarlar, ÅŸekiller, dokular)
                            """)
                        
                        basis_flat = basis_images.reshape(len(basis_images), -1)
                        fig_basis = plot_image_grid(basis_flat, image_shape, n_cols=5)
                        st.pyplot(fig_basis)
                        
                        # Yeniden oluÅŸturulmuÅŸ gÃ¶rÃ¼ntÃ¼
                        st.markdown("### ğŸ”„ Yeniden OluÅŸturulmuÅŸ GÃ¶rÃ¼ntÃ¼")
                        reconstructed_img = reconstructed[0].reshape(image_shape)
                        fig_recon, ax_recon = plt.subplots(1, 2, figsize=(16, 8))
                        
                        ax_recon[0].imshow(img_array, cmap='gray')
                        ax_recon[0].set_title("Orijinal", fontsize=14, fontweight='bold')
                        ax_recon[0].axis('off')
                        
                        ax_recon[1].imshow(reconstructed_img, cmap='gray')
                        ax_recon[1].set_title(f"Yeniden OluÅŸturulmuÅŸ (MSE: {mse:.6f})", fontsize=14, fontweight='bold')
                        ax_recon[1].axis('off')
                        
                        st.pyplot(fig_recon)
                        
                        # Fark gÃ¶rÃ¼ntÃ¼sÃ¼
                        st.markdown("### ğŸ” Fark GÃ¶rÃ¼ntÃ¼sÃ¼ (Orijinal - Yeniden OluÅŸturulmuÅŸ)")
                        diff_img = np.abs(img_array - reconstructed_img)
                        fig_diff, ax_diff = plt.subplots(figsize=(8, 8))
                        im = ax_diff.imshow(diff_img, cmap='hot')
                        ax_diff.set_title("Fark GÃ¶rÃ¼ntÃ¼sÃ¼ (KÄ±rmÄ±zÄ± = Daha Fazla Fark)", fontsize=14, fontweight='bold')
                        ax_diff.axis('off')
                        plt.colorbar(im, ax=ax_diff, label='Fark DeÄŸeri')
                        st.pyplot(fig_diff)
                        
                        st.success("âœ… Analiz tamamlandÄ±!")
                        
                    except Exception as e:
                        st.error(f"âŒ Analiz sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
        else:
            # Ã–rnek gÃ¶rÃ¼ntÃ¼ler
            with st.spinner("GÃ¶rÃ¼ntÃ¼ler iÅŸleniyor..."):
                # GÃ¶rÃ¼ntÃ¼ yÃ¼kle
                images_flat, image_shape = load_sample_images(n_images=n_images)
                
                # NMF
                nmf_image = NMFImageProcessor(n_components=n_components)
                nmf_image.fit(images_flat)
                
                # Yeniden oluÅŸtur
                reconstructed = nmf_image.reconstruct()
                basis_images = nmf_image.get_basis_images(image_shape)
                compression_ratio = nmf_image.get_compression_ratio(image_shape)
                
                # Metrikler
                mse = np.mean((images_flat - reconstructed)**2)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("SÄ±kÄ±ÅŸtÄ±rma OranÄ±", f"{compression_ratio:.2f}x")
                with col2:
                    st.metric("MSE", f"{mse:.6f}")
                with col3:
                    st.metric("Basis GÃ¶rÃ¼ntÃ¼ SayÄ±sÄ±", len(basis_images))
                
                # GÃ¶rselleÅŸtirme
                st.subheader("ğŸ–¼ï¸ GÃ¶rÃ¼ntÃ¼ KarÅŸÄ±laÅŸtÄ±rmasÄ±")
                with st.expander("â„¹ï¸ Bu GÃ¶rÃ¼ntÃ¼ler Ne GÃ¶steriyor?", expanded=False):
                    st.markdown("""
                    **ÃœÃ§ GÃ¶rÃ¼ntÃ¼ Seti**:
                    
                    1. **Orijinal GÃ¶rÃ¼ntÃ¼ler**: Ä°ÅŸlenmemiÅŸ, orijinal gÃ¶rÃ¼ntÃ¼ler
                    2. **Temel GÃ¶rÃ¼ntÃ¼ler (Basis Images)**: NMF'nin bulduÄŸu temel yapÄ± taÅŸlarÄ±
                       - Her temel gÃ¶rÃ¼ntÃ¼, gÃ¶rÃ¼ntÃ¼ koleksiyonundaki ortak pattern'leri temsil eder
                       - Ã–rnek: Kenarlar, kÃ¶ÅŸeler, dairesel ÅŸekiller vb.
                    3. **Yeniden OluÅŸturulmuÅŸ GÃ¶rÃ¼ntÃ¼ler**: Temel gÃ¶rÃ¼ntÃ¼ler kullanÄ±larak yeniden oluÅŸturulan gÃ¶rÃ¼ntÃ¼ler
                       - Orijinal gÃ¶rÃ¼ntÃ¼ye ne kadar yakÄ±nsa, sÄ±kÄ±ÅŸtÄ±rma o kadar baÅŸarÄ±lÄ±
                    
                    **Kalite DeÄŸerlendirmesi**:
                    - Ä°yi yeniden oluÅŸturma: Orijinal ve yeniden oluÅŸturulmuÅŸ gÃ¶rÃ¼ntÃ¼ler benzer
                    - DÃ¼ÅŸÃ¼k MSE: Daha yÃ¼ksek kalite
                    - YÃ¼ksek sÄ±kÄ±ÅŸtÄ±rma oranÄ±: Daha kÃ¼Ã§Ã¼k dosya boyutu
                    """)
                
                st.subheader("ğŸ“¸ Orijinal GÃ¶rÃ¼ntÃ¼ler (Ä°lk 10)")
                fig1 = plot_image_grid(images_flat[:10], image_shape, n_cols=5)
                st.pyplot(fig1)
                
                st.subheader("ğŸ§© Temel GÃ¶rÃ¼ntÃ¼ler (Basis Images)")
                with st.expander("â„¹ï¸ Temel GÃ¶rÃ¼ntÃ¼ler HakkÄ±nda", expanded=False):
                    st.markdown("""
                    **Temel GÃ¶rÃ¼ntÃ¼ler (Basis Images)**:
                    - NMF algoritmasÄ±nÄ±n Ã¶ÄŸrendiÄŸi temel yapÄ± taÅŸlarÄ±dÄ±r
                    - Her gÃ¶rÃ¼ntÃ¼, bu temel gÃ¶rÃ¼ntÃ¼lerin bir kombinasyonudur
                    - SayÄ±larÄ± bileÅŸen sayÄ±sÄ±na eÅŸittir (Ã¶rn: 20 bileÅŸen = 20 temel gÃ¶rÃ¼ntÃ¼)
                    - AnlamlÄ± pattern'ler iÃ§erir (kenarlar, ÅŸekiller, dokular)
                    """)
                
                basis_flat = basis_images.reshape(len(basis_images), -1)
                fig2 = plot_image_grid(basis_flat, image_shape, n_cols=5)
                st.pyplot(fig2)
                
                st.subheader("ğŸ”„ Yeniden OluÅŸturulmuÅŸ GÃ¶rÃ¼ntÃ¼ler (Ä°lk 10)")
                fig3 = plot_image_grid(reconstructed[:10], image_shape, n_cols=5)
                st.pyplot(fig3)


def extract_text_from_file(uploaded_file):
    """
    YÃ¼klenen dosyadan metin Ã§Ä±karÄ±r
    
    Args:
        uploaded_file: Streamlit file_uploader'dan gelen dosya
        
    Returns:
        Metin iÃ§eriÄŸi (string listesi - her satÄ±r bir dokÃ¼man)
    """
    import io
    
    file_name = uploaded_file.name.lower()
    file_content = uploaded_file.read()
    
    documents = []
    
    try:
        if file_name.endswith(('.txt', '.text')):
            # TXT dosyasÄ±
            text = file_content.decode('utf-8', errors='ignore')
            # Paragraflara bÃ¶l (boÅŸ satÄ±rlarla ayrÄ±lmÄ±ÅŸ)
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            if not paragraphs:
                # EÄŸer paragraf yoksa, satÄ±rlara bÃ¶l
                paragraphs = [line.strip() for line in text.split('\n') if line.strip()]
            documents = paragraphs
            
        elif file_name.endswith(('.docx', '.doc')):
            # Word dosyasÄ±
            try:
                from docx import Document
                doc = Document(io.BytesIO(file_content))
                
                # Paragraflardan metin Ã§Ä±kar
                paragraphs = []
                for p in doc.paragraphs:
                    text = p.text.strip()
                    if text:
                        paragraphs.append(text)
                
                # Tablolardan da metin Ã§Ä±kar
                for table in doc.tables:
                    for row in table.rows:
                        row_text = ' '.join([cell.text.strip() for cell in row.cells if cell.text.strip()])
                        if row_text:
                            paragraphs.append(row_text)
                
                # BaÅŸlÄ±klar ve listeler de dahil
                if paragraphs:
                    documents = paragraphs
                else:
                    raise ValueError("Word dosyasÄ±ndan hiÃ§ metin Ã§Ä±karÄ±lamadÄ±. Dosya boÅŸ olabilir.")
                    
            except ImportError:
                raise ImportError("Word dosyalarÄ± iÃ§in 'python-docx' kÃ¼tÃ¼phanesi gerekli. YÃ¼klemek iÃ§in: pip install python-docx")
            except Exception as e:
                if file_name.endswith('.doc'):
                    raise ValueError(
                        f"Eski .doc formatÄ±ndaki dosya okunamadÄ±. "
                        f"LÃ¼tfen dosyanÄ±zÄ± .docx formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n (Word'de 'FarklÄ± Kaydet' ile .docx olarak kaydedin). "
                        f"Hata detayÄ±: {str(e)}"
                    )
                else:
                    raise ValueError(f"Word dosyasÄ± okunamadÄ±: {str(e)}")
        
        elif file_name.endswith('.pdf'):
            # PDF dosyasÄ±
            try:
                import PyPDF2
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
                paragraphs = []
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text.strip():
                        # SayfayÄ± paragraflara bÃ¶l
                        page_paragraphs = [p.strip() for p in page_text.split('\n\n') if p.strip()]
                        paragraphs.extend(page_paragraphs)
                documents = paragraphs if paragraphs else [pdf_reader.pages[0].extract_text()]
            except ImportError:
                try:
                    import pdfplumber
                    with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                        paragraphs = []
                        for page in pdf.pages:
                            page_text = page.extract_text()
                            if page_text and page_text.strip():
                                page_paragraphs = [p.strip() for p in page_text.split('\n\n') if p.strip()]
                                paragraphs.extend(page_paragraphs)
                        documents = paragraphs if paragraphs else []
                except ImportError:
                    raise ImportError("PDF dosyalarÄ± iÃ§in 'PyPDF2' veya 'pdfplumber' kÃ¼tÃ¼phanesi gerekli. YÃ¼klemek iÃ§in: pip install PyPDF2 veya pip install pdfplumber")
        
        elif file_name.endswith('.csv'):
            # CSV dosyasÄ±
            # FarklÄ± delimiter'larÄ± dene
            delimiters = [',', ';', '\t']
            df = None
            for delimiter in delimiters:
                try:
                    file_bytes = io.BytesIO(file_content)
                    df = pd.read_csv(file_bytes, delimiter=delimiter, encoding='utf-8', on_bad_lines='skip')
                    if len(df.columns) > 1:  # En az 2 sÃ¼tun varsa doÄŸru delimiter bulunmuÅŸ
                        break
                except:
                    continue
            
            if df is None or df.empty:
                # Son Ã§are olarak varsayÄ±lan ayarlarla dene
                file_bytes = io.BytesIO(file_content)
                df = pd.read_csv(file_bytes, encoding='utf-8', on_bad_lines='skip')
            
            # Her satÄ±rÄ± bir dokÃ¼man olarak al (tÃ¼m sÃ¼tunlarÄ± birleÅŸtir)
            documents = []
            for idx, row in df.iterrows():
                # TÃ¼m sÃ¼tunlarÄ± birleÅŸtir
                row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])
                if row_text.strip():
                    documents.append(row_text.strip())
        
        elif file_name.endswith(('.xlsx', '.xls')):
            # Excel dosyasÄ±
            df = pd.read_excel(io.BytesIO(file_content))
            # Her satÄ±rÄ± bir dokÃ¼man olarak al (tÃ¼m sÃ¼tunlarÄ± birleÅŸtir)
            documents = []
            for idx, row in df.iterrows():
                # TÃ¼m sÃ¼tunlarÄ± birleÅŸtir
                row_text = ' '.join([str(val) for val in row.values if pd.notna(val) and str(val).strip()])
                if row_text.strip():
                    documents.append(row_text.strip())
        
        else:
            raise ValueError(f"Desteklenmeyen dosya formatÄ±: {file_name}")
        
        # BoÅŸ dokÃ¼manlarÄ± filtrele
        documents = [doc for doc in documents if doc and len(doc.strip()) > 10]  # En az 10 karakter
        
        if not documents:
            raise ValueError("Dosyadan hiÃ§ metin Ã§Ä±karÄ±lamadÄ±! LÃ¼tfen geÃ§erli bir metin iÃ§eren dosya yÃ¼kleyin.")
        
        return documents
        
    except Exception as e:
        raise Exception(f"Dosya okunurken hata oluÅŸtu: {str(e)}")


def show_nmf_topic_modeling():
    """NMF topic modeling"""
    st.header("ğŸ“ NMF - Topic Modeling")
    
    # Info bÃ¶lÃ¼mÃ¼
    with st.expander("â„¹ï¸ NMF Topic Modeling HakkÄ±nda Bilgi", expanded=False):
        st.markdown("""
        ### Topic Modeling Nedir?
        
        **Topic Modeling**, metin dokÃ¼manlarÄ±nda gizli konularÄ± (topic) keÅŸfetme tekniÄŸidir.
        
        ### NMF ile Topic Modeling
        
        1. **TF-IDF VektÃ¶rizasyon**: Metinleri sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        2. **NMF FaktÃ¶rizasyon**: DokÃ¼man-kelime matrisini faktÃ¶rize eder
        3. **Topic Ã‡Ä±karÄ±mÄ±**: Her topic iÃ§in Ã¶nemli kelimeleri bulur
        4. **DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ±**: Her dokÃ¼manÄ±n hangi topic'lere ait olduÄŸunu gÃ¶sterir
        
        ### Ã‡Ä±ktÄ±lar
        
        - **Topic'ler**: Her topic iÃ§in en Ã¶nemli kelimeler
        - **Topic TutarlÄ±lÄ±ÄŸÄ±**: Topic'lerin ne kadar tutarlÄ± olduÄŸu
        - **DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ±**: Her dokÃ¼manÄ±n topic skorlarÄ±
        - **Yeni DokÃ¼man Tahmini**: Yeni bir metin iÃ§in en uygun topic
        
        ### KullanÄ±m AlanlarÄ±
        
        - Haber kategorilendirme
        - MÃ¼ÅŸteri yorumlarÄ± analizi
        - AraÅŸtÄ±rma makaleleri sÄ±nÄ±flandÄ±rma
        - Sosyal medya iÃ§erik analizi
        """)
    
    # Veri kaynaÄŸÄ± seÃ§imi
    data_source = st.radio(
        "Veri KaynaÄŸÄ± SeÃ§in",
        ["ğŸ“ Dosya YÃ¼kle (Word/PDF/Excel/CSV/TXT)", "ğŸ² Ã–rnek Metinler Kullan"],
        horizontal=True
    )
    
    documents = None
    uploaded_file = None
    
    if data_source == "ğŸ“ Dosya YÃ¼kle (Word/PDF/Excel/CSV/TXT)":
        st.markdown("### ğŸ“¤ Dosya YÃ¼kle")
        
        # Bilgilendirme
        st.info("""
        **ğŸ“ Desteklenen Dosya FormatlarÄ±:**
        - **Word**: .docx (Ã¶nerilen), .doc (eski format - .docx'e dÃ¶nÃ¼ÅŸtÃ¼rmeniz Ã¶nerilir)
        - **PDF**: .pdf
        - **Excel**: .xlsx, .xls
        - **Metin**: .txt, .csv
        
        **ğŸ’¡ Ä°pucu:** Her paragraf ayrÄ± bir dokÃ¼man olarak iÅŸlenecektir.
        """)
        
        uploaded_file = st.file_uploader(
            "Dosya seÃ§in (Word, PDF, Excel, CSV, TXT)",
            type=['xlsx', 'xls', 'csv', 'txt', 'docx', 'doc', 'pdf'],
            help="YÃ¼klediÄŸiniz dosyadan metin Ã§Ä±karÄ±lacak ve topic modeling yapÄ±lacaktÄ±r.",
            key="nmf_topic_file_uploader"
        )
        
        if uploaded_file is not None:
            try:
                with st.spinner("Dosya okunuyor..."):
                    documents = extract_text_from_file(uploaded_file)
                    
                st.success(f"âœ… Dosya yÃ¼klendi! {len(documents)} dokÃ¼man bulundu.")
                
                # Dosya Ã¶nizleme
                with st.expander("ğŸ“„ Dosya Ã–nizleme (Ä°lk 3 DokÃ¼man)", expanded=False):
                    for i, doc in enumerate(documents[:3]):
                        st.markdown(f"**DokÃ¼man {i+1}:**")
                        st.text(doc[:500] + "..." if len(doc) > 500 else doc)
                        st.markdown("---")
                
                # Ä°statistikler
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Toplam DokÃ¼man", len(documents))
                with col2:
                    avg_length = np.mean([len(doc.split()) for doc in documents])
                    st.metric("Ortalama Kelime SayÄ±sÄ±", f"{avg_length:.0f}")
                with col3:
                    total_words = sum([len(doc.split()) for doc in documents])
                    st.metric("Toplam Kelime", total_words)
                    
            except ImportError as e:
                st.error(f"âŒ Gerekli kÃ¼tÃ¼phane eksik: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen gerekli kÃ¼tÃ¼phaneyi yÃ¼kleyin.")
            except Exception as e:
                st.error(f"âŒ Dosya okunurken hata oluÅŸtu: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen geÃ§erli bir dosya yÃ¼klediÄŸinizden emin olun.")
    
    else:
        st.markdown("### ğŸ² Ã–rnek Metinler")
        col1, col2 = st.columns(2)
        with col1:
            n_documents = st.slider("DokÃ¼man SayÄ±sÄ±", 50, 300, 200)
        with col2:
            n_topics = st.slider("Topic SayÄ±sÄ±", 3, 10, 5, key="example_topics")
        
        max_features = st.slider("Maksimum Kelime SayÄ±sÄ±", 200, 1000, 500, key="example_features")
    
    # Topic sayÄ±sÄ± ve parametreler (dosya yÃ¼klendiyse)
    if data_source == "ğŸ“ Dosya YÃ¼kle (Word/PDF/Excel/CSV/TXT)" and documents is not None:
        # Optimal parametreleri al (dokÃ¼man sayÄ±sÄ±na gÃ¶re)
        optimal_params = get_optimal_model_params("nmf_topic")
        optimal_n_topics = optimal_params['n_topics']
        optimal_max_features = optimal_params['max_features']
        
        col1, col2 = st.columns(2)
        with col1:
            n_topics = st.slider(
                "Topic SayÄ±sÄ±", 
                3, 15, 
                optimal_n_topics, 
                key="file_topics",
                help=f"Ã–nerilen deÄŸer: {optimal_n_topics}"
            )
        with col2:
            max_features = st.slider(
                "Maksimum Kelime SayÄ±sÄ±", 
                200, 2000, 
                optimal_max_features, 
                key="file_features",
                help=f"Ã–nerilen deÄŸer: {optimal_max_features}"
            )
    
    # Analiz butonu
    analyze_button = st.button("ğŸ” Topic'leri Bul" if data_source == "ğŸ“ Dosya YÃ¼kle (Word/PDF/Excel/CSV/TXT)" else "ğŸ² Topic'leri Bul")
    
    if analyze_button:
        if data_source == "ğŸ“ Dosya YÃ¼kle (Word/PDF/Excel/CSV/TXT)":
            if uploaded_file is None or documents is None:
                st.warning("âš ï¸ LÃ¼tfen Ã¶nce bir dosya yÃ¼kleyin!")
            elif len(documents) < 3:
                st.warning("âš ï¸ Yeterli dokÃ¼man bulunamadÄ±! En az 3 dokÃ¼man gerekli.")
            else:
                with st.spinner("Topic'ler bulunuyor..."):
                    try:
                        # NMF
                        nmf_model = NMFTopicModeler(n_topics=n_topics, max_iter=200)
                        nmf_model.fit(documents, max_features=max_features, min_df=2, max_df=0.95)
                        
                        # Topic'ler
                        topics = nmf_model.get_topics(n_words=10)
                        coherence = nmf_model.get_topic_coherence()
                        
                        # SonuÃ§larÄ± gÃ¶ster
                        st.subheader("ğŸ“Š Analiz SonuÃ§larÄ±")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Topic TutarlÄ±lÄ±ÄŸÄ±", f"{coherence:.4f}")
                        with col2:
                            st.metric("Toplam DokÃ¼man", len(documents))
                        with col3:
                            st.metric("Bulunan Topic SayÄ±sÄ±", n_topics)
                        
                        # Topic'leri gÃ¶ster
                        st.subheader("ğŸ¯ Topic'ler ve Anahtar Kelimeler")
                        for topic_name, words_scores in topics.items():
                            with st.expander(f"{topic_name} - {', '.join([w[0] for w in words_scores[:5]])}"):
                                words_df = pd.DataFrame(words_scores, columns=['Kelime', 'Ã–nem Skoru'])
                                st.dataframe(words_df, width='stretch', use_container_width=True)
                        
                        # GÃ¶rselleÅŸtirme
                        st.subheader("ğŸ“Š Topic Kelime Analizi")
                        with st.expander("â„¹ï¸ Topic Kelime Grafikleri Ne GÃ¶steriyor?", expanded=False):
                            st.markdown("""
                            **Topic Kelime Grafikleri**:
                            - Her topic iÃ§in en Ã¶nemli kelimeleri gÃ¶sterir
                            - **Y Ekseni**: Kelimeler (Ã¶nem sÄ±rasÄ±na gÃ¶re)
                            - **X Ekseni**: Ã–nem skoru (topic iÃ§indeki aÄŸÄ±rlÄ±k)
                            - **YÃ¼ksek skor**: Kelime, topic'i tanÄ±mlamada Ã§ok Ã¶nemli
                            - **DÃ¼ÅŸÃ¼k skor**: Kelime daha az Ã¶nemli
                            
                            **Yorumlama**:
                            - Ä°yi topic: TutarlÄ±, anlamlÄ± kelimeler
                            - KÃ¶tÃ¼ topic: Rastgele, tutarsÄ±z kelimeler
                            - Topic tutarlÄ±lÄ±ÄŸÄ±: TÃ¼m topic'lerin ortalama kalitesi
                            """)
                        
                        fig = plot_topic_words(topics, n_words=10)
                        st.pyplot(fig)
                        
                        # DokÃ¼man-topic daÄŸÄ±lÄ±mÄ±
                        doc_topics = nmf_model.get_document_topics()
                        
                        st.subheader("ğŸ”¥ DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ± Heatmap")
                        with st.expander("â„¹ï¸ Heatmap Ne GÃ¶steriyor?", expanded=False):
                            st.markdown("""
                            **DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ± Heatmap**:
                            - **SatÄ±rlar**: Topic'ler
                            - **SÃ¼tunlar**: DokÃ¼manlar
                            - **Renkler**: DokÃ¼manÄ±n topic'e ait olma skoru
                              - **Koyu renk (dÃ¼ÅŸÃ¼k)**: DokÃ¼man bu topic'e az ait
                              - **AÃ§Ä±k renk (yÃ¼ksek)**: DokÃ¼man bu topic'e Ã§ok ait
                            
                            **Yorumlama**:
                            - Her dokÃ¼man genellikle 1-2 dominant topic'e sahiptir
                            - Koyu sÃ¼tunlar: Belirsiz dokÃ¼manlar (birden fazla topic)
                            - AÃ§Ä±k sÃ¼tunlar: Net topic'lere sahip dokÃ¼manlar
                            - Dikey Ã§izgiler: AynÄ± topic'teki dokÃ¼man gruplarÄ±
                            """)
                        
                        # Heatmap iÃ§in maksimum dokÃ¼man sayÄ±sÄ±
                        max_docs_heatmap = min(50, len(documents))
                        fig2, ax = plt.subplots(figsize=(12, 8))
                        sns.heatmap(doc_topics[:max_docs_heatmap].T, cmap='YlOrRd', ax=ax, 
                                   yticklabels=[f'Topic {i+1}' for i in range(n_topics)],
                                   xticklabels=[f'Doc {i+1}' for i in range(max_docs_heatmap)])
                        ax.set_xlabel('DokÃ¼manlar', fontsize=12)
                        ax.set_ylabel('Topic\'ler', fontsize=12)
                        ax.set_title(f'DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ± (Ä°lk {max_docs_heatmap} DokÃ¼man)', fontsize=14, fontweight='bold')
                        st.pyplot(fig2)
                        
                        # Her dokÃ¼man iÃ§in dominant topic
                        st.subheader("ğŸ“‹ DokÃ¼man-Topic EÅŸleÅŸtirmesi")
                        dominant_topics = np.argmax(doc_topics, axis=1)
                        topic_scores = np.max(doc_topics, axis=1)
                        
                        doc_topic_df = pd.DataFrame({
                            'DokÃ¼man': [f'DokÃ¼man {i+1}' for i in range(len(documents))],
                            'Dominant Topic': [f'Topic {t+1}' for t in dominant_topics],
                            'Topic Skoru': topic_scores,
                            'Ã–nizleme': [doc[:100] + "..." if len(doc) > 100 else doc for doc in documents]
                        })
                        
                        st.dataframe(doc_topic_df, width='stretch', use_container_width=True, height=400)
                        
                        # Yeni dokÃ¼man tahmini
                        st.subheader("ğŸ”® Yeni DokÃ¼man iÃ§in Topic Tahmini")
                        new_doc = st.text_area("DokÃ¼man metni girin:", 
                                               value="",
                                               height=100,
                                               placeholder="Analiz etmek istediÄŸiniz metni buraya yapÄ±ÅŸtÄ±rÄ±n...")
                        if new_doc and new_doc.strip():
                            top_topic, all_scores = nmf_model.predict_topic(new_doc)
                            st.write(f"**En uygun topic:** Topic {top_topic+1} (skor: {all_scores[top_topic]:.3f})")
                            
                            scores_df = pd.DataFrame({
                                'Topic': [f'Topic {i+1}' for i in range(n_topics)],
                                'Skor': all_scores
                            })
                            st.bar_chart(scores_df.set_index('Topic'))
                        
                        st.success("âœ… Analiz tamamlandÄ±!")
                        
                    except Exception as e:
                        st.error(f"âŒ Analiz sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}")
                        import traceback
                        with st.expander("ğŸ” DetaylÄ± Hata MesajÄ±"):
                            st.code(traceback.format_exc())
        
        else:
            # Ã–rnek metinler
            with st.spinner("Topic'ler bulunuyor..."):
                # Korpus oluÅŸtur
                documents = generate_text_corpus(n_documents=n_documents)
                
                # NMF
                nmf_model = NMFTopicModeler(n_topics=n_topics, max_iter=200)
                nmf_model.fit(documents, max_features=max_features, min_df=2, max_df=0.95)
                
                # Topic'ler
                topics = nmf_model.get_topics(n_words=10)
                coherence = nmf_model.get_topic_coherence()
                
                st.metric("Topic TutarlÄ±lÄ±ÄŸÄ±", f"{coherence:.4f}")
                
                # Topic'leri gÃ¶ster
                st.subheader("Topic'ler ve Anahtar Kelimeler")
                for topic_name, words_scores in topics.items():
                    with st.expander(topic_name):
                        words_df = pd.DataFrame(words_scores, columns=['Kelime', 'Skor'])
                        st.dataframe(words_df, width='stretch')
                
                # GÃ¶rselleÅŸtirme
                st.subheader("ğŸ“Š Topic Kelime Analizi")
                with st.expander("â„¹ï¸ Topic Kelime Grafikleri Ne GÃ¶steriyor?", expanded=False):
                    st.markdown("""
                    **Topic Kelime Grafikleri**:
                    - Her topic iÃ§in en Ã¶nemli kelimeleri gÃ¶sterir
                    - **Y Ekseni**: Kelimeler (Ã¶nem sÄ±rasÄ±na gÃ¶re)
                    - **X Ekseni**: Ã–nem skoru (topic iÃ§indeki aÄŸÄ±rlÄ±k)
                    - **YÃ¼ksek skor**: Kelime, topic'i tanÄ±mlamada Ã§ok Ã¶nemli
                    - **DÃ¼ÅŸÃ¼k skor**: Kelime daha az Ã¶nemli
                    
                    **Yorumlama**:
                    - Ä°yi topic: TutarlÄ±, anlamlÄ± kelimeler
                    - KÃ¶tÃ¼ topic: Rastgele, tutarsÄ±z kelimeler
                    - Topic tutarlÄ±lÄ±ÄŸÄ±: TÃ¼m topic'lerin ortalama kalitesi
                    """)
                
                fig = plot_topic_words(topics, n_words=10)
                st.pyplot(fig)
                
                # DokÃ¼man-topic daÄŸÄ±lÄ±mÄ±
                doc_topics = nmf_model.get_document_topics()
                
                st.subheader("ğŸ”¥ DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ± Heatmap")
                with st.expander("â„¹ï¸ Heatmap Ne GÃ¶steriyor?", expanded=False):
                    st.markdown("""
                    **DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ± Heatmap**:
                    - **SatÄ±rlar**: Topic'ler
                    - **SÃ¼tunlar**: DokÃ¼manlar
                    - **Renkler**: DokÃ¼manÄ±n topic'e ait olma skoru
                      - **Koyu renk (dÃ¼ÅŸÃ¼k)**: DokÃ¼man bu topic'e az ait
                      - **AÃ§Ä±k renk (yÃ¼ksek)**: DokÃ¼man bu topic'e Ã§ok ait
                    
                    **Yorumlama**:
                    - Her dokÃ¼man genellikle 1-2 dominant topic'e sahiptir
                    - Koyu sÃ¼tunlar: Belirsiz dokÃ¼manlar (birden fazla topic)
                    - AÃ§Ä±k sÃ¼tunlar: Net topic'lere sahip dokÃ¼manlar
                    - Dikey Ã§izgiler: AynÄ± topic'teki dokÃ¼man gruplarÄ±
                    """)
                
                fig2, ax = plt.subplots(figsize=(12, 8))
                sns.heatmap(doc_topics[:50].T, cmap='YlOrRd', ax=ax, 
                           yticklabels=[f'Topic {i+1}' for i in range(n_topics)],
                           xticklabels=[f'Doc {i+1}' for i in range(50)])
                ax.set_xlabel('DokÃ¼manlar', fontsize=12)
                ax.set_ylabel('Topic\'ler', fontsize=12)
                ax.set_title('DokÃ¼man-Topic DaÄŸÄ±lÄ±mÄ± (Ä°lk 50 DokÃ¼man)', fontsize=14, fontweight='bold')
                st.pyplot(fig2)
                
                # Yeni dokÃ¼man tahmini
                st.subheader("Yeni DokÃ¼man iÃ§in Topic Tahmini")
                new_doc = st.text_input("DokÃ¼man metni girin:", 
                                       value="computer software algorithm data network")
                if new_doc:
                    top_topic, all_scores = nmf_model.predict_topic(new_doc)
                    st.write(f"**En uygun topic:** Topic {top_topic+1} (skor: {all_scores[top_topic]:.3f})")
                    
                    scores_df = pd.DataFrame({
                        'Topic': [f'Topic {i+1}' for i in range(n_topics)],
                        'Skor': all_scores
                    })
                    st.bar_chart(scores_df.set_index('Topic'))


def show_als_recommender():
    """ALS Ã¶neri sistemi"""
    st.header("âš¡ ALS - Ã–neri Sistemi")
    
    # Info bÃ¶lÃ¼mÃ¼
    with st.expander("â„¹ï¸ ALS (Alternating Least Squares) HakkÄ±nda Bilgi", expanded=False):
        st.markdown("""
        ### ALS Nedir?
        
        **ALS**, bÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶neri sistemleri iÃ§in optimize edilmiÅŸ bir matris faktÃ¶rizasyon yÃ¶ntemidir.
        
        ### NasÄ±l Ã‡alÄ±ÅŸÄ±r?
        
        1. **Alternatif Optimizasyon**: KullanÄ±cÄ± ve Ã¼rÃ¼n faktÃ¶rlerini sÄ±rayla optimize eder
        2. **Paralel Ä°ÅŸleme**: Her kullanÄ±cÄ±/Ã¼rÃ¼n baÄŸÄ±msÄ±z iÅŸlenebilir (Spark uyumlu)
        3. **Regularizasyon**: AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) Ã¶nler
        4. **Iteratif GÃ¼ncelleme**: Her iterasyonda faktÃ¶rleri iyileÅŸtirir
        
        ### SVD'den FarklarÄ±
        
        - **Paralel Ã‡alÄ±ÅŸma**: BÃ¼yÃ¼k veri setlerinde daha hÄ±zlÄ±
        - **Eksik Veri**: Sparse matrislerde daha iyi performans
        - **Ã–lÃ§eklenebilirlik**: Milyonlarca kullanÄ±cÄ±/Ã¼rÃ¼n ile Ã§alÄ±ÅŸabilir
        - **Regularizasyon**: Daha iyi genelleme
        
        ### Parametreler
        
        - **Latent FaktÃ¶r SayÄ±sÄ±**: Gizli Ã¶zellik sayÄ±sÄ± (daha fazla = daha detaylÄ±)
        - **Regularizasyon**: AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nler (0.01-1.0 arasÄ±)
        - **Ä°terasyon SayÄ±sÄ±**: EÄŸitim iterasyonu (daha fazla = daha iyi ama yavaÅŸ)
        
        ### KullanÄ±m AlanlarÄ±
        
        - Netflix, Amazon gibi bÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶neri sistemleri
        - Spark, Hadoop gibi daÄŸÄ±tÄ±k sistemler
        - GerÃ§ek zamanlÄ± Ã¶neriler
        """)
    
    st.markdown("""
    ALS (Alternating Least Squares) kullanarak bÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶neri sistemi.
    """)
    
    # Veri yÃ¼kleme seÃ§eneÄŸi
    data_source = st.radio(
        "Veri KaynaÄŸÄ±",
        ["ğŸ“Š Ã–rnek Veri OluÅŸtur", "ğŸ“ Dosyadan YÃ¼kle"],
        horizontal=True,
        key="als_data_source"
    )
    
    # Dosya formatÄ± seÃ§imi (sadece dosya yÃ¼kleme seÃ§ildiyse)
    file_format = None
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        file_format = get_file_format_selector("als", include_image=False)
    
    # Session state ile veriyi koru
    if 'als_rating_matrix' not in st.session_state:
        st.session_state.als_rating_matrix = None
        st.session_state.als_user_mapping = None
        st.session_state.als_item_mapping = None
    
    rating_matrix = st.session_state.als_rating_matrix
    user_mapping = st.session_state.als_user_mapping
    item_mapping = st.session_state.als_item_mapping
    n_users = None
    n_items = None
    n_factors = 20  # VarsayÄ±lan deÄŸer
    
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
        st.info("""
        **Desteklenen Formatlar:** CSV, Excel (.xlsx, .xls)
        
        **Veri FormatÄ±:** Long Format (user_id, item_id, rating) veya Matrix Format
        """)
        
        file = st.file_uploader(
            "Veri dosyasÄ±nÄ± seÃ§in",
            type=['csv', 'xlsx', 'xls'],
            help="CSV veya Excel dosyasÄ± yÃ¼kleyin",
            key="als_file"
        )
        
        if file is not None:
            try:
                # Dosya Ã¶nizlemesi ve format Ã¶nerisi
                # Dosya stream'i bir kez okununca tÃ¼kenir, bu yÃ¼zden iÃ§eriÄŸi hafÄ±zaya al
                import io
                file_content = file.read()
                file_bytes = io.BytesIO(file_content)
                
                import pandas as pd
                if file.name.endswith('.csv'):
                    # Delimiter tespiti
                    file_bytes.seek(0)
                    first_line = file_bytes.readline().decode('utf-8', errors='ignore')
                    delimiters = [',', ';', '\t', '|']
                    detected_delimiter = ','
                    max_cols = 0
                    for delim in delimiters:
                        cols = first_line.split(delim)
                        if len(cols) > max_cols:
                            max_cols = len(cols)
                            detected_delimiter = delim
                    
                    file_bytes.seek(0)
                    preview_df = pd.read_csv(file_bytes, nrows=5, sep=detected_delimiter, engine='python')
                    # Toplam satÄ±r sayÄ±sÄ± iÃ§in dosyayÄ± tekrar oku
                    file_bytes.seek(0)
                    total_df = pd.read_csv(file_bytes, sep=detected_delimiter, engine='python')
                elif file.name.endswith(('.xlsx', '.xls')):
                    file_bytes.seek(0)
                    preview_df = pd.read_excel(file_bytes, nrows=5)
                    # Toplam satÄ±r sayÄ±sÄ± iÃ§in dosyayÄ± tekrar oku
                    file_bytes.seek(0)
                    total_df = pd.read_excel(file_bytes)
                else:
                    preview_df = None
                    total_df = None
                
                if preview_df is not None:
                    with st.expander("ğŸ‘ï¸ Dosya Ã–nizleme (Ä°lk 5 SatÄ±r)", expanded=False):
                        st.dataframe(preview_df, width='stretch')
                        st.info(f"""
                        **Dosya Bilgileri:**
                        - **SatÄ±r SayÄ±sÄ±**: {len(total_df) if total_df is not None else 'Bilinmiyor'} (tahmini)
                        - **SÃ¼tun SayÄ±sÄ±**: {len(preview_df.columns)}
                        - **SÃ¼tun Ä°simleri**: {', '.join(preview_df.columns.tolist()[:10])}{'...' if len(preview_df.columns) > 10 else ''}
                        
                        **Format Ã–nerisi:**
                        - **3 sÃ¼tun varsa** â†’ Long Format seÃ§in (user_id, item_id, rating)
                        - **10+ sÃ¼tun varsa** â†’ Matrix Format seÃ§in (ilk sÃ¼tun kullanÄ±cÄ± ID, diÄŸerleri Ã¼rÃ¼n ID)
                        """)
                
                # Dosya stream'ini tekrar kullanÄ±labilir hale getir
                file.seek(0)
                
                data_format = st.radio(
                    "Veri FormatÄ±",
                    ["Long Format (user_id, item_id, rating)", "Matrix Format (Rating Matrisi)"],
                    horizontal=True,
                    key="als_data_format"
                )
                
                if data_format == "Long Format (user_id, item_id, rating)":
                    # Manuel sÃ¼tun seÃ§imi
                    st.markdown("#### ğŸ“‹ SÃ¼tun SeÃ§imi (Opsiyonel)")
                    use_manual_cols = st.checkbox(
                        "Manuel olarak sÃ¼tun seÃ§mek istiyorum", 
                        key="als_manual_cols",
                        help="Otomatik tespit yanlÄ±ÅŸ Ã§alÄ±ÅŸÄ±yorsa, bu seÃ§eneÄŸi iÅŸaretleyin"
                    )
                    
                    user_col_manual = None
                    item_col_manual = None
                    rating_col_manual = None
                    
                    if use_manual_cols and preview_df is not None:
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            user_col_manual = st.selectbox(
                                "KullanÄ±cÄ± ID SÃ¼tunu",
                                options=preview_df.columns.tolist(),
                                key="als_user_col",
                                help="KullanÄ±cÄ± ID'lerini iÃ§eren sÃ¼tun"
                            )
                        with col2:
                            item_col_manual = st.selectbox(
                                "ÃœrÃ¼n/Ã–ÄŸe ID SÃ¼tunu",
                                options=preview_df.columns.tolist(),
                                index=min(1, len(preview_df.columns)-1),
                                key="als_item_col",
                                help="ÃœrÃ¼n/Ã–ÄŸe ID'lerini iÃ§eren sÃ¼tun"
                            )
                        with col3:
                            # SayÄ±sal sÃ¼tunlarÄ± bul
                            numeric_cols = preview_df.select_dtypes(include=[np.number]).columns.tolist()
                            if not numeric_cols:
                                numeric_cols = preview_df.columns.tolist()
                            
                            default_idx = min(2, len(numeric_cols)-1) if numeric_cols else 0
                            rating_col_manual = st.selectbox(
                                "Rating/Puan SÃ¼tunu",
                                options=numeric_cols if numeric_cols else preview_df.columns.tolist(),
                                index=default_idx,
                                key="als_rating_col",
                                help="Rating/Puan deÄŸerlerini iÃ§eren sÃ¼tun (sayÄ±sal olmalÄ±)"
                            )
                        
                        st.info(f"âœ… SeÃ§ilen: `{user_col_manual}` (kullanÄ±cÄ±) + `{item_col_manual}` (Ã¼rÃ¼n) + `{rating_col_manual}` (rating)")
                    
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="als_load_long"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            # Dosya bilgilerini kaydet
                            file_name = file.name
                            file_size = file.size
                            
                            # Session state'ten dosya iÃ§eriÄŸini al (Ã¶nizleme sÄ±rasÄ±nda kaydedilmiÅŸ)
                            if 'als_file_content' in st.session_state:
                                # Dosya iÃ§eriÄŸini BytesIO'ya Ã§evir
                                import io
                                file_bytes = io.BytesIO(st.session_state.als_file_content)
                                # Dosya objesi gibi davranmasÄ± iÃ§in name attribute ekle
                                file_bytes.name = file_name
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(
                                    file_bytes, 
                                    user_col=user_col_manual, 
                                    item_col=item_col_manual, 
                                    rating_col=rating_col_manual
                                )
                            else:
                                # Fallback: dosyayÄ± tekrar oku
                                file.seek(0)
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(
                                    file, 
                                    user_col=user_col_manual, 
                                    item_col=item_col_manual, 
                                    rating_col=rating_col_manual
                                )
                            
                            # Dosya bilgilerini session state'e kaydet
                            st.session_state.als_file_name = file_name
                            st.session_state.als_file_size = file_size
                            
                            # Session state'e kaydet
                            st.session_state.als_rating_matrix = rating_matrix
                            st.session_state.als_user_mapping = user_mapping
                            st.session_state.als_item_mapping = item_mapping
                            
                            # Veri istatistiklerini hesapla
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                                ratings_data = rating_matrix.data
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                                ratings_data = rating_matrix[mask]
                            
                            # Rating istatistikleri - boÅŸ array kontrolÃ¼
                            if len(ratings_data) == 0:
                                # EÄŸer hiÃ§ rating yoksa varsayÄ±lan deÄŸerler
                                min_rating = 0.0
                                max_rating = 0.0
                                mean_rating = 0.0
                                median_rating = 0.0
                                st.warning("âš ï¸ UyarÄ±: Dosyada hiÃ§ rating deÄŸeri bulunamadÄ±! TÃ¼m deÄŸerler NaN olabilir.")
                            else:
                                min_rating = float(np.min(ratings_data))
                                max_rating = float(np.max(ratings_data))
                                mean_rating = float(np.mean(ratings_data))
                                median_rating = float(np.median(ratings_data))
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            
                            # DetaylÄ± dosya analizi bÃ¶lÃ¼mÃ¼
                            with st.expander("ğŸ“‹ Dosya Analizi - KullanÄ±lan Veriler", expanded=True):
                                st.markdown("### ğŸ“ Dosya Bilgileri")
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Dosya AdÄ±", file_name)
                                    st.metric("Dosya Boyutu", f"{file_size / 1024:.2f} KB")
                                with col2:
                                    st.metric("Veri FormatÄ±", "Long Format")
                                    st.metric("Toplam SatÄ±r SayÄ±sÄ±", f"{n_ratings:,}")
                                
                                st.markdown("### ğŸ“ˆ Veri Ä°statistikleri")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("KullanÄ±cÄ± SayÄ±sÄ±", f"{rating_matrix.shape[0]:,}")
                                with col2:
                                    st.metric("ÃœrÃ¼n SayÄ±sÄ±", f"{rating_matrix.shape[1]:,}")
                                with col3:
                                    st.metric("Toplam Rating", f"{n_ratings:,}")
                                with col4:
                                    st.metric("Veri YoÄŸunluÄŸu", f"{density:.2f}%")
                                
                                st.markdown("### â­ Rating DaÄŸÄ±lÄ±mÄ±")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("Minimum Rating", f"{min_rating:.2f}")
                                with col2:
                                    st.metric("Maksimum Rating", f"{max_rating:.2f}")
                                with col3:
                                    st.metric("Ortalama Rating", f"{mean_rating:.2f}")
                                with col4:
                                    st.metric("Medyan Rating", f"{median_rating:.2f}")
                                
                                st.markdown("""
                                **ğŸ“ AÃ§Ä±klama:**
                                - **Dosya Bilgileri**: YÃ¼klenen dosyanÄ±n adÄ± ve boyutu
                                - **Veri Ä°statistikleri**: Matris boyutlarÄ± ve veri yoÄŸunluÄŸu
                                - **Rating DaÄŸÄ±lÄ±mÄ±**: Rating deÄŸerlerinin istatistiksel Ã¶zeti
                                
                                Bu veriler, ALS (Alternating Least Squares) algoritmasÄ± ile iÅŸlenecek ve 
                                kullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimlerinden latent faktÃ¶rler Ã§Ä±karÄ±lacaktÄ±r.
                                """)
                            
                            st.rerun()  # SayfayÄ± yenile
                else:
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="als_load_matrix"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            # Dosya bilgilerini kaydet
                            file_name = file.name
                            file_size = file.size
                            
                            # Session state'ten dosya iÃ§eriÄŸini al (Ã¶nizleme sÄ±rasÄ±nda kaydedilmiÅŸ)
                            if 'als_file_content' in st.session_state:
                                # Dosya iÃ§eriÄŸini BytesIO'ya Ã§evir
                                import io
                                file_bytes = io.BytesIO(st.session_state.als_file_content)
                                # Dosya objesi gibi davranmasÄ± iÃ§in name attribute ekle
                                file_bytes.name = file_name
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file_bytes)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.info("""
                                    **ğŸ’¡ Matrix Format iÃ§in:**
                                    - Ä°lk sÃ¼tun kullanÄ±cÄ± ID'leri olmalÄ± (index)
                                    - DiÄŸer sÃ¼tunlar Ã¼rÃ¼n ID'leri olmalÄ±
                                    - DeÄŸerler rating'ler olmalÄ± (NaN = eksik veri)
                                    - CSV dosyasÄ±nda ilk sÃ¼tun otomatik olarak index olarak okunur
                                    
                                    **Ã–rnek Format:**
                                    ```
                                    user_id,item_1,item_2,item_3,...
                                    1,4.5,3.0,5.0,...
                                    2,2.5,4.0,NaN,...
                                    ```
                                    """)
                                    st.stop()
                            else:
                                # Fallback: dosyayÄ± tekrar oku
                                file.seek(0)
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.info("""
                                    **ğŸ’¡ Matrix Format iÃ§in:**
                                    - Ä°lk sÃ¼tun kullanÄ±cÄ± ID'leri olmalÄ± (index)
                                    - DiÄŸer sÃ¼tunlar Ã¼rÃ¼n ID'leri olmalÄ±
                                    - DeÄŸerler rating'ler olmalÄ± (NaN = eksik veri)
                                    - CSV dosyasÄ±nda ilk sÃ¼tun otomatik olarak index olarak okunur
                                    
                                    **Ã–rnek Format:**
                                    ```
                                    user_id,item_1,item_2,item_3,...
                                    1,4.5,3.0,5.0,...
                                    2,2.5,4.0,NaN,...
                                    ```
                                    """)
                                    st.stop()
                            
                            # Dosya bilgilerini session state'e kaydet
                            st.session_state.als_file_name = file_name
                            st.session_state.als_file_size = file_size
                            
                            # Session state'e kaydet
                            st.session_state.als_rating_matrix = rating_matrix
                            st.session_state.als_user_mapping = None
                            st.session_state.als_item_mapping = None
                            
                            # Veri istatistiklerini hesapla
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                                ratings_data = rating_matrix.data
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                                ratings_data = rating_matrix[mask]
                            
                            # Rating istatistikleri - boÅŸ array kontrolÃ¼
                            if len(ratings_data) == 0:
                                # EÄŸer hiÃ§ rating yoksa varsayÄ±lan deÄŸerler
                                min_rating = 0.0
                                max_rating = 0.0
                                mean_rating = 0.0
                                median_rating = 0.0
                                st.warning("âš ï¸ UyarÄ±: Dosyada hiÃ§ rating deÄŸeri bulunamadÄ±! TÃ¼m deÄŸerler NaN olabilir.")
                            else:
                                min_rating = float(np.min(ratings_data))
                                max_rating = float(np.max(ratings_data))
                                mean_rating = float(np.mean(ratings_data))
                                median_rating = float(np.median(ratings_data))
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            
                            # DetaylÄ± dosya analizi bÃ¶lÃ¼mÃ¼
                            with st.expander("ğŸ“‹ Dosya Analizi - KullanÄ±lan Veriler", expanded=True):
                                st.markdown("### ğŸ“ Dosya Bilgileri")
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Dosya AdÄ±", file_name)
                                    st.metric("Dosya Boyutu", f"{file_size / 1024:.2f} KB")
                                with col2:
                                    st.metric("Veri FormatÄ±", "Matrix Format")
                                    st.metric("Toplam Rating", f"{n_ratings:,}")
                                
                                st.markdown("### ğŸ“ˆ Veri Ä°statistikleri")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("KullanÄ±cÄ± SayÄ±sÄ±", f"{rating_matrix.shape[0]:,}")
                                with col2:
                                    st.metric("ÃœrÃ¼n SayÄ±sÄ±", f"{rating_matrix.shape[1]:,}")
                                with col3:
                                    st.metric("Toplam Rating", f"{n_ratings:,}")
                                with col4:
                                    st.metric("Veri YoÄŸunluÄŸu", f"{density:.2f}%")
                                
                                st.markdown("### â­ Rating DaÄŸÄ±lÄ±mÄ±")
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("Minimum Rating", f"{min_rating:.2f}")
                                with col2:
                                    st.metric("Maksimum Rating", f"{max_rating:.2f}")
                                with col3:
                                    st.metric("Ortalama Rating", f"{mean_rating:.2f}")
                                with col4:
                                    st.metric("Medyan Rating", f"{median_rating:.2f}")
                                
                                st.markdown("""
                                **ğŸ“ AÃ§Ä±klama:**
                                - **Dosya Bilgileri**: YÃ¼klenen dosyanÄ±n adÄ± ve boyutu
                                - **Veri Ä°statistikleri**: Matris boyutlarÄ± ve veri yoÄŸunluÄŸu
                                - **Rating DaÄŸÄ±lÄ±mÄ±**: Rating deÄŸerlerinin istatistiksel Ã¶zeti
                                
                                Bu veriler, ALS (Alternating Least Squares) algoritmasÄ± ile iÅŸlenecek ve 
                                kullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimlerinden latent faktÃ¶rler Ã§Ä±karÄ±lacaktÄ±r.
                                """)
                            
                            st.rerun()  # SayfayÄ± yenile
                            
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen veri formatÄ±nÄ± kontrol edin.")
    else:
        # Ã–rnek veri oluÅŸtur - session state'i temizle
        if st.session_state.als_rating_matrix is not None:
            st.session_state.als_rating_matrix = None
            st.session_state.als_user_mapping = None
            st.session_state.als_item_mapping = None
        
        col1, col2, col3 = st.columns(3)
        with col1:
            n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 500, 100)
        with col2:
            n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 200, 50)
        with col3:
            n_factors = st.slider("Latent FaktÃ¶r SayÄ±sÄ±", 5, 50, 20)
    
    # Model parametreleri (veri yÃ¼klendiyse)
    if rating_matrix is not None:
        n_users, n_items = rating_matrix.shape
        
        # Veri seti boÅŸ mu kontrol et
        if n_users == 0 or n_items == 0:
            st.error("âŒ Hata: Veri seti boÅŸ! LÃ¼tfen geÃ§erli bir veri dosyasÄ± yÃ¼kleyin.")
            n_factors = 5  # VarsayÄ±lan deÄŸer
        else:
            max_factors = min(50, min(n_users, n_items))
            min_factors = min(5, max_factors)  # min_value max_value'dan kÃ¼Ã§Ã¼k olmalÄ±
            default_factors = min(20, max_factors)
            
            # EÄŸer max_factors Ã§ok kÃ¼Ã§Ã¼kse veya 0 ise, slider yerine sabit deÄŸer kullan
            if max_factors <= 0 or max_factors < min_factors:
                n_factors = max(1, max_factors)  # En az 1 faktÃ¶r
                if max_factors <= 0:
                    st.error("âŒ Hata: Veri seti Ã§ok kÃ¼Ã§Ã¼k! Latent faktÃ¶r sayÄ±sÄ± ayarlanamÄ±yor.")
                else:
                    st.info(f"âš ï¸ Veri seti kÃ¼Ã§Ã¼k olduÄŸu iÃ§in latent faktÃ¶r sayÄ±sÄ± otomatik olarak {n_factors} olarak ayarlandÄ±.")
            else:
                # Optimal parametreleri al
                optimal_params = get_optimal_model_params("als", data_shape=(n_users, n_items))
                optimal_n_factors = optimal_params['n_factors']
                suggested_factors = max(min_factors, min(max_factors, optimal_n_factors))
                
                n_factors = st.slider(
                    "Latent FaktÃ¶r SayÄ±sÄ±", 
                    min_factors, 
                    max_factors, 
                    suggested_factors,
                    key="als_n_factors_loaded",
                    help=f"Ã–nerilen deÄŸer: {optimal_n_factors} (veri boyutuna gÃ¶re otomatik hesaplandÄ±)"
                )
                if n_factors != optimal_n_factors:
                    st.info(f"ğŸ’¡ Veri boyutunuza gÃ¶re Ã¶nerilen deÄŸer: {optimal_n_factors}")
    
    # Optimal parametreleri al (veri yÃ¼klendiyse)
    if rating_matrix is not None:
        optimal_params = get_optimal_model_params("als", data_shape=rating_matrix.shape)
        optimal_regularization = optimal_params['regularization']
        optimal_iterations = optimal_params['iterations']
    else:
        optimal_regularization = 0.1
        optimal_iterations = 15
    
    col1, col2 = st.columns(2)
    with col1:
        regularization = st.slider(
            "Regularizasyon", 
            0.01, 1.0, 
            optimal_regularization,
            help=f"Ã–nerilen deÄŸer: {optimal_regularization}"
        )
    with col2:
        iterations = st.slider(
            "Ä°terasyon SayÄ±sÄ±", 
            5, 30, 
            optimal_iterations,
            help=f"Ã–nerilen deÄŸer: {optimal_iterations}"
        )
    
    # Sparsity sadece Ã¶rnek veri iÃ§in
    if rating_matrix is None:
        sparsity = st.slider("Eksik Veri OranÄ±", 0.3, 0.9, 0.7)
    
    if st.button("ğŸš€ Modeli EÄŸit", key="als_train"):
        if rating_matrix is None and data_source == "ğŸ“ Dosyadan YÃ¼kle":
            st.warning("âš ï¸ LÃ¼tfen Ã¶nce veri dosyasÄ±nÄ± yÃ¼kleyin!")
        else:
            with st.spinner("Model eÄŸitiliyor (bu biraz zaman alabilir)..."):
                # Veri yoksa oluÅŸtur
                if rating_matrix is None:
                    rating_matrix = generate_rating_matrix(
                        n_users=n_users, 
                        n_items=n_items, 
                        sparsity=sparsity
                    )
            
            # Train-test split (sparse matrix desteÄŸi ile)
            from scipy.sparse import issparse
            
            np.random.seed(42)
            
            if issparse(rating_matrix):
                # Sparse matrix iÃ§in
                rows, cols = rating_matrix.nonzero()
                n_ratings = len(rows)
                test_size = min(int(0.2 * n_ratings), 10000)
                test_sample_indices = np.random.choice(n_ratings, size=test_size, replace=False)
                
                test_rows = rows[test_sample_indices]
                test_cols = cols[test_sample_indices]
                # Sparse matrix'ten deÄŸerleri al - matrix objesi iÃ§in np.array kullan
                test_matrix_slice = rating_matrix[test_rows, test_cols]
                # matrix objesi iÃ§in A property veya np.array kullan
                if hasattr(test_matrix_slice, 'A'):
                    test_values = test_matrix_slice.A.flatten()
                elif hasattr(test_matrix_slice, 'toarray'):
                    test_values = test_matrix_slice.toarray().flatten()
                else:
                    test_values = np.array(test_matrix_slice).flatten()
                
                train_matrix = rating_matrix.copy()
                train_matrix[test_rows, test_cols] = 0
                train_matrix.eliminate_zeros()
                
                test_indices = (test_rows, test_cols)
            else:
                # Dense matrix iÃ§in
                mask = ~np.isnan(rating_matrix)
                n_ratings = np.sum(mask)
                test_size = min(int(0.2 * n_ratings), 10000)
                
                valid_indices = np.where(mask)
                test_sample_indices = np.random.choice(
                    len(valid_indices[0]), 
                    size=test_size, 
                    replace=False
                )
                
                test_mask = np.zeros_like(mask, dtype=bool)
                test_mask[valid_indices[0][test_sample_indices], valid_indices[1][test_sample_indices]] = True
                
                train_matrix = rating_matrix.copy()
                train_matrix[test_mask] = np.nan
                
                test_values = rating_matrix[test_mask]
                test_indices = np.where(test_mask)
            
            # Model eÄŸit
            import time
            training_start = time.time()
            
            als_model = ALSRecommender(
                n_factors=n_factors, 
                regularization=regularization, 
                iterations=iterations
            )
            als_model.fit(train_matrix, implicit=False)
            
            training_time = time.time() - training_start
            
            # Test matrisi oluÅŸtur (evaluate iÃ§in)
            if issparse(rating_matrix):
                # Sparse matrix iÃ§in test matrisi oluÅŸtur
                from scipy.sparse import csr_matrix
                test_matrix = csr_matrix((test_values, (test_indices[0], test_indices[1])), 
                                        shape=rating_matrix.shape)
            else:
                # Dense matrix iÃ§in test matrisi oluÅŸtur
                test_matrix = np.full_like(rating_matrix, np.nan)
                test_matrix[test_indices[0], test_indices[1]] = test_values
            
            # DeÄŸerlendirme
            test_predictions = []
            batch_size = 1000
            for i in range(0, len(test_indices[0]), batch_size):
                batch_end = min(i + batch_size, len(test_indices[0]))
                batch_users = test_indices[0][i:batch_end]
                batch_items = test_indices[1][i:batch_end]
                batch_preds = als_model.predict_all()[batch_users, batch_items]
                test_predictions.extend(batch_preds)
            
            test_predictions = np.array(test_predictions)
            rmse = np.sqrt(mean_squared_error(test_values, test_predictions))
            
            # SonuÃ§lar
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Test RMSE", f"{rmse:.4f}")
            with col2:
                st.metric("EÄŸitim Ä°terasyonu", iterations)
            with col3:
                st.metric("EÄŸitim SÃ¼resi", f"{training_time:.2f} saniye")
            
            # DetaylÄ± sonuÃ§ aÃ§Ä±klamasÄ±
            with st.expander("ğŸ“ SonuÃ§ AÃ§Ä±klamasÄ± - Ne Elde Edildi?", expanded=True):
                st.markdown("### ğŸ” KullanÄ±lan Veriler")
                if data_source == "ğŸ“ Dosyadan YÃ¼kle" and 'als_file_name' in st.session_state:
                    st.info(f"""
                    **Dosya**: {st.session_state.als_file_name} ({st.session_state.als_file_size / 1024:.2f} KB)
                    - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                    - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                    - **Toplam Rating**: {n_ratings:,}
                    """)
                else:
                    st.info(f"""
                    **Ã–rnek Veri**:
                    - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                    - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                    """)
                
                st.markdown("### âš™ï¸ Model Parametreleri")
                st.info(f"""
                - **Latent FaktÃ¶r SayÄ±sÄ± (n_factors)**: {n_factors}
                - **Regularizasyon**: {regularization}
                - **Ä°terasyon SayÄ±sÄ±**: {iterations}
                - **EÄŸitim Verisi**: {n_ratings - len(test_values):,} rating
                - **Test Verisi**: {len(test_values):,} rating
                """)
                
                st.markdown("### ğŸ“ˆ Elde Edilen SonuÃ§lar")
                st.success(f"""
                **Model BaÅŸarÄ±yla EÄŸitildi!**
                
                1. **Test RMSE**: {rmse:.4f}
                   - Bu deÄŸer ne kadar dÃ¼ÅŸÃ¼kse, model o kadar iyi tahmin yapÄ±yor demektir
                   - RMSE, gerÃ§ek rating'ler ile tahmin edilen rating'ler arasÄ±ndaki ortalama hata miktarÄ±nÄ± gÃ¶sterir
                   - Ã–rnek: RMSE = {rmse:.4f} â†’ Ortalama {rmse:.2f} puanlÄ±k hata var
                
                2. **EÄŸitim Ä°terasyonu**: {iterations}
                   - Model, {iterations} iterasyon boyunca kullanÄ±cÄ± ve Ã¼rÃ¼n faktÃ¶rlerini optimize etti
                   - Her iterasyonda faktÃ¶rler daha iyi hale geldi
                
                3. **Latent FaktÃ¶r SayÄ±sÄ±**: {n_factors}
                   - Model, veriyi {n_factors} boyutlu latent faktÃ¶r uzayÄ±na indirgedi
                   - Her faktÃ¶r, kullanÄ±cÄ± ve Ã¼rÃ¼n Ã¶zelliklerini temsil eden bir boyuttur
                
                4. **EÄŸitim SÃ¼resi**: {training_time:.2f} saniye
                   - Model eÄŸitimi {training_time:.2f} saniyede tamamlandÄ±
                """)
                
                st.markdown("### ğŸ¯ Ne YapÄ±ldÄ±?")
                st.markdown(f"""
                **ALS (Alternating Least Squares) AlgoritmasÄ±** ÅŸu adÄ±mlarÄ± izledi:
                
                1. **Veri HazÄ±rlama**: Rating matrisi train ve test setlerine ayrÄ±ldÄ±
                2. **FaktÃ¶r BaÅŸlatma**: {n_factors} boyutlu kullanÄ±cÄ± ve Ã¼rÃ¼n faktÃ¶r matrisleri rastgele baÅŸlatÄ±ldÄ±
                3. **Alternatif Optimizasyon**: {iterations} iterasyon boyunca:
                   - **KullanÄ±cÄ± faktÃ¶rleri sabitken**, Ã¼rÃ¼n faktÃ¶rleri optimize edildi
                   - **ÃœrÃ¼n faktÃ¶rleri sabitken**, kullanÄ±cÄ± faktÃ¶rleri optimize edildi
                   - Her iterasyonda hata azaltÄ±ldÄ±
                4. **Regularizasyon**: {regularization} deÄŸeri ile aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) Ã¶nlendi
                5. **Tahmin**: Eksik rating'ler, latent faktÃ¶rler kullanÄ±larak tahmin edildi
                6. **DeÄŸerlendirme**: Test seti Ã¼zerinde RMSE hesaplandÄ±
                
                **SonuÃ§**: Model, kullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimlerinden Ã¶ÄŸrendiÄŸi kalÄ±plarÄ± kullanarak 
                yeni rating'leri tahmin edebiliyor. ALS, SVD'den farklÄ± olarak:
                - **Paralel Ã§alÄ±ÅŸabilir**: Her kullanÄ±cÄ±/Ã¼rÃ¼n baÄŸÄ±msÄ±z iÅŸlenebilir
                - **BÃ¼yÃ¼k veri setlerinde daha hÄ±zlÄ±**: Sparse matrislerde optimize edilmiÅŸtir
                - **Regularizasyon ile daha iyi genelleme**: AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nler
                
                Bu sayede kullanÄ±cÄ±lara henÃ¼z gÃ¶rmedikleri Ã¼rÃ¼nler iÃ§in kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler sunulabilir.
                """)
                
                st.markdown("### ğŸ’¡ Sonraki AdÄ±mlar")
                st.info("""
                - **Ã–neriler**: AÅŸaÄŸÄ±daki "KullanÄ±cÄ± Ã–nerileri" bÃ¶lÃ¼mÃ¼nden belirli bir kullanÄ±cÄ± iÃ§in Ã¶neriler gÃ¶rebilirsiniz
                - **Benzer ÃœrÃ¼nler**: Item similarity bÃ¶lÃ¼mÃ¼nden benzer Ã¼rÃ¼nleri inceleyebilirsiniz
                - **Parametre AyarÄ±**: Regularizasyon ve iterasyon sayÄ±sÄ±nÄ± deÄŸiÅŸtirerek model performansÄ±nÄ± artÄ±rabilirsiniz
                """)
            
            # Ã–neriler
            st.subheader("ğŸ¯ KullanÄ±cÄ± Ã–nerileri")
            with st.expander("â„¹ï¸ ALS Ã–nerileri NasÄ±l Ã‡alÄ±ÅŸÄ±yor?", expanded=False):
                st.markdown("""
                **ALS Ã–neri Sistemi**:
                - KullanÄ±cÄ±nÄ±n latent faktÃ¶rlerini kullanÄ±r
                - TÃ¼m Ã¼rÃ¼nler iÃ§in rating tahmin eder
                - En yÃ¼ksek tahmin edilen rating'lere sahip Ã¼rÃ¼nleri Ã¶nerir
                - Zaten rating verilen Ã¼rÃ¼nleri hariÃ§ tutar
                
                **Tahmin Edilen Rating**: Modelin, kullanÄ±cÄ±nÄ±n bu Ã¼rÃ¼ne vereceÄŸi rating tahmini
                - YÃ¼ksek deÄŸer: KullanÄ±cÄ±nÄ±n beÄŸenme olasÄ±lÄ±ÄŸÄ± yÃ¼ksek
                - DÃ¼ÅŸÃ¼k deÄŸer: KullanÄ±cÄ±nÄ±n beÄŸenme olasÄ±lÄ±ÄŸÄ± dÃ¼ÅŸÃ¼k
                """)
            
            user_idx = st.selectbox("KullanÄ±cÄ± SeÃ§in", range(min(10, n_users)))
            
            recommendations = als_model.recommend(
                user_idx, 
                n_recommendations=10, 
                exclude_rated=True, 
                rating_matrix=rating_matrix
            )
            
            recommendations_df = pd.DataFrame({
                'ÃœrÃ¼n ID': recommendations[0] + 1,
                'Tahmin Edilen Rating': recommendations[1]
            })
            st.dataframe(recommendations_df, width='stretch')
            
            # Benzer item'lar
            st.subheader("ğŸ”— Benzer ÃœrÃ¼nler (Item Similarity)")
            with st.expander("â„¹ï¸ Benzerlik NasÄ±l HesaplanÄ±yor?", expanded=False):
                st.markdown("""
                **Cosine Similarity**:
                - Ä°ki Ã¼rÃ¼nÃ¼n latent faktÃ¶r vektÃ¶rleri arasÄ±ndaki aÃ§Ä±yÄ± Ã¶lÃ§er
                - **1.0**: Tamamen benzer (aynÄ± yÃ¶nde)
                - **0.0**: Ortogonal (iliÅŸkisiz)
                - **-1.0**: Tamamen zÄ±t
                
                **KullanÄ±m**:
                - "Bu Ã¼rÃ¼ne bakanlar ÅŸunlara da baktÄ±" Ã¶zelliÄŸi
                - ÃœrÃ¼n kategorilendirme
                - Cross-selling Ã¶nerileri
                """)
            
            col1, col2 = st.columns([2, 1])
            with col1:
                item_idx = st.selectbox("ÃœrÃ¼n SeÃ§in", range(min(10, n_items)), key="als_item")
            with col2:
                n_similar = st.number_input("GÃ¶sterilecek Benzer ÃœrÃ¼n SayÄ±sÄ±", min_value=5, max_value=50, value=10, step=5, key="als_n_similar")
            
            if st.button("ğŸ” Benzer ÃœrÃ¼nleri GÃ¶ster", key="als_show_similar", type="primary"):
                with st.spinner("Benzer Ã¼rÃ¼nler hesaplanÄ±yor..."):
                    similar_items = als_model.get_similar_items(item_idx, n_similar=n_similar)
                    
                    # Benzer Ã¼rÃ¼nler tablosu
                    similar_df = pd.DataFrame({
                        'Benzer ÃœrÃ¼n ID': similar_items[0] + 1,
                        'Benzerlik Skoru': [f"{score:.4f}" for score in similar_items[1]]
                    })
                    
                    st.markdown(f"### ğŸ“Š ÃœrÃ¼n {item_idx + 1} ile Benzer {len(similar_items[0])} ÃœrÃ¼n")
                    st.dataframe(similar_df, width='stretch')
                    
                    # GÃ¶rselleÅŸtirme
                    fig, ax = plt.subplots(figsize=(12, 6))
                    bars = ax.barh(range(len(similar_items[1])), similar_items[1], alpha=0.7, 
                                  color=plt.cm.viridis(similar_items[1]))
                    ax.set_yticks(range(len(similar_items[1])))
                    ax.set_yticklabels([f"ÃœrÃ¼n {idx + 1}" for idx in similar_items[0]])
                    ax.set_xlabel('Benzerlik Skoru (Cosine Similarity)', fontsize=12)
                    ax.set_title(f'ÃœrÃ¼n {item_idx + 1} ile En Benzer {n_similar} ÃœrÃ¼n', fontsize=14, fontweight='bold')
                    ax.grid(True, alpha=0.3, axis='x')
                    ax.set_xlim(0, 1.1)
                    
                    # DeÄŸerleri Ã§ubuklarÄ±n Ã¼zerine yaz
                    for i, (idx, score) in enumerate(zip(similar_items[0], similar_items[1])):
                        ax.text(score + 0.01, i, f'{score:.3f}', va='center', fontsize=9)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # Ä°statistikler
                    with st.expander("ğŸ“ˆ Benzerlik Ä°statistikleri", expanded=False):
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Ortalama Benzerlik", f"{np.mean(similar_items[1]):.4f}")
                        with col2:
                            st.metric("Maksimum Benzerlik", f"{np.max(similar_items[1]):.4f}")
                        with col3:
                            st.metric("Minimum Benzerlik", f"{np.min(similar_items[1]):.4f}")
                        with col4:
                            st.metric("Standart Sapma", f"{np.std(similar_items[1]):.4f}")
                        
                        st.info(f"""
                        **Yorumlama:**
                        - **YÃ¼ksek benzerlik (>0.7)**: ÃœrÃ¼nler Ã§ok benzer, aynÄ± kategoride olabilir
                        - **Orta benzerlik (0.3-0.7)**: ÃœrÃ¼nler benzer Ã¶zelliklere sahip
                        - **DÃ¼ÅŸÃ¼k benzerlik (<0.3)**: ÃœrÃ¼nler farklÄ± kategorilerde olabilir
                        
                        **ÃœrÃ¼n {item_idx + 1}** iÃ§in en benzer Ã¼rÃ¼n: **ÃœrÃ¼n {similar_items[0][0] + 1}** (Benzerlik: {similar_items[1][0]:.4f})
                        """)


def show_performance_comparison():
    """Performans karÅŸÄ±laÅŸtÄ±rmasÄ±"""
    st.header("ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    
    # Info bÃ¶lÃ¼mÃ¼
    with st.expander("â„¹ï¸ Performans KarÅŸÄ±laÅŸtÄ±rmasÄ± HakkÄ±nda", expanded=False):
        st.markdown("""
        ### KarÅŸÄ±laÅŸtÄ±rÄ±lan Algoritmalar
        
        **SVD (Singular Value Decomposition)**:
        - âœ… Matematiksel olarak en kesin
        - âœ… HÄ±zlÄ± eÄŸitim
        - âŒ BÃ¼yÃ¼k veri setlerinde yavaÅŸ
        - âŒ Paralel Ã§alÄ±ÅŸmaya uygun deÄŸil
        
        **ALS (Alternating Least Squares)**:
        - âœ… BÃ¼yÃ¼k Ã¶lÃ§ekli veri setlerinde hÄ±zlÄ±
        - âœ… Paralel Ã§alÄ±ÅŸmaya uygun (Spark)
        - âœ… Sparse matrislerde iyi
        - âŒ Daha fazla iterasyon gerekir
        
        ### Metrikler
        
        **RMSE (Root Mean Square Error)**:
        - Tahmin hatasÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼
        - **DÃ¼ÅŸÃ¼k RMSE = Daha iyi performans**
        - FormÃ¼l: âˆš(Î£(tahmin - gerÃ§ek)Â² / n)
        
        **EÄŸitim SÃ¼resi**:
        - Modelin eÄŸitilmesi iÃ§in geÃ§en sÃ¼re
        - BÃ¼yÃ¼k veri setlerinde Ã¶nemli
        """)
    
    st.markdown("""
    FarklÄ± algoritmalarÄ±n performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    """)
    
    col1, col2 = st.columns(2)
    with col1:
        n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 300, 100)
    with col2:
        n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 150, 50)
    
    if st.button("KarÅŸÄ±laÅŸtÄ±r"):
        with st.spinner("Algoritmalar karÅŸÄ±laÅŸtÄ±rÄ±lÄ±yor..."):
            # Veri oluÅŸtur
            rating_matrix = generate_rating_matrix(
                n_users=n_users, 
                n_items=n_items, 
                sparsity=0.7
            )
            
            # Train-test split (sparse matrix desteÄŸi ile)
            from scipy.sparse import issparse
            
            np.random.seed(42)
            
            if issparse(rating_matrix):
                # Sparse matrix iÃ§in
                rows, cols = rating_matrix.nonzero()
                n_ratings = len(rows)
                test_size = min(int(0.2 * n_ratings), 10000)
                test_sample_indices = np.random.choice(n_ratings, size=test_size, replace=False)
                
                test_rows = rows[test_sample_indices]
                test_cols = cols[test_sample_indices]
                # Sparse matrix'ten deÄŸerleri al - matrix objesi iÃ§in np.array kullan
                test_matrix_slice = rating_matrix[test_rows, test_cols]
                # matrix objesi iÃ§in A property veya np.array kullan
                if hasattr(test_matrix_slice, 'A'):
                    test_values = test_matrix_slice.A.flatten()
                elif hasattr(test_matrix_slice, 'toarray'):
                    test_values = test_matrix_slice.toarray().flatten()
                else:
                    test_values = np.array(test_matrix_slice).flatten()
                
                train_matrix = rating_matrix.copy()
                train_matrix[test_rows, test_cols] = 0
                train_matrix.eliminate_zeros()
                
                # Test matrix oluÅŸtur (dense format, evaluate iÃ§in)
                test_matrix = np.full_like(rating_matrix.toarray(), np.nan)
                test_matrix[test_rows, test_cols] = test_values
                
                test_indices = (test_rows, test_cols)
            else:
                # Dense matrix iÃ§in
                mask = ~np.isnan(rating_matrix)
                n_ratings = np.sum(mask)
                test_size = min(int(0.2 * n_ratings), 10000)
                
                valid_indices = np.where(mask)
                test_sample_indices = np.random.choice(
                    len(valid_indices[0]), 
                    size=test_size, 
                    replace=False
                )
                
                test_mask = np.zeros_like(mask, dtype=bool)
                test_mask[valid_indices[0][test_sample_indices], valid_indices[1][test_sample_indices]] = True
                
                train_matrix = rating_matrix.copy()
                train_matrix[test_mask] = np.nan
                
                # Test matrix oluÅŸtur (evaluate iÃ§in)
                test_matrix = np.full_like(rating_matrix, np.nan)
                test_matrix[test_mask] = rating_matrix[test_mask]
                
                test_values = rating_matrix[test_mask]
                test_indices = np.where(test_mask)
            
            results = {}
            
            # SVD
            with st.spinner("SVD eÄŸitiliyor..."):
                svd_model = SVDRecommender(n_components=20)
                svd_model.fit(train_matrix)
                svd_rmse = svd_model.evaluate(test_matrix)
                results['SVD'] = svd_rmse
            
            # ALS
            with st.spinner("ALS eÄŸitiliyor..."):
                als_model = ALSRecommender(n_factors=20, regularization=0.1, iterations=15)
                als_model.fit(train_matrix)
                als_rmse = als_model.evaluate(test_matrix)
                results['ALS'] = als_rmse
            
            # SonuÃ§lar
            results_df = pd.DataFrame({
                'Algoritma': list(results.keys()),
                'RMSE': list(results.values())
            })
            
            st.subheader("ğŸ“Š RMSE KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            with st.expander("â„¹ï¸ Bu KarÅŸÄ±laÅŸtÄ±rma Ne GÃ¶steriyor?", expanded=False):
                st.markdown("""
                **RMSE KarÅŸÄ±laÅŸtÄ±rmasÄ±**:
                - **RMSE (Root Mean Square Error)**: Tahmin hatasÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼
                - **DÃ¼ÅŸÃ¼k RMSE**: Daha iyi tahmin, daha az hata
                - **YÃ¼ksek RMSE**: Daha kÃ¶tÃ¼ tahmin, daha fazla hata
                
                **Yorumlama**:
                - En dÃ¼ÅŸÃ¼k RMSE'ye sahip algoritma en iyi performansÄ± gÃ¶sterir
                - Fark kÃ¼Ã§Ã¼kse: Algoritmalar benzer performans
                - Fark bÃ¼yÃ¼kse: Bir algoritma diÄŸerinden belirgin ÅŸekilde daha iyi
                
                **Not**: Bu sonuÃ§lar veri setine ve parametrelere baÄŸlÄ±dÄ±r. 
                FarklÄ± veri setlerinde sonuÃ§lar deÄŸiÅŸebilir.
                """)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.dataframe(results_df, width='stretch')
            
            with col2:
                fig, ax = plt.subplots(figsize=(8, 5))
                colors = ['#1f77b4' if x == results_df['RMSE'].min() else '#ff7f0e' 
                         for x in results_df['RMSE']]
                ax.bar(results_df['Algoritma'], results_df['RMSE'], alpha=0.7, color=colors)
                ax.set_ylabel('RMSE (DÃ¼ÅŸÃ¼k = Daha Ä°yi)', fontsize=12)
                ax.set_title('Algoritma Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=14, fontweight='bold')
                ax.grid(True, alpha=0.3, axis='y')
                st.pyplot(fig)
            
            # En iyi algoritma
            best_algorithm = results_df.loc[results_df['RMSE'].idxmin(), 'Algoritma']
            best_rmse = results_df['RMSE'].min()
            worst_rmse = results_df['RMSE'].max()
            improvement = ((worst_rmse - best_rmse) / worst_rmse * 100)
            
            st.success(f"ğŸ† **En iyi performans**: **{best_algorithm}** (RMSE: {best_rmse:.4f})")
            st.info(f"ğŸ’¡ {best_algorithm}, diÄŸer algoritmaya gÃ¶re %{improvement:.1f} daha iyi performans gÃ¶steriyor.")


# ==================== MODERN ALGORITHM PAGES ====================

def show_ncf_recommender():
    """NCF (Neural Collaborative Filtering) sayfasÄ±"""
    st.header("ğŸ§  Neural Collaborative Filtering (NCF)")
    
    with st.expander("â„¹ï¸ NCF Nedir?", expanded=False):
        st.markdown("""
        **Neural Collaborative Filtering (NCF)**, SVD'nin modern Deep Learning versiyonudur.
        
        **FarklarÄ±:**
        - âœ… SVD sadece **doÄŸrusal** (linear) iliÅŸkileri yakalar
        - âœ… NCF **doÄŸrusal olmayan** (non-linear) iliÅŸkileri Ã¶ÄŸrenir
        - âœ… KullanÄ±cÄ± davranÄ±ÅŸlarÄ±ndaki karmaÅŸÄ±k pattern'leri yakalar
        - âœ… Embedding + Multi-Layer Perceptron (MLP) kullanÄ±r
        
        **KullanÄ±m AlanlarÄ±:**
        - Netflix, YouTube gibi modern Ã¶neri sistemleri
        - BÃ¼yÃ¼k Ã¶lÃ§ekli e-ticaret platformlarÄ±
        """)
    
    # Veri yÃ¼kleme seÃ§eneÄŸi
    data_source = st.radio(
        "Veri KaynaÄŸÄ±",
        ["ğŸ“Š Ã–rnek Veri OluÅŸtur", "ğŸ“ Dosyadan YÃ¼kle"],
        horizontal=True,
        key="ncf_data_source"
    )
    
    # Session state ile veriyi koru
    if 'ncf_rating_matrix' not in st.session_state:
        st.session_state.ncf_rating_matrix = None
        st.session_state.ncf_user_mapping = None
        st.session_state.ncf_item_mapping = None
    
    rating_matrix = st.session_state.ncf_rating_matrix
    user_mapping = st.session_state.ncf_user_mapping
    item_mapping = st.session_state.ncf_item_mapping
    n_users = None
    n_items = None
    
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
        st.info("""
        **Desteklenen Formatlar:** CSV, Excel (.xlsx, .xls)
        
        **Veri FormatÄ±:** Long Format (user_id, item_id, rating) veya Matrix Format
        """)
        
        file = st.file_uploader(
            "Veri dosyasÄ±nÄ± seÃ§in",
            type=['csv', 'xlsx', 'xls'],
            help="CSV veya Excel dosyasÄ± yÃ¼kleyin",
            key="ncf_file"
        )
        
        if file is not None:
            try:
                # Dosya Ã¶nizlemesi ve format Ã¶nerisi
                import io
                file_content = file.read()
                st.session_state.ncf_file_content = file_content
                file_bytes = io.BytesIO(file_content)
                
                import pandas as pd
                if file.name.endswith('.csv'):
                    file_bytes.seek(0)
                    first_line = file_bytes.readline().decode('utf-8', errors='ignore')
                    delimiters = [',', ';', '\t', '|']
                    detected_delimiter = ','
                    max_cols = 0
                    for delim in delimiters:
                        cols = first_line.split(delim)
                        if len(cols) > max_cols:
                            max_cols = len(cols)
                            detected_delimiter = delim
                    
                    file_bytes.seek(0)
                    preview_df = pd.read_csv(file_bytes, nrows=5, sep=detected_delimiter, engine='python')
                    file_bytes.seek(0)
                    total_df = pd.read_csv(file_bytes, sep=detected_delimiter, engine='python')
                elif file.name.endswith(('.xlsx', '.xls')):
                    file_bytes.seek(0)
                    preview_df = pd.read_excel(file_bytes, nrows=5)
                    file_bytes.seek(0)
                    total_df = pd.read_excel(file_bytes)
                else:
                    preview_df = None
                    total_df = None
                
                if preview_df is not None:
                    with st.expander("ğŸ‘ï¸ Dosya Ã–nizleme (Ä°lk 5 SatÄ±r)", expanded=False):
                        st.dataframe(preview_df, width='stretch')
                        st.info(f"""
                        **Dosya Bilgileri:**
                        - **SatÄ±r SayÄ±sÄ±**: {len(total_df) if total_df is not None else 'Bilinmiyor'}
                        - **SÃ¼tun SayÄ±sÄ±**: {len(preview_df.columns)}
                        - **SÃ¼tun Ä°simleri**: {', '.join(preview_df.columns.tolist()[:10])}{'...' if len(preview_df.columns) > 10 else ''}
                        
                        **Format Ã–nerisi:**
                        - **3 sÃ¼tun varsa** â†’ Long Format seÃ§in (user_id, item_id, rating)
                        - **10+ sÃ¼tun varsa** â†’ Matrix Format seÃ§in (ilk sÃ¼tun kullanÄ±cÄ± ID, diÄŸerleri Ã¼rÃ¼n ID)
                        """)
                
                data_format = st.radio(
                    "Veri FormatÄ±",
                    ["Long Format (user_id, item_id, rating)", "Matrix Format (Rating Matrisi)"],
                    horizontal=True,
                    key="ncf_format"
                )
                
                if data_format == "Long Format (user_id, item_id, rating)":
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="ncf_load_long"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'ncf_file_content' in st.session_state:
                                import io
                                file_bytes = io.BytesIO(st.session_state.ncf_file_content)
                                file_bytes.name = file_name
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(file_bytes)
                            else:
                                file.seek(0)
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(file)
                            
                            st.session_state.ncf_file_name = file_name
                            st.session_state.ncf_file_size = file_size
                            st.session_state.ncf_rating_matrix = rating_matrix
                            st.session_state.ncf_user_mapping = user_mapping
                            st.session_state.ncf_item_mapping = item_mapping
                            
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.info(f"ğŸ“Š Veri yoÄŸunluÄŸu: {density:.2f}%")
                            st.rerun()
                else:
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="ncf_load_matrix"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'ncf_file_content' in st.session_state:
                                import io
                                file_bytes = io.BytesIO(st.session_state.ncf_file_content)
                                file_bytes.name = file_name
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file_bytes)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.stop()
                            else:
                                file.seek(0)
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.stop()
                            
                            st.session_state.ncf_file_name = file_name
                            st.session_state.ncf_file_size = file_size
                            st.session_state.ncf_rating_matrix = rating_matrix
                            st.session_state.ncf_user_mapping = None
                            st.session_state.ncf_item_mapping = None
                            
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.info(f"ğŸ“Š Veri yoÄŸunluÄŸu: {density:.2f}%")
                            st.rerun()
                            
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen veri formatÄ±nÄ± kontrol edin.")
    else:
        # Ã–rnek veri oluÅŸtur - session state'i temizle
        if st.session_state.ncf_rating_matrix is not None:
            st.session_state.ncf_rating_matrix = None
            st.session_state.ncf_user_mapping = None
            st.session_state.ncf_item_mapping = None
    
    # Model parametreleri
    if rating_matrix is not None:
        n_users, n_items = rating_matrix.shape
        st.info(f"ğŸ“Š YÃ¼klenen veri: {n_users} kullanÄ±cÄ±, {n_items} Ã¼rÃ¼n")
    else:
        col1, col2 = st.columns(2)
        with col1:
            n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 500, 100, key="ncf_n_users")
            n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 300, 50, key="ncf_n_items")
        with col2:
            sparsity = st.slider("Sparsity (Eksik Rating OranÄ±)", 0.3, 0.9, 0.6, key="ncf_sparsity")
    
    # Optimal parametreleri al
    optimal_params = get_optimal_model_params("ncf")
    optimal_n_factors = optimal_params['n_factors']
    optimal_epochs = optimal_params['epochs']
    optimal_batch_size = optimal_params['batch_size']
    optimal_dropout = optimal_params['dropout']
    
    n_factors = st.slider(
        "Latent FaktÃ¶r SayÄ±sÄ±", 
        10, 100, 
        optimal_n_factors, 
        key="ncf_n_factors",
        help=f"Ã–nerilen deÄŸer: {optimal_n_factors}"
    )
    
    col3, col4 = st.columns(2)
    with col3:
        epochs = st.slider("Epochs (EÄŸitim Ä°terasyonu)", 5, 50, 10, key="ncf_epochs")
        batch_size = st.slider("Batch Size", 32, 512, 256, key="ncf_batch_size")
    with col4:
        hidden_layers_str = st.text_input("Gizli Katmanlar (virgÃ¼lle ayÄ±rÄ±n)", "64,32,16", key="ncf_hidden")
        dropout = st.slider(
            "Dropout Rate", 
            0.0, 0.5, 
            optimal_dropout, 
            key="ncf_dropout",
            help=f"Ã–nerilen deÄŸer: {optimal_dropout}"
        )
    
    if st.button("ğŸš€ Modeli EÄŸit ve Ã–neriler Ãœret", key="ncf_train"):
        if rating_matrix is None and data_source == "ğŸ“ Dosyadan YÃ¼kle":
            st.warning("âš ï¸ LÃ¼tfen Ã¶nce veri dosyasÄ±nÄ± yÃ¼kleyin!")
        else:
            with st.spinner("Veri hazÄ±rlanÄ±yor..."):
                if rating_matrix is None:
                    rating_matrix = generate_rating_matrix(n_users, n_items, sparsity)
        
            # Sparse matrix'i dense'e Ã§evir (NCF iÃ§in gerekli)
            from scipy.sparse import issparse
            if issparse(rating_matrix):
                rating_matrix_dense = rating_matrix.toarray()
                # 0 deÄŸerlerini NaN'a Ã§evir
                rating_matrix_dense = np.where(rating_matrix_dense == 0, np.nan, rating_matrix_dense)
            else:
                rating_matrix_dense = rating_matrix.copy()
            
            with st.spinner("NCF modeli eÄŸitiliyor (bu biraz zaman alabilir)..."):
                try:
                    import time
                    training_start = time.time()
                    
                    hidden_layers = [int(x.strip()) for x in hidden_layers_str.split(',')]
                    ncf_model = NCFRecommender(
                        n_factors=n_factors,
                        hidden_layers=hidden_layers,
                        dropout_rate=dropout
                    )
                    history = ncf_model.fit(
                        rating_matrix_dense,
                        epochs=epochs,
                        batch_size=batch_size,
                        verbose=0
                    )
                    
                    training_time = time.time() - training_start
                    
                    st.success("âœ… Model eÄŸitildi!")
                    
                    # SonuÃ§ aÃ§Ä±klamasÄ±
                    with st.expander("ğŸ“ SonuÃ§ AÃ§Ä±klamasÄ± - Ne Elde Edildi?", expanded=True):
                        st.markdown("### ğŸ” KullanÄ±lan Veriler")
                        if data_source == "ğŸ“ Dosyadan YÃ¼kle" and 'ncf_file_name' in st.session_state:
                            from scipy.sparse import issparse as issparse_check
                            if issparse_check(rating_matrix):
                                n_ratings = rating_matrix.nnz
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                            
                            st.info(f"""
                            **Dosya**: {st.session_state.ncf_file_name} ({st.session_state.ncf_file_size / 1024:.2f} KB)
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            - **Toplam Rating**: {n_ratings:,}
                            """)
                        else:
                            st.info(f"""
                            **Ã–rnek Veri**:
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            """)
                        
                        st.markdown("### âš™ï¸ Model Parametreleri")
                        st.info(f"""
                        - **Latent FaktÃ¶r SayÄ±sÄ±**: {n_factors}
                        - **Gizli Katmanlar**: {hidden_layers_str}
                        - **Dropout Rate**: {dropout}
                        - **Epochs**: {epochs}
                        - **Batch Size**: {batch_size}
                        - **EÄŸitim SÃ¼resi**: {training_time:.2f} saniye
                        """)
                        
                        st.markdown("### ğŸ“ˆ Elde Edilen SonuÃ§lar")
                        st.success(f"""
                        **NCF Modeli BaÅŸarÄ±yla EÄŸitildi!**
                        
                        **Ne YapÄ±ldÄ±?**
                        1. **Embedding KatmanlarÄ±**: KullanÄ±cÄ± ve Ã¼rÃ¼nler {n_factors} boyutlu latent space'e embed edildi
                        2. **MLP (Multi-Layer Perceptron)**: {hidden_layers_str} yapÄ±sÄ±nda derin sinir aÄŸÄ± ile doÄŸrusal olmayan iliÅŸkiler Ã¶ÄŸrenildi
                        3. **EÄŸitim**: {epochs} epoch boyunca model optimize edildi
                        4. **SonuÃ§**: Model, kullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimlerinden karmaÅŸÄ±k pattern'leri Ã¶ÄŸrendi
                        
                        **SVD'den FarkÄ±:**
                        - SVD sadece doÄŸrusal iliÅŸkileri yakalar
                        - NCF doÄŸrusal olmayan, karmaÅŸÄ±k iliÅŸkileri Ã¶ÄŸrenir
                        - Daha gÃ¼Ã§lÃ¼ Ã¶zellik Ã¶ÄŸrenme kapasitesi
                        """)
                    
                    # EÄŸitim geÃ§miÅŸi
                    if history and hasattr(history, 'history'):
                        st.subheader("ğŸ“Š EÄŸitim GeÃ§miÅŸi")
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.plot(history.history['loss'], label='Training Loss', linewidth=2)
                        if 'val_loss' in history.history:
                            ax.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
                        ax.set_xlabel('Epoch', fontsize=12)
                        ax.set_ylabel('Loss', fontsize=12)
                        ax.set_title('NCF EÄŸitim GeÃ§miÅŸi - Loss DeÄŸiÅŸimi', fontsize=14, fontweight='bold')
                        ax.legend()
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)
                    
                    # Ã–neriler
                    st.subheader("ğŸ¯ KullanÄ±cÄ± Ã–nerileri")
                    user_idx = st.selectbox("KullanÄ±cÄ± SeÃ§in", range(min(10, n_users)), key="ncf_user_select")
                    
                    item_indices, predicted_ratings = ncf_model.recommend(
                        user_idx, n_recommendations=10, rating_matrix=rating_matrix_dense
                    )
                    
                    recommendations_df = pd.DataFrame({
                        'ÃœrÃ¼n ID': item_indices + 1,
                        'Tahmin Edilen Rating': np.round(np.clip(predicted_ratings, 1, 5), 2)
                    })
                    st.dataframe(recommendations_df, width='stretch')
                    
                except Exception as e:
                    st.error(f"âŒ Hata: {str(e)}")
                    st.info("ğŸ’¡ PyTorch yÃ¼klÃ¼ olduÄŸundan emin olun: `pip install torch`")


def show_autoencoder_denoising():
    """Denoising Autoencoder sayfasÄ±"""
    st.header("ğŸ¨ Denoising Autoencoder - GÃ¼rÃ¼ltÃ¼ Temizleme")
    
    with st.expander("â„¹ï¸ Autoencoder Nedir?", expanded=False):
        st.markdown("""
        **Autoencoder**, SVD ve PCA'in Deep Learning karÅŸÄ±lÄ±ÄŸÄ±dÄ±r.
        
        **NasÄ±l Ã‡alÄ±ÅŸÄ±r:**
        1. **Encoder**: Veriyi sÄ±kÄ±ÅŸtÄ±rÄ±r (latent space'e)
        2. **Decoder**: SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ veriyi tekrar geniÅŸletir
        3. **Ã–ÄŸrenme**: GÃ¼rÃ¼ltÃ¼lÃ¼ veriden temiz veri Ã¼retmeyi Ã¶ÄŸrenir
        
        **KullanÄ±m AlanlarÄ±:**
        - GÃ¶rÃ¼ntÃ¼ gÃ¼rÃ¼ltÃ¼ temizleme
        - Sinyal iÅŸleme
        - Veri sÄ±kÄ±ÅŸtÄ±rma
        """)
    
    # Veri kaynaÄŸÄ± seÃ§imi
    data_source = st.radio(
        "Veri KaynaÄŸÄ± SeÃ§in",
        ["ğŸ“ Dosya YÃ¼kle (CSV/Excel)", "ğŸ² Ã–rnek Veri OluÅŸtur"],
        horizontal=True
    )
    
    uploaded_file = None
    original_data = None
    noisy_data = None
    data_loaded = False
    file_bytes = None
    file_name = None
    numeric_cols = None
    df_numeric = None  # DataFrame referansÄ± (indirme iÃ§in)
    
    if data_source == "ğŸ“ Dosya YÃ¼kle (CSV/Excel)":
        st.markdown("### ğŸ“¤ Veri DosyasÄ± YÃ¼kle")
        uploaded_file = st.file_uploader(
            "Veri dosyasÄ± seÃ§in (CSV, Excel)",
            type=['csv', 'xlsx', 'xls'],
            help="YÃ¼klediÄŸiniz veri gÃ¼rÃ¼ltÃ¼ temizleme iÃ§in kullanÄ±lacaktÄ±r. Her satÄ±r bir Ã¶rnek, her sÃ¼tun bir Ã¶zellik olmalÄ±dÄ±r."
        )
        
        if uploaded_file is not None:
            try:
                import io
                file_name = uploaded_file.name
                
                # Dosya iÃ§eriÄŸini oku
                file_content = uploaded_file.read()
                file_bytes = io.BytesIO(file_content)
                file_bytes_ref = io.BytesIO(file_content)  # Referans iÃ§in kopya
                
                # Dosya tipine gÃ¶re yÃ¼kle
                if file_name.endswith('.csv'):
                    df = pd.read_csv(file_bytes)
                elif file_name.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file_bytes)
                else:
                    raise ValueError("Desteklenmeyen dosya formatÄ±")
                
                # SayÄ±sal sÃ¼tunlarÄ± seÃ§
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if len(numeric_cols) == 0:
                    raise ValueError("Dosyada sayÄ±sal sÃ¼tun bulunamadÄ±!")
                
                df_numeric = df[numeric_cols]
                
                # NaN deÄŸerleri doldur (ortalama ile)
                df_numeric = df_numeric.fillna(df_numeric.mean())
                
                # Veriyi numpy array'e Ã§evir
                df_numeric = df_numeric  # ReferansÄ± sakla
                original_data = df_numeric.values
                
                st.success(f"âœ… Veri yÃ¼klendi! {original_data.shape[0]} Ã¶rnek, {original_data.shape[1]} Ã¶zellik")
                
                # Veri Ã¶nizleme
                with st.expander("ğŸ“Š Veri Ã–nizleme", expanded=False):
                    st.dataframe(df_numeric.head(10), width='stretch')
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Ã–rnek SayÄ±sÄ±", original_data.shape[0])
                    with col2:
                        st.metric("Ã–zellik SayÄ±sÄ±", original_data.shape[1])
                    with col3:
                        st.metric("Veri Boyutu", f"{original_data.size:,}")
                
                data_loaded = True
                
            except Exception as e:
                st.error(f"âŒ Dosya okunurken hata oluÅŸtu: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen geÃ§erli bir CSV veya Excel dosyasÄ± yÃ¼klediÄŸinizden emin olun.")
    
    else:
        st.markdown("### ğŸ² Ã–rnek Veri")
        col1, col2 = st.columns(2)
        # Optimal parametreleri al
        optimal_params = get_optimal_model_params("autoencoder", n_samples=200, n_features=50)
        optimal_encoding_dim = optimal_params['encoding_dim']
        optimal_epochs = optimal_params['epochs']
        optimal_noise_factor = optimal_params['noise_factor']
        
        with col1:
            n_samples = st.slider("Ã–rnek SayÄ±sÄ±", 100, 1000, 200)
            n_features = st.slider("Ã–zellik SayÄ±sÄ±", 20, 200, 50)
        with col2:
            encoding_dim = st.slider(
                "Encoding Boyutu (Latent Space)", 
                5, 50, 
                optimal_encoding_dim,
                help=f"Ã–nerilen deÄŸer: {optimal_encoding_dim}"
            )
            noise_factor = st.slider(
                "GÃ¼rÃ¼ltÃ¼ FaktÃ¶rÃ¼", 
                0.1, 0.5, 
                optimal_noise_factor,
                help=f"Ã–nerilen deÄŸer: {optimal_noise_factor}"
            )
    
    # Epochs slider (her iki mod iÃ§in)
    epochs = st.slider(
        "Epochs", 
        10, 100, 
        optimal_epochs if data_source == "ğŸ² Ã–rnek Veri OluÅŸtur" else 50,
        help=f"Ã–nerilen deÄŸer: {optimal_epochs if data_source == 'ğŸ² Ã–rnek Veri OluÅŸtur' else 50}"
    )
    
    # GÃ¼rÃ¼ltÃ¼ faktÃ¶rÃ¼ (dosya yÃ¼klendiyse)
    if data_source == "ğŸ“ Dosya YÃ¼kle (CSV/Excel)" and data_loaded:
        # Optimal parametreleri al
        optimal_params = get_optimal_model_params("autoencoder", n_features=original_data.shape[1])
        optimal_encoding_dim = optimal_params['encoding_dim']
        optimal_noise_factor = optimal_params['noise_factor']
        
        col1, col2 = st.columns(2)
        with col1:
            encoding_dim = st.slider(
                "Encoding Boyutu (Latent Space)", 
                5, min(50, original_data.shape[1]//2), 
                optimal_encoding_dim, 
                key="file_encoding",
                help=f"Ã–nerilen deÄŸer: {optimal_encoding_dim}"
            )
        with col2:
            noise_factor = st.slider(
                "GÃ¼rÃ¼ltÃ¼ FaktÃ¶rÃ¼", 
                0.1, 0.5, 
                optimal_noise_factor, 
                key="file_noise",
                help=f"Ã–nerilen deÄŸer: {optimal_noise_factor}"
            )
    
    # Analiz butonu
    analyze_button = st.button("ğŸš€ GÃ¼rÃ¼ltÃ¼ Temizle")
    
    if analyze_button:
        if data_source == "ğŸ“ Dosya YÃ¼kle (CSV/Excel)":
            if uploaded_file is None or original_data is None:
                st.warning("âš ï¸ LÃ¼tfen Ã¶nce bir veri dosyasÄ± yÃ¼kleyin!")
            else:
                with st.spinner("GÃ¼rÃ¼ltÃ¼lÃ¼ veri oluÅŸturuluyor..."):
                    # GÃ¼rÃ¼ltÃ¼lÃ¼ veri oluÅŸtur
                    noisy_data = generate_noisy_data(original_data, noise_level=noise_factor)
                
                with st.spinner("Autoencoder eÄŸitiliyor (bu biraz zaman alabilir)..."):
                    try:
                        autoencoder = DenoisingAutoencoder(
                            encoding_dim=encoding_dim,
                            noise_factor=noise_factor
                        )
                        history = autoencoder.fit(
                            noisy_data,
                            epochs=epochs,
                            verbose=0
                        )
                        
                        st.success("âœ… Model eÄŸitildi!")
                        
                        # GÃ¼rÃ¼ltÃ¼ temizleme
                        with st.spinner("GÃ¼rÃ¼ltÃ¼ temizleniyor..."):
                            denoised_data = autoencoder.denoise(noisy_data)
                        
                        # Metrikler
                        mse_original = mean_squared_error(original_data, noisy_data)
                        mse_denoised = mean_squared_error(original_data, denoised_data)
                        improvement = ((mse_original - mse_denoised) / mse_original) * 100
                        
                        st.subheader("ğŸ“Š SonuÃ§lar")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("GÃ¼rÃ¼ltÃ¼lÃ¼ MSE", f"{mse_original:.6f}")
                        with col2:
                            st.metric("TemizlenmiÅŸ MSE", f"{mse_denoised:.6f}")
                        with col3:
                            st.metric("Ä°yileÅŸtirme", f"%{improvement:.1f}")
                        
                        if improvement > 0:
                            st.success(f"ğŸ‰ %{improvement:.1f} iyileÅŸtirme saÄŸlandÄ±!")
                        else:
                            st.warning("âš ï¸ GÃ¼rÃ¼ltÃ¼ temizleme sonucu beklenen iyileÅŸtirmeyi saÄŸlamadÄ±.")
                        
                        # GÃ¶rselleÅŸtirme
                        st.subheader("ğŸ“ˆ GÃ¶rselleÅŸtirme")
                        with st.expander("â„¹ï¸ Grafikler Ne GÃ¶steriyor?", expanded=False):
                            st.markdown("""
                            **KarÅŸÄ±laÅŸtÄ±rma Grafikleri**:
                            - **Orijinal Veri**: Temiz, gÃ¼rÃ¼ltÃ¼sÃ¼z orijinal veri
                            - **GÃ¼rÃ¼ltÃ¼lÃ¼ Veri**: GÃ¼rÃ¼ltÃ¼ eklenmiÅŸ veri
                            - **TemizlenmiÅŸ Veri**: Autoencoder ile gÃ¼rÃ¼ltÃ¼ temizlenmiÅŸ veri
                            
                            **Heatmap**: Veri matrisinin gÃ¶rselleÅŸtirmesi (ilk 50x50)
                            """)
                        
                        # Heatmap karÅŸÄ±laÅŸtÄ±rmasÄ±
                        max_vis = min(50, original_data.shape[0], original_data.shape[1])
                        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
                        
                        # Orijinal
                        im1 = axes[0].imshow(original_data[:max_vis, :max_vis], cmap='viridis', aspect='auto')
                        axes[0].set_title('Orijinal Veri', fontsize=12, fontweight='bold')
                        axes[0].set_xlabel('Ã–zellikler')
                        axes[0].set_ylabel('Ã–rnekler')
                        plt.colorbar(im1, ax=axes[0])
                        
                        # GÃ¼rÃ¼ltÃ¼lÃ¼
                        im2 = axes[1].imshow(noisy_data[:max_vis, :max_vis], cmap='viridis', aspect='auto')
                        axes[1].set_title('GÃ¼rÃ¼ltÃ¼lÃ¼ Veri', fontsize=12, fontweight='bold')
                        axes[1].set_xlabel('Ã–zellikler')
                        axes[1].set_ylabel('Ã–rnekler')
                        plt.colorbar(im2, ax=axes[1])
                        
                        # TemizlenmiÅŸ
                        im3 = axes[2].imshow(denoised_data[:max_vis, :max_vis], cmap='viridis', aspect='auto')
                        axes[2].set_title('TemizlenmiÅŸ Veri', fontsize=12, fontweight='bold')
                        axes[2].set_xlabel('Ã–zellikler')
                        axes[2].set_ylabel('Ã–rnekler')
                        plt.colorbar(im3, ax=axes[2])
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                        
                        # Ä°ndirilebilir sonuÃ§lar
                        st.subheader("ğŸ’¾ SonuÃ§larÄ± Ä°ndir")
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            # TemizlenmiÅŸ veriyi DataFrame'e Ã§evir
                            if numeric_cols is not None:
                                denoised_df = pd.DataFrame(denoised_data, columns=numeric_cols)
                            else:
                                denoised_df = pd.DataFrame(denoised_data, columns=[f'Feature_{i+1}' for i in range(denoised_data.shape[1])])
                            csv_denoised = denoised_df.to_csv(index=False).encode('utf-8')
                            st.download_button(
                                label="ğŸ“¥ TemizlenmiÅŸ Veriyi Ä°ndir (CSV)",
                                data=csv_denoised,
                                file_name="denoised_data.csv",
                                mime="text/csv"
                            )
                        
                        with col2:
                            # KarÅŸÄ±laÅŸtÄ±rma raporu
                            comparison_df = pd.DataFrame({
                                'Metrik': ['GÃ¼rÃ¼ltÃ¼lÃ¼ MSE', 'TemizlenmiÅŸ MSE', 'Ä°yileÅŸtirme (%)'],
                                'DeÄŸer': [mse_original, mse_denoised, improvement]
                            })
                            csv_report = comparison_df.to_csv(index=False).encode('utf-8')
                            st.download_button(
                                label="ğŸ“Š Raporu Ä°ndir (CSV)",
                                data=csv_report,
                                file_name="denoising_report.csv",
                                mime="text/csv"
                            )
                        
                        # EÄŸitim geÃ§miÅŸi
                        if history and 'loss' in history:
                            st.subheader("ğŸ“‰ EÄŸitim GeÃ§miÅŸi")
                            fig_history, ax_history = plt.subplots(figsize=(10, 5))
                            ax_history.plot(history['loss'], label='Training Loss', linewidth=2)
                            if 'val_loss' in history:
                                ax_history.plot(history['val_loss'], label='Validation Loss', linewidth=2)
                            ax_history.set_xlabel('Epoch')
                            ax_history.set_ylabel('Loss')
                            ax_history.set_title('Model EÄŸitim GeÃ§miÅŸi', fontsize=14, fontweight='bold')
                            ax_history.legend()
                            ax_history.grid(True, alpha=0.3)
                            plt.tight_layout()
                            st.pyplot(fig_history)
                        
                    except Exception as e:
                        st.error(f"âŒ Analiz sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}")
                        import traceback
                        with st.expander("ğŸ” DetaylÄ± Hata MesajÄ±"):
                            st.code(traceback.format_exc())
        
        else:
            # Ã–rnek veri
            with st.spinner("Veri oluÅŸturuluyor..."):
                # Orijinal veri
                original_data = generate_sample_data(n_samples, n_features)[0]
                # GÃ¼rÃ¼ltÃ¼lÃ¼ veri
                noisy_data = generate_noisy_data(original_data, noise_level=noise_factor)
            
            with st.spinner("Autoencoder eÄŸitiliyor..."):
                try:
                    autoencoder = DenoisingAutoencoder(
                        encoding_dim=encoding_dim,
                        noise_factor=noise_factor
                    )
                    history = autoencoder.fit(
                        noisy_data,
                        epochs=epochs,
                        verbose=0
                    )
                    
                    st.success("âœ… Model eÄŸitildi!")
                    
                    # GÃ¼rÃ¼ltÃ¼ temizleme
                    denoised_data = autoencoder.denoise(noisy_data)
                    
                    # Metrikler
                    mse_original = mean_squared_error(original_data, noisy_data)
                    mse_denoised = mean_squared_error(original_data, denoised_data)
                    improvement = ((mse_original - mse_denoised) / mse_original) * 100
                    
                    st.subheader("ğŸ“Š SonuÃ§lar")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("GÃ¼rÃ¼ltÃ¼lÃ¼ MSE", f"{mse_original:.4f}")
                    with col2:
                        st.metric("TemizlenmiÅŸ MSE", f"{mse_denoised:.4f}")
                    with col3:
                        st.metric("Ä°yileÅŸtirme", f"%{improvement:.1f}")
                    
                    if improvement > 0:
                        st.success(f"ğŸ‰ %{improvement:.1f} iyileÅŸtirme saÄŸlandÄ±!")
                    
                    # GÃ¶rselleÅŸtirme
                    st.subheader("ğŸ“ˆ GÃ¶rselleÅŸtirme")
                    max_vis = min(50, original_data.shape[0], original_data.shape[1])
                    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
                    
                    # Orijinal
                    im1 = axes[0].imshow(original_data[:max_vis, :max_vis], cmap='viridis', aspect='auto')
                    axes[0].set_title('Orijinal Veri', fontsize=12, fontweight='bold')
                    axes[0].set_xlabel('Ã–zellikler')
                    axes[0].set_ylabel('Ã–rnekler')
                    plt.colorbar(im1, ax=axes[0])
                    
                    # GÃ¼rÃ¼ltÃ¼lÃ¼
                    im2 = axes[1].imshow(noisy_data[:max_vis, :max_vis], cmap='viridis', aspect='auto')
                    axes[1].set_title('GÃ¼rÃ¼ltÃ¼lÃ¼ Veri', fontsize=12, fontweight='bold')
                    axes[1].set_xlabel('Ã–zellikler')
                    axes[1].set_ylabel('Ã–rnekler')
                    plt.colorbar(im2, ax=axes[1])
                    
                    # TemizlenmiÅŸ
                    im3 = axes[2].imshow(denoised_data[:max_vis, :max_vis], cmap='viridis', aspect='auto')
                    axes[2].set_title('TemizlenmiÅŸ Veri', fontsize=12, fontweight='bold')
                    axes[2].set_xlabel('Ã–zellikler')
                    axes[2].set_ylabel('Ã–rnekler')
                    plt.colorbar(im3, ax=axes[2])
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # EÄŸitim geÃ§miÅŸi
                    if history and 'loss' in history:
                        st.subheader("ğŸ“‰ EÄŸitim GeÃ§miÅŸi")
                        fig_history, ax_history = plt.subplots(figsize=(10, 5))
                        ax_history.plot(history['loss'], label='Training Loss', linewidth=2)
                        if 'val_loss' in history:
                            ax_history.plot(history['val_loss'], label='Validation Loss', linewidth=2)
                        ax_history.set_xlabel('Epoch')
                        ax_history.set_ylabel('Loss')
                        ax_history.set_title('Model EÄŸitim GeÃ§miÅŸi', fontsize=14, fontweight='bold')
                        ax_history.legend()
                        ax_history.grid(True, alpha=0.3)
                        plt.tight_layout()
                        st.pyplot(fig_history)
                    
                except Exception as e:
                    st.error(f"âŒ Hata: {str(e)}")
                    import traceback
                    with st.expander("ğŸ” DetaylÄ± Hata MesajÄ±"):
                        st.code(traceback.format_exc())


def show_vae_recommender():
    """VAE (Variational Autoencoder) sayfasÄ±"""
    st.header("ğŸ¯ Variational Autoencoder (VAE) - Ã–neri Sistemi")
    
    with st.expander("â„¹ï¸ VAE Nedir?", expanded=False):
        st.markdown("""
        **Variational Autoencoder (VAE)**, SVD'nin probabilistic, Deep Learning versiyonudur.
        
        **Ã–zellikleri:**
        - âœ… Latent space'de **probabilistic** daÄŸÄ±lÄ±m Ã¶ÄŸrenir
        - âœ… **KL divergence** ile dÃ¼zenli latent space
        - âœ… Daha iyi genelleme (generalization)
        
        **KullanÄ±m AlanlarÄ±:**
        - Ã–neri sistemleri
        - GÃ¶rÃ¼ntÃ¼ Ã¼retimi
        - Anomali tespiti
        """)
    
    # Veri yÃ¼kleme seÃ§eneÄŸi
    data_source = st.radio(
        "Veri KaynaÄŸÄ±",
        ["ğŸ“Š Ã–rnek Veri OluÅŸtur", "ğŸ“ Dosyadan YÃ¼kle"],
        horizontal=True,
        key="vae_data_source"
    )
    
    # Session state ile veriyi koru
    if 'vae_rating_matrix' not in st.session_state:
        st.session_state.vae_rating_matrix = None
        st.session_state.vae_user_mapping = None
        st.session_state.vae_item_mapping = None
    
    rating_matrix = st.session_state.vae_rating_matrix
    user_mapping = st.session_state.vae_user_mapping
    item_mapping = st.session_state.vae_item_mapping
    n_users = None
    n_items = None
    
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
        st.info("""
        **Desteklenen Formatlar:** CSV, Excel (.xlsx, .xls)
        
        **Veri FormatÄ±:** Long Format (user_id, item_id, rating) veya Matrix Format
        """)
        
        file = st.file_uploader(
            "Veri dosyasÄ±nÄ± seÃ§in",
            type=['csv', 'xlsx', 'xls'],
            help="CSV veya Excel dosyasÄ± yÃ¼kleyin",
            key="vae_file"
        )
        
        if file is not None:
            try:
                # Dosya Ã¶nizlemesi ve format Ã¶nerisi
                import io
                file_content = file.read()
                st.session_state.vae_file_content = file_content
                file_bytes = io.BytesIO(file_content)
                
                import pandas as pd
                if file.name.endswith('.csv'):
                    file_bytes.seek(0)
                    first_line = file_bytes.readline().decode('utf-8', errors='ignore')
                    delimiters = [',', ';', '\t', '|']
                    detected_delimiter = ','
                    max_cols = 0
                    for delim in delimiters:
                        cols = first_line.split(delim)
                        if len(cols) > max_cols:
                            max_cols = len(cols)
                            detected_delimiter = delim
                    
                    file_bytes.seek(0)
                    preview_df = pd.read_csv(file_bytes, nrows=5, sep=detected_delimiter, engine='python')
                    file_bytes.seek(0)
                    total_df = pd.read_csv(file_bytes, sep=detected_delimiter, engine='python')
                elif file.name.endswith(('.xlsx', '.xls')):
                    file_bytes.seek(0)
                    preview_df = pd.read_excel(file_bytes, nrows=5)
                    file_bytes.seek(0)
                    total_df = pd.read_excel(file_bytes)
                else:
                    preview_df = None
                    total_df = None
                
                if preview_df is not None:
                    with st.expander("ğŸ‘ï¸ Dosya Ã–nizleme (Ä°lk 5 SatÄ±r)", expanded=False):
                        st.dataframe(preview_df, width='stretch')
                        st.info(f"""
                        **Dosya Bilgileri:**
                        - **SatÄ±r SayÄ±sÄ±**: {len(total_df) if total_df is not None else 'Bilinmiyor'}
                        - **SÃ¼tun SayÄ±sÄ±**: {len(preview_df.columns)}
                        - **SÃ¼tun Ä°simleri**: {', '.join(preview_df.columns.tolist()[:10])}{'...' if len(preview_df.columns) > 10 else ''}
                        
                        **Format Ã–nerisi:**
                        - **3 sÃ¼tun varsa** â†’ Long Format seÃ§in (user_id, item_id, rating)
                        - **10+ sÃ¼tun varsa** â†’ Matrix Format seÃ§in (ilk sÃ¼tun kullanÄ±cÄ± ID, diÄŸerleri Ã¼rÃ¼n ID)
                        """)
                
                data_format = st.radio(
                    "Veri FormatÄ±",
                    ["Long Format (user_id, item_id, rating)", "Matrix Format (Rating Matrisi)"],
                    horizontal=True,
                    key="vae_format"
                )
                
                if data_format == "Long Format (user_id, item_id, rating)":
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="vae_load_long"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'vae_file_content' in st.session_state:
                                import io
                                file_bytes = io.BytesIO(st.session_state.vae_file_content)
                                file_bytes.name = file_name
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(file_bytes)
                            else:
                                file.seek(0)
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(file)
                            
                            st.session_state.vae_file_name = file_name
                            st.session_state.vae_file_size = file_size
                            st.session_state.vae_rating_matrix = rating_matrix
                            st.session_state.vae_user_mapping = user_mapping
                            st.session_state.vae_item_mapping = item_mapping
                            
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.info(f"ğŸ“Š Veri yoÄŸunluÄŸu: {density:.2f}%")
                            st.rerun()
                else:
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="vae_load_matrix"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'vae_file_content' in st.session_state:
                                import io
                                file_bytes = io.BytesIO(st.session_state.vae_file_content)
                                file_bytes.name = file_name
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file_bytes)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.stop()
                            else:
                                file.seek(0)
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.stop()
                            
                            st.session_state.vae_file_name = file_name
                            st.session_state.vae_file_size = file_size
                            st.session_state.vae_rating_matrix = rating_matrix
                            st.session_state.vae_user_mapping = None
                            st.session_state.vae_item_mapping = None
                            
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.info(f"ğŸ“Š Veri yoÄŸunluÄŸu: {density:.2f}%")
                            st.rerun()
                            
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen veri formatÄ±nÄ± kontrol edin.")
    else:
        # Ã–rnek veri oluÅŸtur - session state'i temizle
        if st.session_state.vae_rating_matrix is not None:
            st.session_state.vae_rating_matrix = None
            st.session_state.vae_user_mapping = None
            st.session_state.vae_item_mapping = None
    
    # Model parametreleri
    if rating_matrix is not None:
        n_users, n_items = rating_matrix.shape
        st.info(f"ğŸ“Š YÃ¼klenen veri: {n_users} kullanÄ±cÄ±, {n_items} Ã¼rÃ¼n")
    else:
        col1, col2 = st.columns(2)
        with col1:
            n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 500, 100, key="vae_n_users")
            n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 300, 50, key="vae_n_items")
        with col2:
            sparsity = st.slider("Sparsity", 0.3, 0.9, 0.6, key="vae_sparsity")
    
    # Optimal parametreleri al
    optimal_params = get_optimal_model_params("vae")
    optimal_latent_dim = optimal_params['latent_dim']
    optimal_epochs = optimal_params['epochs']
    
    latent_dim = st.slider(
        "Latent Dimension", 
        10, 100, 
        optimal_latent_dim, 
        key="vae_latent",
        help=f"Ã–nerilen deÄŸer: {optimal_latent_dim}"
    )
    epochs = st.slider(
        "Epochs", 
        10, 100, 
        optimal_epochs, 
        key="vae_epochs",
        help=f"Ã–nerilen deÄŸer: {optimal_epochs}"
    )
    
    if st.button("ğŸš€ VAE Modeli EÄŸit", key="vae_train"):
        if rating_matrix is None and data_source == "ğŸ“ Dosyadan YÃ¼kle":
            st.warning("âš ï¸ LÃ¼tfen Ã¶nce veri dosyasÄ±nÄ± yÃ¼kleyin!")
        else:
            with st.spinner("Veri hazÄ±rlanÄ±yor..."):
                if rating_matrix is None:
                    rating_matrix = generate_rating_matrix(n_users, n_items, sparsity)
            
            # Sparse matrix'i dense'e Ã§evir (VAE iÃ§in gerekli)
            from scipy.sparse import issparse
            if issparse(rating_matrix):
                rating_matrix_dense = rating_matrix.toarray()
                # 0 deÄŸerlerini NaN'a Ã§evir
                rating_matrix_dense = np.where(rating_matrix_dense == 0, np.nan, rating_matrix_dense)
            else:
                rating_matrix_dense = rating_matrix.copy()
            
            with st.spinner("VAE modeli eÄŸitiliyor..."):
                try:
                    import time
                    training_start = time.time()
                    
                    vae_model = VAERecommender(latent_dim=latent_dim)
                    history = vae_model.fit(
                        rating_matrix_dense,
                        epochs=epochs,
                        verbose=0
                    )
                    
                    training_time = time.time() - training_start
                    
                    st.success("âœ… Model eÄŸitildi!")
                    
                    # SonuÃ§ aÃ§Ä±klamasÄ±
                    with st.expander("ğŸ“ SonuÃ§ AÃ§Ä±klamasÄ± - Ne Elde Edildi?", expanded=True):
                        st.markdown("### ğŸ” KullanÄ±lan Veriler")
                        if data_source == "ğŸ“ Dosyadan YÃ¼kle" and 'vae_file_name' in st.session_state:
                            from scipy.sparse import issparse as issparse_check
                            if issparse_check(rating_matrix):
                                n_ratings = rating_matrix.nnz
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                            
                            st.info(f"""
                            **Dosya**: {st.session_state.vae_file_name} ({st.session_state.vae_file_size / 1024:.2f} KB)
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            - **Toplam Rating**: {n_ratings:,}
                            """)
                        else:
                            st.info(f"""
                            **Ã–rnek Veri**:
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            """)
                        
                        st.markdown("### âš™ï¸ Model Parametreleri")
                        st.info(f"""
                        - **Latent Dimension**: {latent_dim}
                        - **Epochs**: {epochs}
                        - **EÄŸitim SÃ¼resi**: {training_time:.2f} saniye
                        """)
                        
                        st.markdown("### ğŸ“ˆ Elde Edilen SonuÃ§lar")
                        st.success(f"""
                        **VAE Modeli BaÅŸarÄ±yla EÄŸitildi!**
                        
                        **Ne YapÄ±ldÄ±?**
                        1. **Encoder**: KullanÄ±cÄ± rating'leri {latent_dim} boyutlu latent space'e encode edildi
                        2. **Probabilistic Latent Space**: Latent space'de Gaussian daÄŸÄ±lÄ±m Ã¶ÄŸrenildi (mean + variance)
                        3. **KL Divergence**: Latent space dÃ¼zenlendi (overfitting Ã¶nlendi)
                        4. **Decoder**: Latent representation'dan rating'ler decode edildi
                        5. **EÄŸitim**: {epochs} epoch boyunca reconstruction + KL loss minimize edildi
                        
                        **SVD'den FarkÄ±:**
                        - SVD deterministik latent factors Ã¶ÄŸrenir
                        - VAE probabilistic latent distribution Ã¶ÄŸrenir
                        - Daha iyi genelleme (generalization) saÄŸlar
                        - KL divergence ile dÃ¼zenli latent space
                        """)
                    
                    # Ã–neriler
                    st.subheader("ğŸ¯ KullanÄ±cÄ± Ã–nerileri")
                    user_idx = st.selectbox("KullanÄ±cÄ± SeÃ§in", range(min(10, n_users)), key="vae_user_select")
                    
                    item_indices, predicted_ratings = vae_model.recommend(
                        user_idx, n_recommendations=10, rating_matrix=rating_matrix_dense
                    )
                    
                    recommendations_df = pd.DataFrame({
                        'ÃœrÃ¼n ID': item_indices + 1,
                        'Tahmin Edilen Rating': np.round(np.clip(predicted_ratings, 1, 5), 2)
                    })
                    st.dataframe(recommendations_df, width='stretch')
                    
                except Exception as e:
                    st.error(f"âŒ Hata: {str(e)}")
                    st.info("ğŸ’¡ PyTorch yÃ¼klÃ¼ olduÄŸundan emin olun: `pip install torch`")


def show_fm_recommender():
    """Factorization Machines sayfasÄ±"""
    st.header("ğŸ”— Factorization Machines (FM)")
    
    with st.expander("â„¹ï¸ FM Nedir?", expanded=False):
        st.markdown("""
        **Factorization Machines**, context-aware Ã¶neri sistemi saÄŸlar.
        
        **FarklarÄ±:**
        - âœ… Sadece kullanÄ±cÄ±-Ã¼rÃ¼n ID'sine bakmaz
        - âœ… **Yan bilgileri** (context) kullanÄ±r:
          - Saat, gÃ¼n, cihaz tipi
          - ÃœrÃ¼n Ã¶zellikleri (renk, kategori)
          - KullanÄ±cÄ± Ã¶zellikleri (yaÅŸ, konum)
        
        **KullanÄ±m AlanlarÄ±:**
        - Reklam tÄ±klama tahmini (CTR)
        - Context-aware Ã¶neriler
        """)
    
    # Veri yÃ¼kleme seÃ§eneÄŸi
    data_source = st.radio(
        "Veri KaynaÄŸÄ±",
        ["ğŸ“Š Ã–rnek Veri OluÅŸtur", "ğŸ“ Dosyadan YÃ¼kle"],
        horizontal=True,
        key="fm_data_source"
    )
    
    # Session state ile veriyi koru
    if 'fm_rating_matrix' not in st.session_state:
        st.session_state.fm_rating_matrix = None
        st.session_state.fm_user_mapping = None
        st.session_state.fm_item_mapping = None
    
    rating_matrix = st.session_state.fm_rating_matrix
    user_mapping = st.session_state.fm_user_mapping
    item_mapping = st.session_state.fm_item_mapping
    n_users = None
    n_items = None
    
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
        st.info("""
        **Desteklenen Formatlar:** CSV, Excel (.xlsx, .xls)
        
        **Veri FormatÄ±:** Long Format (user_id, item_id, rating) veya Matrix Format
        
        **Not:** FM iÃ§in context features otomatik oluÅŸturulacak (saat, cihaz tipi gibi).
        """)
        
        file = st.file_uploader(
            "Veri dosyasÄ±nÄ± seÃ§in",
            type=['csv', 'xlsx', 'xls'],
            help="CSV veya Excel dosyasÄ± yÃ¼kleyin",
            key="fm_file"
        )
        
        if file is not None:
            try:
                # Dosya Ã¶nizlemesi ve format Ã¶nerisi
                import io
                file_content = file.read()
                st.session_state.fm_file_content = file_content
                file_bytes = io.BytesIO(file_content)
                
                import pandas as pd
                if file.name.endswith('.csv'):
                    file_bytes.seek(0)
                    first_line = file_bytes.readline().decode('utf-8', errors='ignore')
                    delimiters = [',', ';', '\t', '|']
                    detected_delimiter = ','
                    max_cols = 0
                    for delim in delimiters:
                        cols = first_line.split(delim)
                        if len(cols) > max_cols:
                            max_cols = len(cols)
                            detected_delimiter = delim
                    
                    file_bytes.seek(0)
                    preview_df = pd.read_csv(file_bytes, nrows=5, sep=detected_delimiter, engine='python')
                    file_bytes.seek(0)
                    total_df = pd.read_csv(file_bytes, sep=detected_delimiter, engine='python')
                elif file.name.endswith(('.xlsx', '.xls')):
                    file_bytes.seek(0)
                    preview_df = pd.read_excel(file_bytes, nrows=5)
                    file_bytes.seek(0)
                    total_df = pd.read_excel(file_bytes)
                else:
                    preview_df = None
                    total_df = None
                
                if preview_df is not None:
                    with st.expander("ğŸ‘ï¸ Dosya Ã–nizleme (Ä°lk 5 SatÄ±r)", expanded=False):
                        st.dataframe(preview_df, width='stretch')
                        st.info(f"""
                        **Dosya Bilgileri:**
                        - **SatÄ±r SayÄ±sÄ±**: {len(total_df) if total_df is not None else 'Bilinmiyor'}
                        - **SÃ¼tun SayÄ±sÄ±**: {len(preview_df.columns)}
                        - **SÃ¼tun Ä°simleri**: {', '.join(preview_df.columns.tolist()[:10])}{'...' if len(preview_df.columns) > 10 else ''}
                        
                        **Format Ã–nerisi:**
                        - **3 sÃ¼tun varsa** â†’ Long Format seÃ§in (user_id, item_id, rating)
                        - **10+ sÃ¼tun varsa** â†’ Matrix Format seÃ§in (ilk sÃ¼tun kullanÄ±cÄ± ID, diÄŸerleri Ã¼rÃ¼n ID)
                        """)
                
                data_format = st.radio(
                    "Veri FormatÄ±",
                    ["Long Format (user_id, item_id, rating)", "Matrix Format (Rating Matrisi)"],
                    horizontal=True,
                    key="fm_format"
                )
                
                if data_format == "Long Format (user_id, item_id, rating)":
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="fm_load_long"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'fm_file_content' in st.session_state:
                                import io
                                file_bytes = io.BytesIO(st.session_state.fm_file_content)
                                file_bytes.name = file_name
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(file_bytes)
                            else:
                                file.seek(0)
                                rating_matrix, user_mapping, item_mapping = load_rating_data_from_file(file)
                            
                            st.session_state.fm_file_name = file_name
                            st.session_state.fm_file_size = file_size
                            st.session_state.fm_rating_matrix = rating_matrix
                            st.session_state.fm_user_mapping = user_mapping
                            st.session_state.fm_item_mapping = item_mapping
                            
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.info(f"ğŸ“Š Veri yoÄŸunluÄŸu: {density:.2f}%")
                            st.rerun()
                else:
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="fm_load_matrix"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'fm_file_content' in st.session_state:
                                import io
                                file_bytes = io.BytesIO(st.session_state.fm_file_content)
                                file_bytes.name = file_name
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file_bytes)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.stop()
                            else:
                                file.seek(0)
                                try:
                                    rating_matrix = load_rating_matrix_from_file(file)
                                except Exception as e:
                                    st.error(f"âŒ Hata: {str(e)}")
                                    st.stop()
                            
                            st.session_state.fm_file_name = file_name
                            st.session_state.fm_file_size = file_size
                            st.session_state.fm_rating_matrix = rating_matrix
                            st.session_state.fm_user_mapping = None
                            st.session_state.fm_item_mapping = None
                            
                            from scipy.sparse import issparse
                            if issparse(rating_matrix):
                                n_ratings = rating_matrix.nnz
                                density = rating_matrix.nnz / (rating_matrix.shape[0] * rating_matrix.shape[1]) * 100 if rating_matrix.shape[0] * rating_matrix.shape[1] > 0 else 0
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                                density = (1 - np.isnan(rating_matrix).sum() / rating_matrix.size) * 100 if rating_matrix.size > 0 else 0
                            
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.info(f"ğŸ“Š Veri yoÄŸunluÄŸu: {density:.2f}%")
                            st.rerun()
                            
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
                st.info("ğŸ’¡ LÃ¼tfen veri formatÄ±nÄ± kontrol edin.")
    else:
        # Ã–rnek veri oluÅŸtur - session state'i temizle
        if st.session_state.fm_rating_matrix is not None:
            st.session_state.fm_rating_matrix = None
            st.session_state.fm_user_mapping = None
            st.session_state.fm_item_mapping = None
    
    # Model parametreleri
    if rating_matrix is not None:
        n_users, n_items = rating_matrix.shape
        st.info(f"ğŸ“Š YÃ¼klenen veri: {n_users} kullanÄ±cÄ±, {n_items} Ã¼rÃ¼n")
    else:
        col1, col2 = st.columns(2)
        with col1:
            n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 500, 100, key="fm_n_users")
            n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 300, 50, key="fm_n_items")
        with col2:
            sparsity = st.slider("Sparsity", 0.3, 0.9, 0.6, key="fm_sparsity")
    
    st.info("ğŸ’¡ Bu demo iÃ§in basit context features oluÅŸturulacak.")
    
    # Optimal parametreleri al
    optimal_params = get_optimal_model_params("fm")
    optimal_epochs = optimal_params['epochs']
    
    n_factors = st.slider("FaktÃ¶r SayÄ±sÄ±", 5, 50, 10, key="fm_n_factors")
    epochs = st.slider(
        "Epochs", 
        5, 30, 
        optimal_epochs, 
        key="fm_epochs",
        help=f"Ã–nerilen deÄŸer: {optimal_epochs}"
    )
    
    if st.button("ğŸš€ FM Modeli EÄŸit", key="fm_train"):
        if rating_matrix is None and data_source == "ğŸ“ Dosyadan YÃ¼kle":
            st.warning("âš ï¸ LÃ¼tfen Ã¶nce veri dosyasÄ±nÄ± yÃ¼kleyin!")
        else:
            with st.spinner("Veri hazÄ±rlanÄ±yor..."):
                if rating_matrix is None:
                    rating_matrix = generate_rating_matrix(n_users, n_items, sparsity)
            
            # Context features oluÅŸtur (Ã¶rnek: saat, cihaz tipi)
            user_ids = []
            item_ids = []
            ratings = []
            context_features = []
            
            # Sparse matrix desteÄŸi
            from scipy.sparse import issparse
            if issparse(rating_matrix):
                # Sparse matrix iÃ§in - sadece mevcut rating'leri al
                rows, cols = rating_matrix.nonzero()
                user_ids = rows.tolist()
                item_ids = cols.tolist()
                ratings = rating_matrix.data.tolist()
                # Context features ekle
                np.random.seed(42)
                for _ in range(len(user_ids)):
                    context_features.append([
                        np.random.rand(),  # Ã–rnek: saat (normalize)
                        np.random.rand()   # Ã–rnek: cihaz tipi
                    ])
            else:
                # Dense matrix iÃ§in
                np.random.seed(42)
                for u in range(n_users):
                    for i in range(n_items):
                        if not np.isnan(rating_matrix[u, i]):
                            user_ids.append(u)
                            item_ids.append(i)
                            ratings.append(rating_matrix[u, i])
                            # Basit context: rastgele Ã¶zellikler
                            context_features.append([
                                np.random.rand(),  # Ã–rnek: saat (normalize)
                                np.random.rand()   # Ã–rnek: cihaz tipi
                            ])
            
            user_ids = np.array(user_ids)
            item_ids = np.array(item_ids)
            ratings = np.array(ratings)
            context_features = np.array(context_features)
        
            with st.spinner("FM modeli eÄŸitiliyor..."):
                try:
                    import time
                    training_start = time.time()
                    
                    fm_model = FactorizationMachine(n_factors=n_factors)
                    history = fm_model.fit(
                        user_ids, item_ids, ratings, context_features,
                        epochs=epochs,
                        verbose=0
                    )
                    
                    training_time = time.time() - training_start
                    
                    st.success("âœ… Model eÄŸitildi!")
                    
                    # SonuÃ§ aÃ§Ä±klamasÄ±
                    with st.expander("ğŸ“ SonuÃ§ AÃ§Ä±klamasÄ± - Ne Elde Edildi?", expanded=True):
                        st.markdown("### ğŸ” KullanÄ±lan Veriler")
                        if data_source == "ğŸ“ Dosyadan YÃ¼kle" and 'fm_file_name' in st.session_state:
                            from scipy.sparse import issparse as issparse_check
                            if issparse_check(rating_matrix):
                                n_ratings = rating_matrix.nnz
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                            
                            st.info(f"""
                            **Dosya**: {st.session_state.fm_file_name} ({st.session_state.fm_file_size / 1024:.2f} KB)
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            - **Toplam Rating**: {n_ratings:,}
                            - **Context Features**: 2 (saat, cihaz tipi - otomatik oluÅŸturuldu)
                            """)
                        else:
                            st.info(f"""
                            **Ã–rnek Veri**:
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            - **Context Features**: 2 (saat, cihaz tipi - otomatik oluÅŸturuldu)
                            """)
                        
                        st.markdown("### âš™ï¸ Model Parametreleri")
                        st.info(f"""
                        - **FaktÃ¶r SayÄ±sÄ±**: {n_factors}
                        - **Epochs**: {epochs}
                        - **EÄŸitim SÃ¼resi**: {training_time:.2f} saniye
                        """)
                        
                        st.markdown("### ğŸ“ˆ Elde Edilen SonuÃ§lar")
                        st.success(f"""
                        **FM Modeli BaÅŸarÄ±yla EÄŸitildi!**
                        
                        **Ne YapÄ±ldÄ±?**
                        1. **Feature Engineering**: KullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimleri + context features (saat, cihaz tipi) birleÅŸtirildi
                        2. **Factorization**: {n_factors} boyutlu latent factors Ã¶ÄŸrenildi
                        3. **Pairwise Interactions**: TÃ¼m feature Ã§iftleri arasÄ±ndaki etkileÅŸimler modellendi
                        4. **EÄŸitim**: {epochs} epoch boyunca model optimize edildi
                        
                        **SVD'den FarkÄ±:**
                        - SVD sadece kullanÄ±cÄ±-Ã¼rÃ¼n ID'lerini kullanÄ±r
                        - FM context features (saat, cihaz, konum vb.) kullanÄ±r
                        - Daha zengin Ã¶zellik seti ile daha iyi tahmin
                        - Context-aware Ã¶neriler saÄŸlar
                        """)
                    
                    st.info("ğŸ’¡ FM, context features kullanarak daha kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler saÄŸlar.")
                    
                except Exception as e:
                    st.error(f"âŒ Hata: {str(e)}")
                    st.info("ğŸ’¡ PyTorch yÃ¼klÃ¼ olduÄŸundan emin olun: `pip install torch`")


def show_deepfm_recommender():
    """DeepFM sayfasÄ±"""
    st.header("ğŸš€ DeepFM - Factorization Machines + Deep Learning")
    
    with st.expander("â„¹ï¸ DeepFM Nedir?", expanded=False):
        st.markdown("""
        **DeepFM**, FM ve Deep Learning'i birleÅŸtirir.
        
        **AvantajlarÄ±:**
        - âœ… **FM Component**: DoÄŸrusal ve Ã§ift etkileÅŸimleri yakalar
        - âœ… **Deep Component**: DoÄŸrusal olmayan karmaÅŸÄ±k pattern'leri Ã¶ÄŸrenir
        - âœ… Her iki yaklaÅŸÄ±mÄ±n gÃ¼Ã§lÃ¼ yÃ¶nlerini birleÅŸtirir
        
        **KullanÄ±m AlanlarÄ±:**
        - BÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶neri sistemleri
        - CTR tahmini
        """)
    
    # Veri yÃ¼kleme seÃ§eneÄŸi - FM ile aynÄ± yapÄ±
    data_source = st.radio(
        "Veri KaynaÄŸÄ±",
        ["ğŸ“Š Ã–rnek Veri OluÅŸtur", "ğŸ“ Dosyadan YÃ¼kle"],
        horizontal=True,
        key="deepfm_data_source"
    )
    
    # Session state ile veriyi koru
    if 'deepfm_rating_matrix' not in st.session_state:
        st.session_state.deepfm_rating_matrix = None
    
    rating_matrix = st.session_state.deepfm_rating_matrix
    n_users = None
    n_items = None
    
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
        st.info("""
        **Desteklenen Formatlar:** CSV, Excel (.xlsx, .xls)
        
        **Veri FormatÄ±:** Long Format (user_id, item_id, rating) veya Matrix Format
        
        **Not:** DeepFM iÃ§in context features otomatik oluÅŸturulacak.
        """)
        
        file = st.file_uploader(
            "Veri dosyasÄ±nÄ± seÃ§in",
            type=['csv', 'xlsx', 'xls'],
            help="CSV veya Excel dosyasÄ± yÃ¼kleyin",
            key="deepfm_file"
        )
        
        if file is not None:
            try:
                import io
                file_content = file.read()
                st.session_state.deepfm_file_content = file_content
                file_bytes = io.BytesIO(file_content)
                
                import pandas as pd
                if file.name.endswith('.csv'):
                    file_bytes.seek(0)
                    first_line = file_bytes.readline().decode('utf-8', errors='ignore')
                    delimiters = [',', ';', '\t', '|']
                    detected_delimiter = ','
                    for delim in delimiters:
                        if len(first_line.split(delim)) > len(first_line.split(detected_delimiter)):
                            detected_delimiter = delim
                    file_bytes.seek(0)
                    preview_df = pd.read_csv(file_bytes, nrows=5, sep=detected_delimiter, engine='python')
                elif file.name.endswith(('.xlsx', '.xls')):
                    file_bytes.seek(0)
                    preview_df = pd.read_excel(file_bytes, nrows=5)
                else:
                    preview_df = None
                
                if preview_df is not None:
                    with st.expander("ğŸ‘ï¸ Dosya Ã–nizleme (Ä°lk 5 SatÄ±r)", expanded=False):
                        st.dataframe(preview_df, width='stretch')
                
                data_format = st.radio(
                    "Veri FormatÄ±",
                    ["Long Format (user_id, item_id, rating)", "Matrix Format (Rating Matrisi)"],
                    horizontal=True,
                    key="deepfm_format"
                )
                
                if data_format == "Long Format (user_id, item_id, rating)":
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="deepfm_load_long"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'deepfm_file_content' in st.session_state:
                                file_bytes = io.BytesIO(st.session_state.deepfm_file_content)
                                file_bytes.name = file_name
                                rating_matrix, _, _ = load_rating_data_from_file(file_bytes)
                            else:
                                file.seek(0)
                                rating_matrix, _, _ = load_rating_data_from_file(file)
                            
                            st.session_state.deepfm_file_name = file_name
                            st.session_state.deepfm_file_size = file_size
                            st.session_state.deepfm_rating_matrix = rating_matrix
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.rerun()
                else:
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="deepfm_load_matrix"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'deepfm_file_content' in st.session_state:
                                file_bytes = io.BytesIO(st.session_state.deepfm_file_content)
                                file_bytes.name = file_name
                                rating_matrix = load_rating_matrix_from_file(file_bytes)
                            else:
                                file.seek(0)
                                rating_matrix = load_rating_matrix_from_file(file)
                            
                            st.session_state.deepfm_file_name = file_name
                            st.session_state.deepfm_file_size = file_size
                            st.session_state.deepfm_rating_matrix = rating_matrix
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.rerun()
                            
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
    else:
        if st.session_state.deepfm_rating_matrix is not None:
            st.session_state.deepfm_rating_matrix = None
    
    # Model parametreleri
    if rating_matrix is not None:
        n_users, n_items = rating_matrix.shape
        st.info(f"ğŸ“Š YÃ¼klenen veri: {n_users} kullanÄ±cÄ±, {n_items} Ã¼rÃ¼n")
    else:
        col1, col2 = st.columns(2)
        with col1:
            n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 500, 100, key="deepfm_n_users")
            n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 300, 50, key="deepfm_n_items")
        with col2:
            sparsity = st.slider("Sparsity", 0.3, 0.9, 0.6, key="deepfm_sparsity")
    
    st.info("ğŸ’¡ DeepFM, FM'nin geliÅŸmiÅŸ versiyonudur.")
    
    # Optimal parametreleri al
    optimal_params = get_optimal_model_params("deepfm")
    optimal_epochs = optimal_params['epochs']
    
    n_factors = st.slider("FM FaktÃ¶r SayÄ±sÄ±", 5, 50, 10, key="deepfm_n_factors")
    epochs = st.slider(
        "Epochs", 
        5, 30, 
        optimal_epochs, 
        key="deepfm_epochs",
        help=f"Ã–nerilen deÄŸer: {optimal_epochs}"
    )
    
    if st.button("ğŸš€ DeepFM Modeli EÄŸit", key="deepfm_train"):
        if rating_matrix is None and data_source == "ğŸ“ Dosyadan YÃ¼kle":
            st.warning("âš ï¸ LÃ¼tfen Ã¶nce veri dosyasÄ±nÄ± yÃ¼kleyin!")
        else:
            with st.spinner("Veri hazÄ±rlanÄ±yor..."):
                if rating_matrix is None:
                    rating_matrix = generate_rating_matrix(n_users, n_items, sparsity)
            
            user_ids = []
            item_ids = []
            ratings = []
            context_features = []
            
            # Sparse matrix desteÄŸi
            from scipy.sparse import issparse
            if issparse(rating_matrix):
                # Sparse matrix iÃ§in - sadece mevcut rating'leri al
                rows, cols = rating_matrix.nonzero()
                user_ids = rows.tolist()
                item_ids = cols.tolist()
                ratings = rating_matrix.data.tolist()
                # Context features ekle
                for _ in range(len(user_ids)):
                    context_features.append([np.random.rand(), np.random.rand()])
            else:
                # Dense matrix iÃ§in
                for u in range(n_users):
                    for i in range(n_items):
                        if not np.isnan(rating_matrix[u, i]):
                            user_ids.append(u)
                            item_ids.append(i)
                            ratings.append(rating_matrix[u, i])
                            context_features.append([np.random.rand(), np.random.rand()])
            
            user_ids = np.array(user_ids)
            item_ids = np.array(item_ids)
            ratings = np.array(ratings)
            context_features = np.array(context_features)
        
            with st.spinner("DeepFM modeli eÄŸitiliyor..."):
                try:
                    import time
                    training_start = time.time()
                    
                    deepfm_model = DeepFM(n_factors=n_factors)
                    history = deepfm_model.fit(
                        user_ids, item_ids, ratings, context_features,
                        epochs=epochs,
                        verbose=0
                    )
                    
                    training_time = time.time() - training_start
                    
                    st.success("âœ… Model eÄŸitildi!")
                    
                    # SonuÃ§ aÃ§Ä±klamasÄ±
                    with st.expander("ğŸ“ SonuÃ§ AÃ§Ä±klamasÄ± - Ne Elde Edildi?", expanded=True):
                        st.markdown("### ğŸ” KullanÄ±lan Veriler")
                        if data_source == "ğŸ“ Dosyadan YÃ¼kle" and 'deepfm_file_name' in st.session_state:
                            from scipy.sparse import issparse as issparse_check
                            if issparse_check(rating_matrix):
                                n_ratings = rating_matrix.nnz
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                            
                            st.info(f"""
                            **Dosya**: {st.session_state.deepfm_file_name} ({st.session_state.deepfm_file_size / 1024:.2f} KB)
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            - **Toplam Rating**: {n_ratings:,}
                            - **Context Features**: 2 (otomatik oluÅŸturuldu)
                            """)
                        else:
                            st.info(f"""
                            **Ã–rnek Veri**:
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            """)
                        
                        st.markdown("### âš™ï¸ Model Parametreleri")
                        st.info(f"""
                        - **FM FaktÃ¶r SayÄ±sÄ±**: {n_factors}
                        - **Epochs**: {epochs}
                        - **EÄŸitim SÃ¼resi**: {training_time:.2f} saniye
                        """)
                        
                        st.markdown("### ğŸ“ˆ Elde Edilen SonuÃ§lar")
                        st.success(f"""
                        **DeepFM Modeli BaÅŸarÄ±yla EÄŸitildi!**
                        
                        **Ne YapÄ±ldÄ±?**
                        1. **FM Component**: DoÄŸrusal ve pairwise etkileÅŸimler Ã¶ÄŸrenildi
                        2. **Deep Component**: Derin sinir aÄŸÄ± ile doÄŸrusal olmayan pattern'ler Ã¶ÄŸrenildi
                        3. **BirleÅŸik Model**: FM + Deep Learning birleÅŸtirildi
                        4. **EÄŸitim**: {epochs} epoch boyunca model optimize edildi
                        
                        **FM'den FarkÄ±:**
                        - FM sadece doÄŸrusal ve pairwise etkileÅŸimleri yakalar
                        - DeepFM ek olarak derin sinir aÄŸÄ± ile karmaÅŸÄ±k pattern'leri Ã¶ÄŸrenir
                        - Daha gÃ¼Ã§lÃ¼ Ã¶zellik Ã¶ÄŸrenme kapasitesi
                        """)
                    
                    st.info("ğŸ’¡ DeepFM, hem doÄŸrusal hem de doÄŸrusal olmayan Ã¶zellikleri Ã¶ÄŸrenir.")
                    
                except Exception as e:
                    st.error(f"âŒ Hata: {str(e)}")
                    st.info("ğŸ’¡ PyTorch yÃ¼klÃ¼ olduÄŸundan emin olun: `pip install torch`")


def show_transformer_recommender():
    """Transformer-based recommendation sayfasÄ±"""
    st.header("ğŸ”„ Transformer - Sequential Recommendation")
    
    with st.expander("â„¹ï¸ Transformer Nedir?", expanded=False):
        st.markdown("""
        **Transformer**, ChatGPT mimarisinin Ã¶neri sistemlerine uyarlanmÄ±ÅŸ halidir.
        
        **Ã–zellikleri:**
        - âœ… **Zaman ve sÄ±ra** bilgisini kullanÄ±r (SVD bunu yapamaz)
        - âœ… **Self-attention** mekanizmasÄ± ile uzun mesafe baÄŸÄ±mlÄ±lÄ±klarÄ± yakalar
        - âœ… "SÄ±radaki ne?" sorusunu cevaplar
        
        **KullanÄ±m AlanlarÄ±:**
        - TikTok, YouTube gibi sequential Ã¶neriler
        - E-ticaret sepet Ã¶nerileri
        """)
    
    col1, col2 = st.columns(2)
    with col1:
        n_items = st.slider("Toplam ÃœrÃ¼n SayÄ±sÄ±", 50, 500, 100)
        max_seq_length = st.slider("Maksimum Sequence UzunluÄŸu", 10, 100, 50)
    with col2:
        d_model = st.slider("Model Boyutu", 64, 256, 128)
        n_heads = st.slider("Attention Head SayÄ±sÄ±", 2, 8, 4)
    
    epochs = st.slider("Epochs", 5, 30, 10)
    
    if st.button("ğŸš€ Transformer Modeli EÄŸit"):
        with st.spinner("Sequential veri oluÅŸturuluyor..."):
            # Ã–rnek sequential veri (her kullanÄ±cÄ± iÃ§in item sequence)
            n_users = 50
            user_sequences = []
            for u in range(n_users):
                # Her kullanÄ±cÄ± iÃ§in rastgele item sequence
                seq_length = np.random.randint(5, max_seq_length)
                sequence = np.random.choice(n_items, size=seq_length, replace=False).tolist()
                user_sequences.append(sequence)
        
        with st.spinner("Transformer modeli eÄŸitiliyor..."):
            try:
                transformer_model = TransformerRecommender(
                    n_items=n_items,
                    d_model=d_model,
                    n_heads=n_heads,
                    max_seq_length=max_seq_length
                )
                history = transformer_model.fit(
                    user_sequences,
                    epochs=epochs,
                    verbose=0
                )
                
                st.success("âœ… Model eÄŸitildi!")
                
                # Ã–rnek tahmin
                st.subheader("SÄ±radaki Item Tahmini")
                example_user = st.selectbox("KullanÄ±cÄ± SeÃ§in", range(min(10, n_users)))
                example_sequence = user_sequences[example_user][:-1]  # Son item hariÃ§
                
                item_indices, probabilities = transformer_model.predict_next(example_sequence)
                
                recommendations_df = pd.DataFrame({
                    'SÄ±radaki ÃœrÃ¼n ID': item_indices + 1,
                    'OlasÄ±lÄ±k': np.round(probabilities, 4)
                })
                st.dataframe(recommendations_df, width='stretch')
                
                st.info(f"ğŸ’¡ KullanÄ±cÄ±nÄ±n geÃ§miÅŸ sequence'i: {[x+1 for x in example_sequence[-5:]]}")
                
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")


def show_gnn_recommender():
    """GNN (Graph Neural Network) sayfasÄ±"""
    st.header("ğŸ•¸ï¸ Graph Neural Network (GNN) - Ã–neri Sistemi")
    
    with st.expander("â„¹ï¸ GNN Nedir?", expanded=False):
        st.markdown("""
        **Graph Neural Network**, veriyi tablo deÄŸil, **aÄŸ (graph)** olarak gÃ¶rÃ¼r.
        
        **Ã–zellikleri:**
        - âœ… KullanÄ±cÄ±-Ã¼rÃ¼n iliÅŸkilerini graph olarak modeler
        - âœ… **Ä°liÅŸkisel veriyi** en iyi iÅŸleyen yÃ¶ntem
        - âœ… ArkadaÅŸlÄ±k aÄŸlarÄ±, Ã¼rÃ¼n benzerlik aÄŸlarÄ± kullanÄ±r
        
        **KullanÄ±m AlanlarÄ±:**
        - Pinterest, Uber Eats
        - Sosyal aÄŸ tabanlÄ± Ã¶neriler
        - Ä°liÅŸkisel veri analizi
        """)
    
    # PyTorch kontrolÃ¼
    if not MODERN_AVAILABLE:
        st.warning("âš ï¸ GNN iÃ§in PyTorch ve PyTorch Geometric gerekli!")
        st.info("ğŸ’¡ LÃ¼tfen ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: `pip install torch torch-geometric`")
        st.stop()
    else:
        st.success("âœ… PyTorch ve PyTorch Geometric yÃ¼klÃ¼ - GNN kullanÄ±labilir!")
    
    # Veri yÃ¼kleme seÃ§eneÄŸi
    data_source = st.radio(
        "Veri KaynaÄŸÄ±",
        ["ğŸ“Š Ã–rnek Veri OluÅŸtur", "ğŸ“ Dosyadan YÃ¼kle"],
        horizontal=True,
        key="gnn_data_source"
    )
    
    # Session state ile veriyi koru
    if 'gnn_rating_matrix' not in st.session_state:
        st.session_state.gnn_rating_matrix = None
    
    rating_matrix = st.session_state.gnn_rating_matrix
    n_users = None
    n_items = None
    
    if data_source == "ğŸ“ Dosyadan YÃ¼kle":
        st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
        st.info("""
        **Desteklenen Formatlar:** CSV, Excel (.xlsx, .xls)
        
        **Veri FormatÄ±:** Long Format (user_id, item_id, rating) veya Matrix Format
        """)
        
        file = st.file_uploader(
            "Veri dosyasÄ±nÄ± seÃ§in",
            type=['csv', 'xlsx', 'xls'],
            help="CSV veya Excel dosyasÄ± yÃ¼kleyin",
            key="gnn_file"
        )
        
        if file is not None:
            try:
                import io
                file_content = file.read()
                st.session_state.gnn_file_content = file_content
                
                data_format = st.radio(
                    "Veri FormatÄ±",
                    ["Long Format (user_id, item_id, rating)", "Matrix Format (Rating Matrisi)"],
                    horizontal=True,
                    key="gnn_format"
                )
                
                if data_format == "Long Format (user_id, item_id, rating)":
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="gnn_load_long"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'gnn_file_content' in st.session_state:
                                file_bytes = io.BytesIO(st.session_state.gnn_file_content)
                                file_bytes.name = file_name
                                rating_matrix, _, _ = load_rating_data_from_file(file_bytes)
                            else:
                                file.seek(0)
                                rating_matrix, _, _ = load_rating_data_from_file(file)
                            
                            st.session_state.gnn_file_name = file_name
                            st.session_state.gnn_file_size = file_size
                            st.session_state.gnn_rating_matrix = rating_matrix
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.rerun()
                else:
                    if st.button("ğŸ“¥ Veriyi YÃ¼kle", key="gnn_load_matrix"):
                        with st.spinner("Veri yÃ¼kleniyor..."):
                            file_name = file.name
                            file_size = file.size
                            
                            if 'gnn_file_content' in st.session_state:
                                file_bytes = io.BytesIO(st.session_state.gnn_file_content)
                                file_bytes.name = file_name
                                rating_matrix = load_rating_matrix_from_file(file_bytes)
                            else:
                                file.seek(0)
                                rating_matrix = load_rating_matrix_from_file(file)
                            
                            st.session_state.gnn_file_name = file_name
                            st.session_state.gnn_file_size = file_size
                            st.session_state.gnn_rating_matrix = rating_matrix
                            st.success(f"âœ… Veri yÃ¼klendi! {rating_matrix.shape[0]} kullanÄ±cÄ±, {rating_matrix.shape[1]} Ã¼rÃ¼n")
                            st.rerun()
                            
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
    else:
        if st.session_state.gnn_rating_matrix is not None:
            st.session_state.gnn_rating_matrix = None
    
    # Model parametreleri
    if rating_matrix is not None:
        n_users, n_items = rating_matrix.shape
        st.info(f"ğŸ“Š YÃ¼klenen veri: {n_users} kullanÄ±cÄ±, {n_items} Ã¼rÃ¼n")
    else:
        col1, col2 = st.columns(2)
        with col1:
            n_users = st.slider("KullanÄ±cÄ± SayÄ±sÄ±", 50, 300, 100, key="gnn_n_users")
            n_items = st.slider("ÃœrÃ¼n SayÄ±sÄ±", 30, 200, 50, key="gnn_n_items")
        with col2:
            sparsity = st.slider("Sparsity", 0.3, 0.9, 0.6, key="gnn_sparsity")
    
    # Optimal parametreleri al
    optimal_params = get_optimal_model_params("gnn")
    optimal_embedding_dim = optimal_params['embedding_dim']
    optimal_epochs = optimal_params['epochs']
    
    embedding_dim = st.slider(
        "Embedding Boyutu", 
        32, 128, 
        optimal_embedding_dim, 
        key="gnn_embedding",
        help=f"Ã–nerilen deÄŸer: {optimal_embedding_dim}"
    )
    epochs = st.slider(
        "Epochs", 
        10, 100, 
        optimal_epochs, 
        key="gnn_epochs",
        help=f"Ã–nerilen deÄŸer: {optimal_epochs}"
    )
    
    if st.button("ğŸš€ GNN Modeli EÄŸit", key="gnn_train"):
        if rating_matrix is None and data_source == "ğŸ“ Dosyadan YÃ¼kle":
            st.warning("âš ï¸ LÃ¼tfen Ã¶nce veri dosyasÄ±nÄ± yÃ¼kleyin!")
        else:
            with st.spinner("Veri hazÄ±rlanÄ±yor..."):
                if rating_matrix is None:
                    rating_matrix = generate_rating_matrix(n_users, n_items, sparsity)
            
            with st.spinner("GNN modeli eÄŸitiliyor (bu biraz zaman alabilir)..."):
                try:
                    import time
                    training_start = time.time()
                    
                    gnn_model = GNNRecommender(embedding_dim=embedding_dim)
                    gnn_model.fit(
                        rating_matrix,
                        epochs=epochs,
                        verbose=False
                    )
                    
                    training_time = time.time() - training_start
                    
                    st.success("âœ… Model eÄŸitildi!")
                    
                    # SonuÃ§ aÃ§Ä±klamasÄ±
                    with st.expander("ğŸ“ SonuÃ§ AÃ§Ä±klamasÄ± - Ne Elde Edildi?", expanded=True):
                        st.markdown("### ğŸ” KullanÄ±lan Veriler")
                        if data_source == "ğŸ“ Dosyadan YÃ¼kle" and 'gnn_file_name' in st.session_state:
                            from scipy.sparse import issparse as issparse_check
                            if issparse_check(rating_matrix):
                                n_ratings = rating_matrix.nnz
                            else:
                                mask = ~np.isnan(rating_matrix)
                                n_ratings = np.sum(mask)
                            
                            st.info(f"""
                            **Dosya**: {st.session_state.gnn_file_name} ({st.session_state.gnn_file_size / 1024:.2f} KB)
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            - **Toplam Rating**: {n_ratings:,}
                            """)
                        else:
                            st.info(f"""
                            **Ã–rnek Veri**:
                            - **KullanÄ±cÄ± SayÄ±sÄ±**: {n_users:,}
                            - **ÃœrÃ¼n SayÄ±sÄ±**: {n_items:,}
                            """)
                        
                        st.markdown("### âš™ï¸ Model Parametreleri")
                        st.info(f"""
                        - **Embedding Boyutu**: {embedding_dim}
                        - **Epochs**: {epochs}
                        - **EÄŸitim SÃ¼resi**: {training_time:.2f} saniye
                        """)
                        
                        st.markdown("### ğŸ“ˆ Elde Edilen SonuÃ§lar")
                        st.success(f"""
                        **GNN Modeli BaÅŸarÄ±yla EÄŸitildi!**
                        
                        **Ne YapÄ±ldÄ±?**
                        1. **Graph OluÅŸturma**: KullanÄ±cÄ±-Ã¼rÃ¼n etkileÅŸimleri graph yapÄ±sÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼
                        2. **Node Embedding**: Her kullanÄ±cÄ± ve Ã¼rÃ¼n {embedding_dim} boyutlu embedding'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼
                        3. **Message Passing**: Graph Ã¼zerinde mesaj geÃ§iÅŸi ile komÅŸu bilgileri toplandÄ±
                        4. **EÄŸitim**: {epochs} epoch boyunca model optimize edildi
                        
                        **SVD'den FarkÄ±:**
                        - SVD rating matrisini direkt faktÃ¶rize eder
                        - GNN kullanÄ±cÄ±-Ã¼rÃ¼n iliÅŸkilerini graph olarak modeler
                        - Ä°liÅŸkisel veriyi daha iyi iÅŸler
                        - Graph yapÄ±sÄ±ndan faydalanÄ±r
                        """)
                    
                    # Ã–neriler
                    st.subheader("ğŸ¯ KullanÄ±cÄ± Ã–nerileri")
                    user_idx = st.selectbox("KullanÄ±cÄ± SeÃ§in", range(min(10, n_users)), key="gnn_user_select")
                    
                    item_indices, predicted_ratings = gnn_model.recommend(
                        user_idx, n_recommendations=10, rating_matrix=rating_matrix
                    )
                    
                    recommendations_df = pd.DataFrame({
                        'ÃœrÃ¼n ID': item_indices + 1,
                        'Tahmin Edilen Rating': np.round(np.clip(predicted_ratings, 1, 5), 2)
                    })
                    st.dataframe(recommendations_df, width='stretch')
                    
                    st.info("ğŸ’¡ GNN, kullanÄ±cÄ±-Ã¼rÃ¼n iliÅŸkilerini graph olarak modelleyerek Ã¶neriler Ã¼retir.")
                    
                except ImportError:
                    st.error("âŒ PyTorch ve PyTorch Geometric yÃ¼klÃ¼ deÄŸil!")
                    st.info("ğŸ’¡ LÃ¼tfen ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: `pip install torch torch-geometric`")
                except Exception as e:
                    st.error(f"âŒ Hata: {str(e)}")
                    st.info("ğŸ’¡ PyTorch yÃ¼klÃ¼ olduÄŸundan emin olun: `pip install torch torch-geometric`")


def show_ai_chatbot():
    """
    AI Chatbot - Veri AsistanÄ± sayfasÄ±
    PandasAI kullanarak doÄŸal dil ile veri analizi yapÄ±lmasÄ±nÄ± saÄŸlar
    """
    st.header("ğŸ¤– AI Chat - Veri AsistanÄ±")
    
    with st.expander("â„¹ï¸ AI Chat Nedir?", expanded=False):
        st.markdown("""
        **AI Chat - Veri AsistanÄ±**, doÄŸal dil kullanarak veri analizi yapmanÄ±zÄ± saÄŸlar.
        
        **Ã–zellikler:**
        - âœ… Excel/CSV dosyalarÄ±nÄ± yÃ¼kleyin
        - âœ… DoÄŸal dil ile sorular sorun: "En Ã§ok puan veren kullanÄ±cÄ± kim?"
        - âœ… Grafikler oluÅŸturun: "Rating daÄŸÄ±lÄ±mÄ±nÄ± Ã§iz"
        - âœ… Veri temizleme: "BoÅŸ verileri temizle"
        - âœ… Ã–zel komutlar: "SVD Ã§alÄ±ÅŸtÄ±r" gibi matrix factorization iÅŸlemleri
        
        **KullanÄ±m:**
        1. OpenAI API Key'inizi girin (sidebar)
        2. Veri dosyanÄ±zÄ± yÃ¼kleyin
        3. SorularÄ±nÄ±zÄ± sorun!
        """)
    
    # Sidebar - API Key giriÅŸi
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ”‘ API AyarlarÄ±")
    
    # API Provider seÃ§imi
    api_provider = st.sidebar.radio(
        "AI Provider SeÃ§in",
        ["OpenAI", "Google Gemini"],
        help="Kullanmak istediÄŸiniz AI servisini seÃ§in",
        key="ai_provider"
    )
    
    if api_provider == "OpenAI":
        api_key = st.sidebar.text_input(
            "OpenAI API Key",
            type="password",
            help="OpenAI API key'inizi buraya girin. https://platform.openai.com/api-keys adresinden alabilirsiniz.",
            key="openai_api_key"
        )
        
        if not api_key:
            st.warning("âš ï¸ LÃ¼tfen OpenAI API Key'inizi sidebar'dan girin.")
            st.info("ğŸ’¡ API Key almak iÃ§in: https://platform.openai.com/api-keys")
            st.stop()
    else:  # Gemini
        api_key = st.sidebar.text_input(
            "Google Gemini API Key",
            type="password",
            help="Google Gemini API key'inizi buraya girin. https://aistudio.google.com/app/apikey adresinden alabilirsiniz.",
            key="gemini_api_key"
        )
        
        if not api_key:
            st.warning("âš ï¸ LÃ¼tfen Google Gemini API Key'inizi sidebar'dan girin.")
            st.info("ğŸ’¡ API Key almak iÃ§in: https://aistudio.google.com/app/apikey")
            st.stop()
    
    # Dosya yÃ¼kleme
    st.markdown("### ğŸ“ Veri DosyasÄ± YÃ¼kle")
    uploaded_file = st.file_uploader(
        "Excel veya CSV dosyasÄ± seÃ§in",
        type=['csv', 'xlsx', 'xls'],
        help="Analiz yapmak istediÄŸiniz veri dosyasÄ±nÄ± yÃ¼kleyin"
    )
    
    # Session state ile chat geÃ§miÅŸini ve DataFrame'i koru
    if 'chat_messages' not in st.session_state:
        st.session_state.chat_messages = []
    
    if 'df' not in st.session_state:
        st.session_state.df = None
    
    if 'pandasai_agent' not in st.session_state:
        st.session_state.pandasai_agent = None
    
    # Dosya yÃ¼klendiÄŸinde DataFrame'e Ã§evir
    if uploaded_file is not None:
        try:
            import io
            import pandas as pd
            
            # Dosya tipine gÃ¶re oku
            if uploaded_file.name.endswith('.csv'):
                # CSV iÃ§in delimiter tespiti
                file_bytes = uploaded_file.read()
                first_line = file_bytes.decode('utf-8', errors='ignore').split('\n')[0]
                delimiters = [',', ';', '\t', '|']
                detected_delimiter = ','
                max_cols = 0
                for delim in delimiters:
                    cols = first_line.split(delim)
                    if len(cols) > max_cols:
                        max_cols = len(cols)
                        detected_delimiter = delim
                
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, sep=detected_delimiter, engine='python')
            elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(uploaded_file)
            else:
                st.error("âŒ Desteklenmeyen dosya formatÄ±!")
                st.stop()
            
            # DataFrame'i session state'e kaydet
            st.session_state.df = df
            st.session_state.chat_messages = []  # Yeni dosya yÃ¼klendiÄŸinde chat geÃ§miÅŸini temizle
            
            st.success(f"âœ… Dosya yÃ¼klendi! {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
            
            # Veri Ã¶nizlemesi
            with st.expander("ğŸ‘ï¸ Veri Ã–nizleme", expanded=False):
                st.dataframe(df.head(10), width='stretch')
                st.info(f"**SÃ¼tunlar**: {', '.join(df.columns.tolist()[:10])}{'...' if len(df.columns) > 10 else ''}")
            
        except Exception as e:
            st.error(f"âŒ Dosya yÃ¼klenirken hata oluÅŸtu: {str(e)}")
            st.stop()
    
    # DataFrame yoksa uyarÄ± gÃ¶ster
    if st.session_state.df is None:
        st.info("ğŸ’¡ LÃ¼tfen analiz yapmak iÃ§in bir veri dosyasÄ± yÃ¼kleyin.")
        st.stop()
    
    df = st.session_state.df
    
    # PandasAI agent'Ä± oluÅŸtur (sadece bir kez)
    if st.session_state.pandasai_agent is None:
        try:
            # PandasAI import - duplicate validator hatasÄ± olabilir
            try:
                from pandasai import SmartDataframe
            except (ValueError, TypeError) as import_error:
                error_str = str(import_error)
                if "duplicate validator" in error_str.lower() or "validate_llm" in error_str:
                    # Validator Ã§akÄ±ÅŸmasÄ± import sÄ±rasÄ±nda oluÅŸtu
                    st.warning("âš ï¸ PandasAI validator Ã§akÄ±ÅŸmasÄ± (import sÄ±rasÄ±nda)")
                    st.info("ğŸ’¡ Alternatif moda geÃ§iliyor...")
                    st.session_state.pandasai_agent = "alternative_mode"
                    raise  # DÄ±ÅŸ exception handler'a geÃ§
                else:
                    raise
            
            # LLM seÃ§imi (OpenAI veya Gemini)
            if api_provider == "OpenAI":
                try:
                    from pandasai.llm import OpenAI
                    llm = OpenAI(api_token=api_key)
                except ImportError:
                    st.warning("âš ï¸ PandasAI OpenAI LLM import edilemedi. Alternatif moda geÃ§iliyor...")
                    st.session_state.pandasai_agent = "alternative_mode"
            else:  # Gemini
                try:
                    from pandasai.llm import GoogleGemini
                    llm = GoogleGemini(api_key=api_key)
                except ImportError:
                    st.warning("âš ï¸ PandasAI Gemini LLM import edilemedi. Alternatif moda geÃ§iliyor...")
                    st.session_state.pandasai_agent = "alternative_mode"
            
            # EÄŸer alternatif moda geÃ§ildiyse, burayÄ± atla
            if st.session_state.pandasai_agent == "alternative_mode":
                pass  # AÅŸaÄŸÄ±daki alternatif mod kÄ±smÄ±na geÃ§
            else:
                # SmartDataframe oluÅŸtur (custom skills ile)
                # Not: Python 3.14 uyumluluk sorunlarÄ± olabilir
                try:
                    smart_df = SmartDataframe(
                    df,
                    config={
                        "llm": llm,
                        "verbose": False,
                        "save_logs": False,
                        "enable_cache": False,
                        "custom_instructions": """
                        Sen bir Matris FaktÃ¶rizasyon UygulamasÄ±'nÄ±n AI asistanÄ±sÄ±n. Bu uygulama matris faktÃ¶rizasyon algoritmalarÄ± iÃ§in kapsamlÄ± bir araÃ§tÄ±r.
                        
                        UYGULAMA Ã–ZELLÄ°KLERÄ°:
                        
                        ğŸ“Š KLASÄ°K ALGORÄ°TMALAR:
                        1. SVD (Singular Value Decomposition) - Ã–neri sistemleri ve gÃ¼rÃ¼ltÃ¼ temizleme iÃ§in
                        2. PCA (Principal Component Analysis) - Veri gÃ¶rselleÅŸtirme ve Ã¶zellik seÃ§imi iÃ§in
                        3. NMF (Non-negative Matrix Factorization) - GÃ¶rÃ¼ntÃ¼ iÅŸleme ve topic modeling iÃ§in
                        4. ALS (Alternating Least Squares) - BÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶neri motorlarÄ± iÃ§in
                        
                        ğŸš€ MODERN DEEP LEARNING ALGORÄ°TMALAR (PyTorch gerekli):
                        5. NCF (Neural Collaborative Filtering) - Netflix/YouTube tarzÄ± Ã¶neriler
                        6. Autoencoder (Denoising & VAE) - GÃ¼rÃ¼ltÃ¼ temizleme ve Ã¶neriler
                        7. Factorization Machines (FM) & DeepFM - Context-aware Ã¶neriler
                        8. Transformer (BERT4Rec/SASRec) - Sequential Ã¶neriler (TikTok/YouTube tarzÄ±)
                        9. GNN (Graph Neural Network) - Graph tabanlÄ± Ã¶neriler (Pinterest/Uber Eats tarzÄ±)
                        
                        KULLANILABÄ°LÄ°R FONKSÄ°YONLAR:
                        - SVD analizi yapmak iÃ§in: run_svd_analysis(n_components=5)
                        - Veri seti hakkÄ±nda sorular sorulabilir
                        - Algoritma Ã¶nerileri yapabilirsin
                        - Veri analizi ve gÃ¶rselleÅŸtirme Ã¶nerileri sunabilirsin
                        
                        GÃ–REVÄ°N:
                        - KullanÄ±cÄ±larÄ±n veri setleri hakkÄ±ndaki sorularÄ±nÄ± yanÄ±tla
                        - Hangi algoritmanÄ±n ne zaman kullanÄ±lacaÄŸÄ±nÄ± Ã¶ner
                        - Matris faktÃ¶rizasyon teknikleri hakkÄ±nda bilgi ver
                        - Veri analizi iÃ§in Python pandas kodlarÄ± Ã¶ner
                        - Gerekirse grafikler oluÅŸtur
                        - KullanÄ±cÄ± "SVD Ã§alÄ±ÅŸtÄ±r" veya "matrix factorization yap" derse, run_svd_analysis fonksiyonunu kullan
                        """
                    }
                    )
                except Exception as config_error:
                    error_str = str(config_error)
                    # Duplicate validator hatasÄ±nÄ± Ã¶zel olarak yakala
                    if "duplicate validator" in error_str.lower() or "validate_llm" in error_str:
                        st.warning("âš ï¸ PandasAI validator Ã§akÄ±ÅŸmasÄ± tespit edildi. Ã‡Ã¶zÃ¼m deneniyor...")
                        try:
                            # Pydantic model cache'ini temizlemeyi dene
                            import pydantic
                            if hasattr(pydantic, 'BaseModel'):
                                # Pydantic v1 iÃ§in
                                from pydantic import BaseModel
                                BaseModel.__config__ = None
                            # Basit config ile tekrar dene
                            smart_df = SmartDataframe(df, config={"llm": llm, "verbose": False})
                        except Exception as retry_error:
                            st.warning(f"âš ï¸ Ä°kinci deneme baÅŸarÄ±sÄ±z: {str(retry_error)}")
                            st.info("ğŸ’¡ Alternatif moda geÃ§iliyor...")
                            st.session_state.pandasai_agent = "alternative_mode"
                    else:
                        # EÄŸer config ile baÅŸka bir sorun varsa, basit config dene
                        st.warning(f"âš ï¸ PandasAI config hatasÄ±: {error_str}")
                        st.info("ğŸ’¡ Basit mod ile devam ediliyor...")
                        try:
                            smart_df = SmartDataframe(df, config={"llm": llm, "verbose": False})
                        except:
                            st.session_state.pandasai_agent = "alternative_mode"
            
                # Custom skills ekle - SVD Ã§alÄ±ÅŸtÄ±rma
                def run_svd_analysis(n_components=5):
                    """
                    SVD (Singular Value Decomposition) analizi yapar
                    
                    Args:
                        n_components: KullanÄ±lacak tekil deÄŸer sayÄ±sÄ±
                    
                    Returns:
                        SVD sonuÃ§larÄ± hakkÄ±nda bilgi
                    """
                    try:
                        from algorithms.svd import SVDRecommender
                        from scipy.sparse import issparse
                        import numpy as np
                        
                        # DataFrame'i rating matrix'e Ã§evir (sayÄ±sal sÃ¼tunlar)
                        numeric_df = df.select_dtypes(include=[np.number])
                        
                        if numeric_df.empty:
                            return "Hata: DataFrame'de sayÄ±sal sÃ¼tun bulunamadÄ±!"
                        
                        # NaN deÄŸerleri 0 ile doldur
                        rating_matrix = numeric_df.fillna(0).values
                        
                        # SVD modeli oluÅŸtur ve eÄŸit
                        svd_model = SVDRecommender(n_components=n_components)
                        svd_model.fit(rating_matrix)
                        
                        # SonuÃ§larÄ± dÃ¶ndÃ¼r
                        explained_variance = svd_model.explained_variance_ratio_
                        total_variance = explained_variance.sum()
                        
                        result = f"""
                        SVD Analizi TamamlandÄ±!
                        
                        - KullanÄ±lan BileÅŸen SayÄ±sÄ±: {n_components}
                        - Toplam AÃ§Ä±klanan Varyans: {total_variance:.2%}
                        - BileÅŸen BazÄ±nda Varyans:
                        """
                        for i, var in enumerate(explained_variance):
                            result += f"\n  - BileÅŸen {i+1}: {var:.2%}"
                        
                        return result
                    except Exception as e:
                        return f"SVD analizi sÄ±rasÄ±nda hata: {str(e)}"
                
                # Custom skill'i ekle (eÄŸer destekleniyorsa)
                try:
                    smart_df.add_skills([run_svd_analysis])
                except Exception as skill_error:
                    st.warning(f"âš ï¸ Custom skills eklenemedi: {str(skill_error)}")
                    st.info("ğŸ’¡ Temel Ã¶zelliklerle devam ediliyor...")
                
                st.session_state.pandasai_agent = smart_df
                
                st.success("âœ… AI AsistanÄ± hazÄ±r!")
            
        except ImportError as import_err:
            st.error("âŒ PandasAI yÃ¼klÃ¼ deÄŸil veya import edilemedi!")
            st.info(f"ğŸ’¡ Hata: {str(import_err)}")
            st.info("ğŸ’¡ Alternatif mod aktif ediliyor...")
            # Alternatif moda geÃ§
            st.session_state.pandasai_agent = "alternative_mode"
        except Exception as e:
            error_msg = str(e)
            # Duplicate validator hatasÄ±nÄ± Ã¶zel olarak yakala
            if "duplicate validator" in error_msg.lower() or "validate_llm" in error_msg:
                st.warning("âš ï¸ PandasAI validator Ã§akÄ±ÅŸmasÄ± tespit edildi!")
                st.info("ğŸ’¡ Bu, Pydantic versiyon uyumsuzluÄŸundan kaynaklanabilir.")
                st.info("ğŸ’¡ Alternatif moda geÃ§iliyor...")
                st.session_state.pandasai_agent = "alternative_mode"
            elif "Python 3.14" in error_msg or "pydantic" in error_msg.lower() or "default_factory" in error_msg:
                st.warning("âš ï¸ PandasAI Python 3.14 ile uyumlu deÄŸil!")
                st.info("ğŸ’¡ Alternatif OpenAI modu aktif ediliyor...")
                # Alternatif moda geÃ§
                st.session_state.pandasai_agent = "alternative_mode"
            else:
                st.error(f"âŒ AI AsistanÄ± oluÅŸturulurken hata: {error_msg}")
                st.info("ğŸ’¡ Alternatif mod denenecek...")
                st.session_state.pandasai_agent = "alternative_mode"
        
        # Alternatif mod: Direkt API kullan (OpenAI veya Gemini)
        if st.session_state.pandasai_agent == "alternative_mode":
            try:
                if api_provider == "OpenAI":
                    from openai import OpenAI as OpenAIClient
                    client = OpenAIClient(api_key=api_key)
                    model_name = "gpt-4o-mini"
                    provider_name = "OpenAI"
                else:  # Gemini
                    try:
                        import google.generativeai as genai
                        genai.configure(api_key=api_key)
                        client = genai
                        # GÃ¼ncel Gemini model adlarÄ± - Ã¶nce mevcut modelleri kontrol et
                        try:
                            # Mevcut modelleri listele ve uygun olanÄ± seÃ§
                            available_models = []
                            for model in genai.list_models():
                                if 'generateContent' in model.supported_generation_methods:
                                    # Model adÄ±nÄ± normalize et (models/ prefix'ini kaldÄ±r)
                                    model_name_clean = model.name.replace("models/", "") if model.name.startswith("models/") else model.name
                                    available_models.append(model_name_clean)
                            
                            # Ã–ncelik sÄ±rasÄ±: flash, pro, genel
                            if available_models:
                                if any('flash' in model.lower() for model in available_models):
                                    model_name = next((m for m in available_models if 'flash' in m.lower()), "gemini-1.5-flash")
                                elif any('pro' in model.lower() for model in available_models):
                                    model_name = next((m for m in available_models if 'pro' in m.lower()), "gemini-1.5-pro")
                                else:
                                    model_name = available_models[0]
                            else:
                                model_name = "gemini-1.5-flash"  # VarsayÄ±lan
                        except Exception as list_err:
                            # EÄŸer model listesi alÄ±namazsa, gÃ¼ncel model adlarÄ±nÄ± dene
                            model_name = "gemini-1.5-flash"  # VarsayÄ±lan
                        provider_name = "Google Gemini"
                    except ImportError:
                        st.error("âŒ Google Generative AI kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil!")
                        st.info("ğŸ’¡ LÃ¼tfen ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: `python -m pip install google-generativeai`")
                        st.stop()
                
                st.session_state.pandasai_agent = {
                    "type": "direct_api",
                    "client": client,
                    "df": df,
                    "provider": api_provider,
                    "model": model_name
                }
                st.success(f"âœ… AI AsistanÄ± hazÄ±r! ({provider_name} Direct Mode)")
                st.info(f"ğŸ’¡ Bu mod PandasAI yerine direkt {provider_name} API kullanÄ±r.")
            except Exception as alt_err:
                st.error(f"âŒ Alternatif mod da baÅŸarÄ±sÄ±z: {str(alt_err)}")
                st.stop()
    
    # Chat arayÃ¼zÃ¼
    st.markdown("### ğŸ’¬ Sohbet")
    
    # Chat geÃ§miÅŸini gÃ¶ster
    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # EÄŸer grafik varsa gÃ¶ster
            if "figure" in message:
                st.pyplot(message["figure"])
    
    # KullanÄ±cÄ± giriÅŸi
    user_query = st.chat_input("Veri hakkÄ±nda bir soru sorun... (Ã¶rn: 'En Ã§ok puan veren kullanÄ±cÄ± kim?')")
    
    if user_query:
        # KullanÄ±cÄ± mesajÄ±nÄ± ekle
        st.session_state.chat_messages.append({
            "role": "user",
            "content": user_query
        })
        
        # KullanÄ±cÄ± mesajÄ±nÄ± gÃ¶ster
        with st.chat_message("user"):
            st.markdown(user_query)
        
        # AI yanÄ±tÄ±nÄ± oluÅŸtur
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” DÃ¼ÅŸÃ¼nÃ¼yorum..."):
                try:
                    # PandasAI veya alternatif mod kontrolÃ¼
                    if isinstance(st.session_state.pandasai_agent, dict) and st.session_state.pandasai_agent.get("type") == "direct_api":
                        # Alternatif mod: Direkt API (OpenAI veya Gemini)
                        client = st.session_state.pandasai_agent["client"]
                        df_for_analysis = st.session_state.pandasai_agent["df"]
                        provider = st.session_state.pandasai_agent["provider"]
                        model_name = st.session_state.pandasai_agent["model"]
                        
                        # DataFrame'i string formatÄ±na Ã§evir (ilk 100 satÄ±r)
                        df_preview = df_for_analysis.head(100).to_string()
                        
                        # Uygulama hakkÄ±nda kapsamlÄ± bilgi
                        app_info = """
MATRÄ°S FAKTÃ–RÄ°ZASYON UYGULAMASI - AI ASÄ°STANI

Bu uygulama matris faktÃ¶rizasyon algoritmalarÄ± iÃ§in kapsamlÄ± bir araÃ§tÄ±r. AÅŸaÄŸÄ±daki Ã¶zelliklere sahiptir:

ğŸ“Š KLASÄ°K ALGORÄ°TMALAR:
1. SVD (Singular Value Decomposition)
   - KullanÄ±m: Ã–neri sistemleri, gÃ¼rÃ¼ltÃ¼ temizleme
   - SÄ±nÄ±f: SVDRecommender, SVDNoiseReducer
   - En iyi: Matematiksel olarak en kesin yÃ¶ntem

2. PCA (Principal Component Analysis)
   - KullanÄ±m: Veri gÃ¶rselleÅŸtirme, Ã¶zellik seÃ§imi
   - SÄ±nÄ±f: PCAAnalyzer
   - En iyi: Boyut indirgeme iÃ§in

3. NMF (Non-negative Matrix Factorization)
   - KullanÄ±m: GÃ¶rÃ¼ntÃ¼ iÅŸleme, topic modeling
   - SÄ±nÄ±f: NMFImageProcessor, NMFTopicModeler
   - En iyi: Pozitif deÄŸerlerle Ã§alÄ±ÅŸan veriler iÃ§in

4. ALS (Alternating Least Squares)
   - KullanÄ±m: BÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶neri motorlarÄ±
   - SÄ±nÄ±f: ALSRecommender
   - En iyi: Paralel Ã§alÄ±ÅŸma gerektiren sistemler

ğŸš€ MODERN DEEP LEARNING ALGORÄ°TMALAR (PyTorch gerekli):
5. NCF (Neural Collaborative Filtering)
   - KullanÄ±m: Netflix, YouTube tarzÄ± Ã¶neriler
   - SÄ±nÄ±f: NCFRecommender
   - Ã–zellik: DoÄŸrusal olmayan iliÅŸkileri Ã¶ÄŸrenir

6. Autoencoder (Denoising & VAE)
   - KullanÄ±m: GÃ¼rÃ¼ltÃ¼ temizleme, Ã¶neriler
   - SÄ±nÄ±f: DenoisingAutoencoder, VAERecommender
   - Ã–zellik: SVD ve PCA'in Deep Learning karÅŸÄ±lÄ±ÄŸÄ±

7. Factorization Machines (FM) & DeepFM
   - KullanÄ±m: Context-aware Ã¶neriler, CTR tahmini
   - SÄ±nÄ±f: FactorizationMachine, DeepFM
   - Ã–zellik: Yan bilgileri (saat, cihaz, vb.) kullanÄ±r

8. Transformer (BERT4Rec/SASRec)
   - KullanÄ±m: TikTok, YouTube tarzÄ± sequential Ã¶neriler
   - SÄ±nÄ±f: TransformerRecommender
   - Ã–zellik: Zaman ve sÄ±ra bilgisini kullanÄ±r

9. GNN (Graph Neural Network)
   - KullanÄ±m: Pinterest, Uber Eats tarzÄ± graph tabanlÄ± Ã¶neriler
   - SÄ±nÄ±f: GNNRecommender
   - Ã–zellik: Veriyi graph olarak modelleyerek Ã¶neriler Ã¼retir

KULLANILABÄ°LÄ°R FONKSÄ°YONLAR:
- SVD analizi: run_svd_analysis(n_components=5)
- Veri yÃ¼kleme: generate_sample_data(), load_rating_data_from_file()
- GÃ¶rselleÅŸtirme: plot_ratings_matrix(), plot_recommendations()

GÃ–REVÄ°N:
- KullanÄ±cÄ±larÄ±n veri setleri hakkÄ±ndaki sorularÄ±nÄ± yanÄ±tla
- Hangi algoritmanÄ±n ne zaman kullanÄ±lacaÄŸÄ±nÄ± Ã¶ner
- Matris faktÃ¶rizasyon teknikleri hakkÄ±nda detaylÄ± bilgi ver
- Veri analizi iÃ§in Python pandas kodlarÄ± Ã¶ner
- KullanÄ±cÄ± "SVD Ã§alÄ±ÅŸtÄ±r" veya "matrix factorization yap" derse, uygun algoritmayÄ± Ã¶ner
"""
                        
                        df_info = f"""
{app_info}

VERÄ° SETÄ° BÄ°LGÄ°LERÄ°:
- SatÄ±r sayÄ±sÄ±: {len(df_for_analysis)}
- SÃ¼tun sayÄ±sÄ±: {len(df_for_analysis.columns)}
- SÃ¼tunlar: {', '.join(df_for_analysis.columns.tolist())}

VERÄ° Ã–NÄ°ZLEME (ilk 100 satÄ±r):
{df_preview}

KULLANICI SORUSU: {user_query}

LÃ¼tfen bu veri seti hakkÄ±nda kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla. EÄŸer kullanÄ±cÄ± matris faktÃ¶rizasyon algoritmalarÄ± hakkÄ±nda soru sorarsa, yukarÄ±daki bilgileri kullanarak detaylÄ± aÃ§Ä±klama yap. EÄŸer grafik Ã§izilmesi gerekiyorsa, Python kodu Ã¶ner ama grafik Ã§izme.
"""
                        
                        # API Ã§aÄŸrÄ±sÄ± (OpenAI veya Gemini)
                        if provider == "OpenAI":
                            # OpenAI API Ã§aÄŸrÄ±sÄ±
                            response_obj = client.chat.completions.create(
                                model=model_name,
                                messages=[
                                    {"role": "system", "content": """Sen bir Matris FaktÃ¶rizasyon UygulamasÄ±'nÄ±n AI asistanÄ±sÄ±n. Bu uygulama matris faktÃ¶rizasyon algoritmalarÄ± iÃ§in kapsamlÄ± bir araÃ§tÄ±r.

UYGULAMA Ã–ZELLÄ°KLERÄ°:
ğŸ“Š KLASÄ°K ALGORÄ°TMALAR: SVD (Ã¶neri/gÃ¼rÃ¼ltÃ¼ temizleme), PCA (gÃ¶rselleÅŸtirme), NMF (gÃ¶rÃ¼ntÃ¼/topic modeling), ALS (bÃ¼yÃ¼k Ã¶lÃ§ekli)
ğŸš€ MODERN ALGORÄ°TMALAR: NCF, Autoencoder, Factorization Machines, Transformer, GNN (PyTorch gerekli)

GÃ–REVÄ°N:
- Veri setleri hakkÄ±ndaki sorularÄ± yanÄ±tla
- Hangi algoritmanÄ±n ne zaman kullanÄ±lacaÄŸÄ±nÄ± Ã¶ner
- Matris faktÃ¶rizasyon teknikleri hakkÄ±nda bilgi ver
- Python pandas kodlarÄ± Ã¶ner
- KullanÄ±cÄ± "SVD Ã§alÄ±ÅŸtÄ±r" veya "matrix factorization yap" derse, uygun algoritmayÄ± Ã¶ner"""},
                                    {"role": "user", "content": df_info}
                                ],
                                temperature=0.7,
                                max_tokens=1000
                            )
                            response = response_obj.choices[0].message.content
                        else:  # Gemini
                            # Gemini API Ã§aÄŸrÄ±sÄ±
                            try:
                                # Model adÄ±nÄ± kullan (GenerativeModel otomatik olarak normalize eder)
                                model = client.GenerativeModel(model_name)
                                # Prompt'ta emoji kullanmamaya dikkat et (encoding sorunlarÄ± iÃ§in)
                                prompt = f"""Sen bir veri analizi asistanÄ±sÄ±n. KullanÄ±cÄ±larÄ±n veri setleri hakkÄ±ndaki sorularÄ±nÄ± yanÄ±tla. Python pandas kodlarÄ± Ã¶nerebilirsin.

{df_info}"""
                                
                                response_obj = model.generate_content(prompt)
                                # Response'u gÃ¼venli ÅŸekilde al
                                try:
                                    response = response_obj.text
                                except Exception as text_err:
                                    # EÄŸer text alÄ±namazsa, parts'tan al
                                    if hasattr(response_obj, 'parts') and response_obj.parts:
                                        response = ''.join([part.text for part in response_obj.parts if hasattr(part, 'text')])
                                    else:
                                        response = str(response_obj)
                            except Exception as gemini_err:
                                # EÄŸer model bulunamazsa, alternatif model dene
                                if "404" in str(gemini_err) or "not found" in str(gemini_err).lower():
                                    # Alternatif modelleri dene - gÃ¼ncel model adlarÄ±
                                    alternative_models = [
                                        "gemini-1.5-flash",
                                        "gemini-1.5-pro", 
                                        "gemini-pro",
                                        "gemini-1.0-pro",
                                        "models/gemini-1.5-flash",
                                        "models/gemini-1.5-pro"
                                    ]
                                    response = None
                                    for alt_model in alternative_models:
                                        try:
                                            # Her model adÄ±nÄ± hem prefix'li hem de prefix'siz dene
                                            model_variants = [alt_model]
                                            if not alt_model.startswith("models/"):
                                                model_variants.append(f"models/{alt_model}")
                                            else:
                                                model_variants.append(alt_model.replace("models/", ""))
                                            
                                            for try_model in model_variants:
                                                try:
                                                    model = client.GenerativeModel(try_model)
                                                    prompt = f"""Sen bir Matris FaktÃ¶rizasyon UygulamasÄ±'nÄ±n AI asistanÄ±sÄ±n. Bu uygulama matris faktÃ¶rizasyon algoritmalarÄ± iÃ§in kapsamlÄ± bir araÃ§tÄ±r.

UYGULAMA Ã–ZELLÄ°KLERÄ°:
ğŸ“Š KLASÄ°K ALGORÄ°TMALAR: SVD (Ã¶neri/gÃ¼rÃ¼ltÃ¼ temizleme), PCA (gÃ¶rselleÅŸtirme), NMF (gÃ¶rÃ¼ntÃ¼/topic modeling), ALS (bÃ¼yÃ¼k Ã¶lÃ§ekli)
ğŸš€ MODERN ALGORÄ°TMALAR: NCF, Autoencoder, Factorization Machines, Transformer, GNN (PyTorch gerekli)

GÃ–REVÄ°N:
- Veri setleri hakkÄ±ndaki sorularÄ± yanÄ±tla
- Hangi algoritmanÄ±n ne zaman kullanÄ±lacaÄŸÄ±nÄ± Ã¶ner
- Matris faktÃ¶rizasyon teknikleri hakkÄ±nda bilgi ver
- Python pandas kodlarÄ± Ã¶ner
- KullanÄ±cÄ± "SVD Ã§alÄ±ÅŸtÄ±r" veya "matrix factorization yap" derse, uygun algoritmayÄ± Ã¶ner

{df_info}"""
                                                    response_obj = model.generate_content(prompt)
                                                    # Response'u gÃ¼venli ÅŸekilde al
                                                    try:
                                                        response = response_obj.text
                                                    except Exception:
                                                        if hasattr(response_obj, 'parts') and response_obj.parts:
                                                            response = ''.join([part.text for part in response_obj.parts if hasattr(part, 'text')])
                                                        else:
                                                            response = str(response_obj)
                                                    # Model'i gÃ¼ncelle
                                                    st.session_state.pandasai_agent["model"] = try_model
                                                    break
                                                except:
                                                    continue
                                            
                                            if response:
                                                break
                                        except:
                                            continue
                                    
                                    if response is None:
                                        response = f"Hata: Gemini API - Model bulunamadi. Lutfen API key'inizi kontrol edin.\n\nDetay: {str(gemini_err)}"
                                else:
                                    # Hata mesajÄ±nÄ± gÃ¼venli ÅŸekilde encode et
                                    try:
                                        error_msg = str(gemini_err).encode('utf-8', errors='replace').decode('utf-8')
                                    except:
                                        error_msg = str(gemini_err)
                                    response = f"Hata: Gemini API - {error_msg}"
                        
                        # EÄŸer kullanÄ±cÄ± SVD istiyorsa
                        if "svd" in user_query.lower() or "matrix factorization" in user_query.lower():
                            try:
                                from algorithms.svd import SVDRecommender
                                import numpy as np
                                
                                numeric_df = df_for_analysis.select_dtypes(include=[np.number])
                                if not numeric_df.empty:
                                    rating_matrix = numeric_df.fillna(0).values
                                    svd_model = SVDRecommender(n_components=5)
                                    svd_model.fit(rating_matrix)
                                    explained_variance = svd_model.explained_variance_ratio_
                                    total_variance = explained_variance.sum()
                                    
                                    svd_result = f"""

**SVD Analizi SonuÃ§larÄ±:**
- KullanÄ±lan BileÅŸen SayÄ±sÄ±: 5
- Toplam AÃ§Ä±klanan Varyans: {total_variance:.2%}
- BileÅŸen BazÄ±nda Varyans:
"""
                                    for i, var in enumerate(explained_variance):
                                        svd_result += f"  - BileÅŸen {i+1}: {var:.2%}\n"
                                    
                                    response += svd_result
                            except Exception as svd_err:
                                response += f"\n\nâš ï¸ SVD analizi sÄ±rasÄ±nda hata: {str(svd_err)}"
                    else:
                        # PandasAI modu
                        response = st.session_state.pandasai_agent.chat(user_query)
                    
                    # YanÄ±tÄ± gÃ¶ster
                    st.markdown(response)
                    
                    # MesajÄ± geÃ§miÅŸe ekle
                    message_to_save = {
                        "role": "assistant",
                        "content": response
                    }
                    
                    # EÄŸer grafik oluÅŸturulduysa (matplotlib figure)
                    # PandasAI genellikle grafikleri otomatik olarak gÃ¶sterir
                    # Ancak manuel kontrol iÃ§in:
                    try:
                        import matplotlib.pyplot as plt
                        if plt.get_fignums():
                            fig = plt.gcf()
                            st.pyplot(fig)
                            message_to_save["figure"] = fig
                            plt.close(fig)
                    except:
                        pass
                    
                    st.session_state.chat_messages.append(message_to_save)
                    
                except Exception as e:
                    error_msg = f"âŒ Hata: {str(e)}"
                    st.error(error_msg)
                    st.session_state.chat_messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
    
    # YardÄ±mcÄ± Ã¶rnek sorular
    with st.expander("ğŸ’¡ Ã–rnek Sorular", expanded=False):
        st.markdown("""
        **Temel Analiz:**
        - "Veri setinde kaÃ§ satÄ±r ve sÃ¼tun var?"
        - "Eksik veriler var mÄ±?"
        - "SÃ¼tunlarÄ±n istatistiklerini gÃ¶ster"
        
        **Grafikler:**
        - "Rating daÄŸÄ±lÄ±mÄ±nÄ± histogram olarak Ã§iz"
        - "KullanÄ±cÄ± sayÄ±larÄ±nÄ± bar chart olarak gÃ¶ster"
        - "Korelasyon matrisini gÃ¶ster"
        
        **Ã–zel Komutlar:**
        - "SVD Ã§alÄ±ÅŸtÄ±r" veya "SVD analizi yap"
        - "Matrix factorization yap"
        
        **Veri Ä°ÅŸleme:**
        - "BoÅŸ verileri temizle"
        - "En yÃ¼ksek rating'e sahip 10 kullanÄ±cÄ±yÄ± gÃ¶ster"
        - "Ortalama rating'i hesapla"
        """)


if __name__ == "__main__":
    main()

